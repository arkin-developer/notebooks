




















import platform
import sys

print("操作系统:", platform.system())          # Windows / Linux / Darwin (macOS)
print("系统版本:", platform.version())         # 内核或版本号
print("发行版本:", platform.release())         # 例如 10 / 11 / 22.6.0
print("详细信息:", platform.platform())        # 汇总
print("Python版本:", sys.version)             # Python 解释器版本
print("处理器:", platform.processor())         # CPU 信息
print("机器类型:", platform.machine())         # x86_64 / arm64

# 安装检测显卡的依赖
%pip install gputil

import GPUtil

gpus = GPUtil.getGPUs()
for gpu in gpus:
    print(f"显卡型号: {gpu.name}")
    print(f"显存总量: {gpu.memoryTotal} MB")
    print("-" * 30)





%pip install torch==2.3.1+cu121
%pip install transformers==4.55.4
%pip install modelscope==1.29.0
%pip install peft==0.17.1
%pip install datasets==3.2.0
%pip install accelerate==1.10.0





# 📌 打印脚本相关库的版本信息
import torch, transformers, modelscope, peft

print("torch:", torch.__version__)

# transformers 是 peft 和 modelscope 依赖的核心库
try:
    import transformers
    print("transformers:", transformers.__version__)
except ImportError:
    print("transformers: 未安装")

try:
    import modelscope
    print("modelscope:", modelscope.__version__)
except ImportError:
    print("modelscope: 未安装")

try:
    import peft
    print("peft:", peft.__version__)
except ImportError:
    print("peft: 未安装")

try:
    import datasets
    print("datasets:", datasets.__version__)
except ImportError:
    print("datasets: 未安装")

try:
    import accelerate
    print("accelerate:", accelerate.__version__)
except ImportError:
    print("accelerate: 未安装")








from modelscope import AutoTokenizer, AutoModelForCausalLM

model_id = "qwen/Qwen2.5-1.5B-Instruct"  # 可替换
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    trust_remote_code=True
)





from modelscope.msdatasets import MsDataset

ds =  MsDataset.load('AI-ModelScope/alpaca-gpt4-data-zh', subset_name='default', split='train')








%pip install tqdm

from tqdm import tqdm
import numpy as np

def recommend_max_length(dataset, tokenizer, sample_size=5000, quantile=95):
    """
    自动统计 token 长度分布，并推荐 max_length
    Args:
        dataset: MsDataset 对象
        tokenizer: HF AutoTokenizer
        sample_size: 抽样数量（避免全量太慢）
        quantile: 分位数（默认95）
    """
    total = min(sample_size, len(dataset))
    lengths = []

    print(f"开始统计，抽样 {total} 条数据 ...")

    for i in tqdm(range(total)):
        ex = dataset[i]
        instruction = ex.get("instruction", "")
        input_text = ex.get("input", "") or ""
        output_text = ex.get("output", "")

        if input_text.strip():
            prompt = f"指令: {instruction}\n输入: {input_text}\n回答:"
        else:
            prompt = f"指令: {instruction}\n回答:"

        full_text = prompt + output_text
        tokenized = tokenizer(full_text, truncation=False)
        lengths.append(len(tokenized["input_ids"]))

    max_len = max(lengths)
    avg_len = np.mean(lengths)
    q_len = np.percentile(lengths, quantile)

    print("\n=== Token 长度统计结果 ===")
    print(f"最大长度: {max_len}")
    print(f"平均长度: {avg_len:.2f}")
    print(f"{quantile}% 分位数长度: {q_len:.0f}")
    print("=========================")
    print(f"👉 推荐 max_length = {int(min(q_len, tokenizer.model_max_length))}")
    print(f"(模型支持的最大长度 = {tokenizer.model_max_length})")

    return int(min(q_len, tokenizer.model_max_length))


# 计算数据集应该定义的最大长度
max_length = recommend_max_length(ds, tokenizer)





# 自定义数据处理函数(需要针对自己的数据集范式来编写，这里只针对alpaca)
def preprocess(example):
    # 丢掉 instruction 或 output 缺失的样本
    if not example['instruction'] or not example['output']:
        return None

    # alpaca 数据有指令、输入、输出三个标签
    instruction = example['instruction']
    input_text = example.get('input') or ""  # input 可能为 None
    output_text = example['output']

    if input_text.strip():
        prompt = f"指令: {instruction}\n输入: {input_text}\n回答:"
    else:
        prompt = f"指令: {instruction}\n回答:"

    full_text = prompt + output_text

    enc = tokenizer(
        full_text,  # 需要进行token化的文本
        truncation=True,  # 文本过大的时候是否截断
        max_length=max_length,  # 根据模型和数据集决定最合适的
        padding="max_length",  # 🔹 保证长度一致，DataLoader 堆叠安全
        return_tensors="pt"  # 返回的数据类型，pt:pytorch.tensor; tf:tensorflow; np:numpy
    )
    # 单个样本是字典格式
    return {
        "input_ids": enc["input_ids"][0],
        "labels": enc["input_ids"][0]
    }

train_dataset = ds.map(preprocess)
train_dataset = train_dataset.filter(lambda x: x is not None)

print('数据处理完成')





from torch.utils.data import DataLoader, Subset

# 🛠️ 自定义批处理函数 (collate_fn)
def collate_fn(batch):
    """
    作用：
    - DataLoader 会把一个 batch 的样本（list[dict]）传进来
    - 这里需要手动拼接成 tensor，并且对齐长度（pad）
    """

    # 取出每个样本的 input_ids 和 labels，转成 tensor
    input_ids = [torch.tensor(item["input_ids"]) for item in batch]
    labels = [torch.tensor(item["labels"]) for item in batch]

    # 🔹 对 input_ids 做 padding
    #   - batch_first=True: 结果形状 (batch_size, seq_len)
    #   - padding_value=tokenizer.pad_token_id: 使用 tokenizer 的 pad_token_id 填充
    input_ids = torch.nn.utils.rnn.pad_sequence(
        input_ids, batch_first=True, padding_value=tokenizer.pad_token_id
    )

    # 🔹 对 labels 做 padding
    #   - 注意这里 padding_value = -100
    #   - 在 PyTorch 的 CrossEntropyLoss 里，-100 会被忽略，不参与 loss 计算
    labels = torch.nn.utils.rnn.pad_sequence(
        labels, batch_first=True, padding_value=-100
    )

    # 返回字典，方便直接喂给模型
    return {
        "input_ids": input_ids,
        "labels": labels
    }


# 在数据预处理后，分割训练集和测试集
from sklearn.model_selection import train_test_split
import numpy as np

# 获取数据集的总长度
total_samples = len(train_dataset)
print(f"总样本数: {total_samples}")

# 设置随机种子确保可重复性
np.random.seed(42)

# 分割训练集和测试集 (80% 训练, 20% 测试)
train_indices, test_indices = train_test_split(
    range(total_samples), 
    test_size=0.2, 
    random_state=42
)

# 创建训练集和测试集
train_subset = Subset(train_dataset, train_indices)
test_subset = Subset(train_dataset, test_indices)

print(f"训练集样本数: {len(train_subset)}")
print(f"测试集样本数: {len(test_subset)}")

# 为了快速验证，可以只取部分数据
small_train_dataset = Subset(train_subset, range(min(2000, len(train_subset))))
small_test_dataset = Subset(test_subset, range(min(500, len(test_subset))))

print(f"小训练集样本数: {len(small_train_dataset)}")
print(f"小测试集样本数: {len(small_test_dataset)}")

# 构建训练和测试的 DataLoader
train_loader = DataLoader(
    small_train_dataset,
    batch_size=4,
    shuffle=True,
    collate_fn=collate_fn
)

test_loader = DataLoader(
    small_test_dataset,
    batch_size=4,
    shuffle=False,  # 测试时不需要打乱
    collate_fn=collate_fn
)














from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=4,            
    lora_alpha=16,                
    target_modules=["q_proj", "v_proj"], 
    lora_dropout=0.05,             
    bias="none",                    
    task_type="CAUSAL_LM"           
)

# 🚀 将基础模型包装为 PEFT 模型
model = get_peft_model(model, lora_config)  # 这里默认会冻结非LoRA的参数

# 打印当前可训练参数量（仅 LoRA 部分），其余参数被冻结
model.print_trainable_parameters()

# 训练超参数
num_train_epochs = 2  # 对完整的数据集训练多少个批次





for name, param in model.named_parameters():
    if param.requires_grad:  # 这里只打印有梯度信息的
        print(name)





from torch.optim import AdamW
import os

# 创建保存目录
model_save_path = "./qwen2.5-finetuned-lora"
os.makedirs(model_save_path, exist_ok=True)

# 优化器设置
optimizer = AdamW(model.parameters(), lr=2e-4)

# 训练循环
model.train()
total_steps = len(train_loader) * num_train_epochs
current_step = 0

for epoch in range(num_train_epochs):
    epoch_loss = 0.0
    print(f"\n开始第 {epoch + 1}/{num_train_epochs} 轮训练...")
    
    for step, batch in enumerate(train_loader):
        input_ids = batch["input_ids"].to(model.device)
        labels = batch["labels"].to(model.device)

        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        
        epoch_loss += loss.item()
        current_step += 1

        # 每 100 个 step 打印一次进度
        if step % 100 == 0:
            avg_loss = epoch_loss / (step + 1)
            print(f"Epoch {epoch + 1}/{num_train_epochs} | Step {step}/{len(train_loader)} | "
                  f"Loss {loss.item():.4f} | Avg Loss {avg_loss:.4f} | "
                  f"Progress {current_step}/{total_steps}")
    
    # 每轮结束后打印平均损失
    avg_epoch_loss = epoch_loss / len(train_loader)
    print(f"第 {epoch + 1} 轮训练完成，平均损失: {avg_epoch_loss:.4f}")

print("\n训练完成！")

# 保存微调后的模型
print(f"正在保存模型到: {model_save_path}")
model.save_pretrained(model_save_path)
tokenizer.save_pretrained(model_save_path)

# 保存训练配置信息
import json
config_info = {
    "model_id": model_id,
    "lora_config": {
        "r": lora_config.r,
        "lora_alpha": lora_config.lora_alpha,
        "target_modules": list(lora_config.target_modules),  # 将set转换为list
        "lora_dropout": lora_config.lora_dropout,
        "bias": lora_config.bias,
        "task_type": lora_config.task_type
    },
    "training_config": {
        "num_epochs": num_train_epochs,
        "learning_rate": 2e-4,
        "batch_size": 4,
        "total_steps": total_steps
    }
}

with open(os.path.join(model_save_path, "training_config.json"), "w", encoding="utf-8") as f:
    json.dump(config_info, f, ensure_ascii=False, indent=2)

print(f"模型和配置已保存到: {model_save_path}")
print(f"可训练参数数量: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")








from peft import PeftModel

# 重新加载基础模型
base_model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    trust_remote_code=True
)

# 加载 LoRA 权重
finetuned_model = PeftModel.from_pretrained(base_model, model_save_path)
finetuned_model.eval()  # 设置为评估模式

print("微调后的模型加载完成")





from sklearn.metrics import accuracy_score
import re

def evaluate_model(model, test_loader, tokenizer):
    """
    评估模型在测试集上的表现
    """
    model.eval()
    total_loss = 0
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for batch in tqdm(test_loader, desc="测试中"):
            input_ids = batch["input_ids"].to(model.device)
            labels = batch["labels"].to(model.device)
            
            # 计算损失
            outputs = model(input_ids=input_ids, labels=labels)
            total_loss += outputs.loss.item()
            
            # 生成预测
            generated_ids = model.generate(
                input_ids=input_ids,
                max_new_tokens=max_length,  # 最大生成100个新token
                do_sample=True,
                temperature=0.7,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
            
            # 解码预测结果和目标
            for i in range(len(generated_ids)):
                # 获取输入部分（用于提取指令）
                input_text = tokenizer.decode(input_ids[i], skip_special_tokens=True)
                
                # 获取生成的部分（去掉输入部分）
                generated_text = tokenizer.decode(generated_ids[i][len(input_ids[i]):], skip_special_tokens=True)
                
                # 获取目标答案
                target_text = tokenizer.decode(labels[i][labels[i] != -100], skip_special_tokens=True)
                
                all_predictions.append(generated_text.strip())
                all_targets.append(target_text.strip())
    
    # 计算平均损失
    avg_loss = total_loss / len(test_loader)
    return avg_loss, all_predictions, all_targets

# 执行测试
print("开始测试微调后的模型...")
test_loss, predictions, targets = evaluate_model(finetuned_model, test_loader, tokenizer)

print(f"测试集平均损失: {test_loss:.4f}")
print(f"测试样本数: {len(predictions)}")














%pip install bert-score

from bert_score import score

# 假设 predictions 和 targets 已经是你的生成结果和真实答案列表
P, R, F1 = score(predictions, targets, lang="zh", verbose=True)  # lang 可以根据任务选择 'en' 或 'zh'

# P, R, F1 都是 tensor，形状为 [样本数]
avg_precision = P.mean().item()
avg_recall = R.mean().item()
avg_f1 = F1.mean().item()

print(f"BERTScore 平均 Precision: {avg_precision:.4f}")
print(f"BERTScore 平均 Recall:    {avg_recall:.4f}")
print(f"BERTScore 平均 F1:        {avg_f1:.4f}")
