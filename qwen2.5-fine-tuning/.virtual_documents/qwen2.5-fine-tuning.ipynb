




















import platform
import sys

print("æ“ä½œç³»ç»Ÿ:", platform.system())          # Windows / Linux / Darwin (macOS)
print("ç³»ç»Ÿç‰ˆæœ¬:", platform.version())         # å†…æ ¸æˆ–ç‰ˆæœ¬å·
print("å‘è¡Œç‰ˆæœ¬:", platform.release())         # ä¾‹å¦‚ 10 / 11 / 22.6.0
print("è¯¦ç»†ä¿¡æ¯:", platform.platform())        # æ±‡æ€»
print("Pythonç‰ˆæœ¬:", sys.version)             # Python è§£é‡Šå™¨ç‰ˆæœ¬
print("å¤„ç†å™¨:", platform.processor())         # CPU ä¿¡æ¯
print("æœºå™¨ç±»å‹:", platform.machine())         # x86_64 / arm64

# å®‰è£…æ£€æµ‹æ˜¾å¡çš„ä¾èµ–
%pip install gputil

import GPUtil

gpus = GPUtil.getGPUs()
for gpu in gpus:
    print(f"æ˜¾å¡å‹å·: {gpu.name}")
    print(f"æ˜¾å­˜æ€»é‡: {gpu.memoryTotal} MB")
    print("-" * 30)





%pip install torch==2.3.1+cu121
%pip install transformers==4.55.4
%pip install modelscope==1.29.0
%pip install peft==0.17.1
%pip install datasets==3.2.0
%pip install accelerate==1.10.0





# ğŸ“Œ æ‰“å°è„šæœ¬ç›¸å…³åº“çš„ç‰ˆæœ¬ä¿¡æ¯
import torch, transformers, modelscope, peft

print("torch:", torch.__version__)

# transformers æ˜¯ peft å’Œ modelscope ä¾èµ–çš„æ ¸å¿ƒåº“
try:
    import transformers
    print("transformers:", transformers.__version__)
except ImportError:
    print("transformers: æœªå®‰è£…")

try:
    import modelscope
    print("modelscope:", modelscope.__version__)
except ImportError:
    print("modelscope: æœªå®‰è£…")

try:
    import peft
    print("peft:", peft.__version__)
except ImportError:
    print("peft: æœªå®‰è£…")

try:
    import datasets
    print("datasets:", datasets.__version__)
except ImportError:
    print("datasets: æœªå®‰è£…")

try:
    import accelerate
    print("accelerate:", accelerate.__version__)
except ImportError:
    print("accelerate: æœªå®‰è£…")








from modelscope import AutoTokenizer, AutoModelForCausalLM

model_id = "qwen/Qwen2.5-1.5B-Instruct"  # å¯æ›¿æ¢
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    trust_remote_code=True
)





from modelscope.msdatasets import MsDataset

ds =  MsDataset.load('AI-ModelScope/alpaca-gpt4-data-zh', subset_name='default', split='train')








%pip install tqdm

from tqdm import tqdm
import numpy as np

def recommend_max_length(dataset, tokenizer, sample_size=5000, quantile=95):
    """
    è‡ªåŠ¨ç»Ÿè®¡ token é•¿åº¦åˆ†å¸ƒï¼Œå¹¶æ¨è max_length
    Args:
        dataset: MsDataset å¯¹è±¡
        tokenizer: HF AutoTokenizer
        sample_size: æŠ½æ ·æ•°é‡ï¼ˆé¿å…å…¨é‡å¤ªæ…¢ï¼‰
        quantile: åˆ†ä½æ•°ï¼ˆé»˜è®¤95ï¼‰
    """
    total = min(sample_size, len(dataset))
    lengths = []

    print(f"å¼€å§‹ç»Ÿè®¡ï¼ŒæŠ½æ · {total} æ¡æ•°æ® ...")

    for i in tqdm(range(total)):
        ex = dataset[i]
        instruction = ex.get("instruction", "")
        input_text = ex.get("input", "") or ""
        output_text = ex.get("output", "")

        if input_text.strip():
            prompt = f"æŒ‡ä»¤: {instruction}\nè¾“å…¥: {input_text}\nå›ç­”:"
        else:
            prompt = f"æŒ‡ä»¤: {instruction}\nå›ç­”:"

        full_text = prompt + output_text
        tokenized = tokenizer(full_text, truncation=False)
        lengths.append(len(tokenized["input_ids"]))

    max_len = max(lengths)
    avg_len = np.mean(lengths)
    q_len = np.percentile(lengths, quantile)

    print("\n=== Token é•¿åº¦ç»Ÿè®¡ç»“æœ ===")
    print(f"æœ€å¤§é•¿åº¦: {max_len}")
    print(f"å¹³å‡é•¿åº¦: {avg_len:.2f}")
    print(f"{quantile}% åˆ†ä½æ•°é•¿åº¦: {q_len:.0f}")
    print("=========================")
    print(f"ğŸ‘‰ æ¨è max_length = {int(min(q_len, tokenizer.model_max_length))}")
    print(f"(æ¨¡å‹æ”¯æŒçš„æœ€å¤§é•¿åº¦ = {tokenizer.model_max_length})")

    return int(min(q_len, tokenizer.model_max_length))


# è®¡ç®—æ•°æ®é›†åº”è¯¥å®šä¹‰çš„æœ€å¤§é•¿åº¦
max_length = recommend_max_length(ds, tokenizer)





# è‡ªå®šä¹‰æ•°æ®å¤„ç†å‡½æ•°(éœ€è¦é’ˆå¯¹è‡ªå·±çš„æ•°æ®é›†èŒƒå¼æ¥ç¼–å†™ï¼Œè¿™é‡Œåªé’ˆå¯¹alpaca)
def preprocess(example):
    # ä¸¢æ‰ instruction æˆ– output ç¼ºå¤±çš„æ ·æœ¬
    if not example['instruction'] or not example['output']:
        return None

    # alpaca æ•°æ®æœ‰æŒ‡ä»¤ã€è¾“å…¥ã€è¾“å‡ºä¸‰ä¸ªæ ‡ç­¾
    instruction = example['instruction']
    input_text = example.get('input') or ""  # input å¯èƒ½ä¸º None
    output_text = example['output']

    if input_text.strip():
        prompt = f"æŒ‡ä»¤: {instruction}\nè¾“å…¥: {input_text}\nå›ç­”:"
    else:
        prompt = f"æŒ‡ä»¤: {instruction}\nå›ç­”:"

    full_text = prompt + output_text

    enc = tokenizer(
        full_text,  # éœ€è¦è¿›è¡ŒtokenåŒ–çš„æ–‡æœ¬
        truncation=True,  # æ–‡æœ¬è¿‡å¤§çš„æ—¶å€™æ˜¯å¦æˆªæ–­
        max_length=max_length,  # æ ¹æ®æ¨¡å‹å’Œæ•°æ®é›†å†³å®šæœ€åˆé€‚çš„
        padding="max_length",  # ğŸ”¹ ä¿è¯é•¿åº¦ä¸€è‡´ï¼ŒDataLoader å †å å®‰å…¨
        return_tensors="pt"  # è¿”å›çš„æ•°æ®ç±»å‹ï¼Œpt:pytorch.tensor; tf:tensorflow; np:numpy
    )
    # å•ä¸ªæ ·æœ¬æ˜¯å­—å…¸æ ¼å¼
    return {
        "input_ids": enc["input_ids"][0],
        "labels": enc["input_ids"][0]
    }

train_dataset = ds.map(preprocess)
train_dataset = train_dataset.filter(lambda x: x is not None)

print('æ•°æ®å¤„ç†å®Œæˆ')





from torch.utils.data import DataLoader, Subset

# ğŸ› ï¸ è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•° (collate_fn)
def collate_fn(batch):
    """
    ä½œç”¨ï¼š
    - DataLoader ä¼šæŠŠä¸€ä¸ª batch çš„æ ·æœ¬ï¼ˆlist[dict]ï¼‰ä¼ è¿›æ¥
    - è¿™é‡Œéœ€è¦æ‰‹åŠ¨æ‹¼æ¥æˆ tensorï¼Œå¹¶ä¸”å¯¹é½é•¿åº¦ï¼ˆpadï¼‰
    """

    # å–å‡ºæ¯ä¸ªæ ·æœ¬çš„ input_ids å’Œ labelsï¼Œè½¬æˆ tensor
    input_ids = [torch.tensor(item["input_ids"]) for item in batch]
    labels = [torch.tensor(item["labels"]) for item in batch]

    # ğŸ”¹ å¯¹ input_ids åš padding
    #   - batch_first=True: ç»“æœå½¢çŠ¶ (batch_size, seq_len)
    #   - padding_value=tokenizer.pad_token_id: ä½¿ç”¨ tokenizer çš„ pad_token_id å¡«å……
    input_ids = torch.nn.utils.rnn.pad_sequence(
        input_ids, batch_first=True, padding_value=tokenizer.pad_token_id
    )

    # ğŸ”¹ å¯¹ labels åš padding
    #   - æ³¨æ„è¿™é‡Œ padding_value = -100
    #   - åœ¨ PyTorch çš„ CrossEntropyLoss é‡Œï¼Œ-100 ä¼šè¢«å¿½ç•¥ï¼Œä¸å‚ä¸ loss è®¡ç®—
    labels = torch.nn.utils.rnn.pad_sequence(
        labels, batch_first=True, padding_value=-100
    )

    # è¿”å›å­—å…¸ï¼Œæ–¹ä¾¿ç›´æ¥å–‚ç»™æ¨¡å‹
    return {
        "input_ids": input_ids,
        "labels": labels
    }


# åœ¨æ•°æ®é¢„å¤„ç†åï¼Œåˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
from sklearn.model_selection import train_test_split
import numpy as np

# è·å–æ•°æ®é›†çš„æ€»é•¿åº¦
total_samples = len(train_dataset)
print(f"æ€»æ ·æœ¬æ•°: {total_samples}")

# è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
np.random.seed(42)

# åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›† (80% è®­ç»ƒ, 20% æµ‹è¯•)
train_indices, test_indices = train_test_split(
    range(total_samples), 
    test_size=0.2, 
    random_state=42
)

# åˆ›å»ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
train_subset = Subset(train_dataset, train_indices)
test_subset = Subset(train_dataset, test_indices)

print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_subset)}")
print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_subset)}")

# ä¸ºäº†å¿«é€ŸéªŒè¯ï¼Œå¯ä»¥åªå–éƒ¨åˆ†æ•°æ®
small_train_dataset = Subset(train_subset, range(min(2000, len(train_subset))))
small_test_dataset = Subset(test_subset, range(min(500, len(test_subset))))

print(f"å°è®­ç»ƒé›†æ ·æœ¬æ•°: {len(small_train_dataset)}")
print(f"å°æµ‹è¯•é›†æ ·æœ¬æ•°: {len(small_test_dataset)}")

# æ„å»ºè®­ç»ƒå’Œæµ‹è¯•çš„ DataLoader
train_loader = DataLoader(
    small_train_dataset,
    batch_size=4,
    shuffle=True,
    collate_fn=collate_fn
)

test_loader = DataLoader(
    small_test_dataset,
    batch_size=4,
    shuffle=False,  # æµ‹è¯•æ—¶ä¸éœ€è¦æ‰“ä¹±
    collate_fn=collate_fn
)














from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=4,            
    lora_alpha=16,                
    target_modules=["q_proj", "v_proj"], 
    lora_dropout=0.05,             
    bias="none",                    
    task_type="CAUSAL_LM"           
)

# ğŸš€ å°†åŸºç¡€æ¨¡å‹åŒ…è£…ä¸º PEFT æ¨¡å‹
model = get_peft_model(model, lora_config)  # è¿™é‡Œé»˜è®¤ä¼šå†»ç»“éLoRAçš„å‚æ•°

# æ‰“å°å½“å‰å¯è®­ç»ƒå‚æ•°é‡ï¼ˆä»… LoRA éƒ¨åˆ†ï¼‰ï¼Œå…¶ä½™å‚æ•°è¢«å†»ç»“
model.print_trainable_parameters()

# è®­ç»ƒè¶…å‚æ•°
num_train_epochs = 2  # å¯¹å®Œæ•´çš„æ•°æ®é›†è®­ç»ƒå¤šå°‘ä¸ªæ‰¹æ¬¡





for name, param in model.named_parameters():
    if param.requires_grad:  # è¿™é‡Œåªæ‰“å°æœ‰æ¢¯åº¦ä¿¡æ¯çš„
        print(name)





from torch.optim import AdamW
import os

# åˆ›å»ºä¿å­˜ç›®å½•
model_save_path = "./qwen2.5-finetuned-lora"
os.makedirs(model_save_path, exist_ok=True)

# ä¼˜åŒ–å™¨è®¾ç½®
optimizer = AdamW(model.parameters(), lr=2e-4)

# è®­ç»ƒå¾ªç¯
model.train()
total_steps = len(train_loader) * num_train_epochs
current_step = 0

for epoch in range(num_train_epochs):
    epoch_loss = 0.0
    print(f"\nå¼€å§‹ç¬¬ {epoch + 1}/{num_train_epochs} è½®è®­ç»ƒ...")
    
    for step, batch in enumerate(train_loader):
        input_ids = batch["input_ids"].to(model.device)
        labels = batch["labels"].to(model.device)

        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        
        epoch_loss += loss.item()
        current_step += 1

        # æ¯ 100 ä¸ª step æ‰“å°ä¸€æ¬¡è¿›åº¦
        if step % 100 == 0:
            avg_loss = epoch_loss / (step + 1)
            print(f"Epoch {epoch + 1}/{num_train_epochs} | Step {step}/{len(train_loader)} | "
                  f"Loss {loss.item():.4f} | Avg Loss {avg_loss:.4f} | "
                  f"Progress {current_step}/{total_steps}")
    
    # æ¯è½®ç»“æŸåæ‰“å°å¹³å‡æŸå¤±
    avg_epoch_loss = epoch_loss / len(train_loader)
    print(f"ç¬¬ {epoch + 1} è½®è®­ç»ƒå®Œæˆï¼Œå¹³å‡æŸå¤±: {avg_epoch_loss:.4f}")

print("\nè®­ç»ƒå®Œæˆï¼")

# ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹
print(f"æ­£åœ¨ä¿å­˜æ¨¡å‹åˆ°: {model_save_path}")
model.save_pretrained(model_save_path)
tokenizer.save_pretrained(model_save_path)

# ä¿å­˜è®­ç»ƒé…ç½®ä¿¡æ¯
import json
config_info = {
    "model_id": model_id,
    "lora_config": {
        "r": lora_config.r,
        "lora_alpha": lora_config.lora_alpha,
        "target_modules": list(lora_config.target_modules),  # å°†setè½¬æ¢ä¸ºlist
        "lora_dropout": lora_config.lora_dropout,
        "bias": lora_config.bias,
        "task_type": lora_config.task_type
    },
    "training_config": {
        "num_epochs": num_train_epochs,
        "learning_rate": 2e-4,
        "batch_size": 4,
        "total_steps": total_steps
    }
}

with open(os.path.join(model_save_path, "training_config.json"), "w", encoding="utf-8") as f:
    json.dump(config_info, f, ensure_ascii=False, indent=2)

print(f"æ¨¡å‹å’Œé…ç½®å·²ä¿å­˜åˆ°: {model_save_path}")
print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")








from peft import PeftModel

# é‡æ–°åŠ è½½åŸºç¡€æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    trust_remote_code=True
)

# åŠ è½½ LoRA æƒé‡
finetuned_model = PeftModel.from_pretrained(base_model, model_save_path)
finetuned_model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

print("å¾®è°ƒåçš„æ¨¡å‹åŠ è½½å®Œæˆ")





from sklearn.metrics import accuracy_score
import re

def evaluate_model(model, test_loader, tokenizer):
    """
    è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°
    """
    model.eval()
    total_loss = 0
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for batch in tqdm(test_loader, desc="æµ‹è¯•ä¸­"):
            input_ids = batch["input_ids"].to(model.device)
            labels = batch["labels"].to(model.device)
            
            # è®¡ç®—æŸå¤±
            outputs = model(input_ids=input_ids, labels=labels)
            total_loss += outputs.loss.item()
            
            # ç”Ÿæˆé¢„æµ‹
            generated_ids = model.generate(
                input_ids=input_ids,
                max_new_tokens=max_length,  # æœ€å¤§ç”Ÿæˆ100ä¸ªæ–°token
                do_sample=True,
                temperature=0.7,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
            
            # è§£ç é¢„æµ‹ç»“æœå’Œç›®æ ‡
            for i in range(len(generated_ids)):
                # è·å–è¾“å…¥éƒ¨åˆ†ï¼ˆç”¨äºæå–æŒ‡ä»¤ï¼‰
                input_text = tokenizer.decode(input_ids[i], skip_special_tokens=True)
                
                # è·å–ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»æ‰è¾“å…¥éƒ¨åˆ†ï¼‰
                generated_text = tokenizer.decode(generated_ids[i][len(input_ids[i]):], skip_special_tokens=True)
                
                # è·å–ç›®æ ‡ç­”æ¡ˆ
                target_text = tokenizer.decode(labels[i][labels[i] != -100], skip_special_tokens=True)
                
                all_predictions.append(generated_text.strip())
                all_targets.append(target_text.strip())
    
    # è®¡ç®—å¹³å‡æŸå¤±
    avg_loss = total_loss / len(test_loader)
    return avg_loss, all_predictions, all_targets

# æ‰§è¡Œæµ‹è¯•
print("å¼€å§‹æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹...")
test_loss, predictions, targets = evaluate_model(finetuned_model, test_loader, tokenizer)

print(f"æµ‹è¯•é›†å¹³å‡æŸå¤±: {test_loss:.4f}")
print(f"æµ‹è¯•æ ·æœ¬æ•°: {len(predictions)}")














%pip install bert-score

from bert_score import score

# å‡è®¾ predictions å’Œ targets å·²ç»æ˜¯ä½ çš„ç”Ÿæˆç»“æœå’ŒçœŸå®ç­”æ¡ˆåˆ—è¡¨
P, R, F1 = score(predictions, targets, lang="zh", verbose=True)  # lang å¯ä»¥æ ¹æ®ä»»åŠ¡é€‰æ‹© 'en' æˆ– 'zh'

# P, R, F1 éƒ½æ˜¯ tensorï¼Œå½¢çŠ¶ä¸º [æ ·æœ¬æ•°]
avg_precision = P.mean().item()
avg_recall = R.mean().item()
avg_f1 = F1.mean().item()

print(f"BERTScore å¹³å‡ Precision: {avg_precision:.4f}")
print(f"BERTScore å¹³å‡ Recall:    {avg_recall:.4f}")
print(f"BERTScore å¹³å‡ F1:        {avg_f1:.4f}")
