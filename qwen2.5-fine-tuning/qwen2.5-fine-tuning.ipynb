{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e6565e-d437-46a9-b294-d024e162f573",
   "metadata": {},
   "source": [
    "# 🎯 Qwen2.5系列模型**LoRA/QLoRA** **微调案例**\n",
    "\n",
    "> 说明：先用一个兼容的小模型（例如 `Qwen/Qwen2.5-1.5B-Instruct`）跑通流程，后续将 `MODEL_ID` 替换为你找到的 DeepSeek 模型仓库名即可，代码无需改动。\n",
    "> \n",
    "\n",
    "**目标**：在单卡 A10（24GB）上，以 *小参数量* 的 DeepSeek 系列模型为例（本案例采用ModelScope来替换HuggingFace），用 **LoRA/QLoRA** 跑通一次完整的 *指令微调*（Instruction Tuning）流程。  \n",
    "**硬件建议**：A10 24GB；  \n",
    "**软件建议**：Python 3.10+、CUDA 12.x、PyTorch 2.3+。\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 本教程包括\n",
    "1. LoRA/QLoRA 简介\n",
    "2. 硬件检测与配置环境\n",
    "3. 模型与数据集下载  \n",
    "4. 数据预处理 \n",
    "5. LoRA微调\n",
    "6. 模型测试评估\n",
    "\n",
    "> 注：全流程都在 **Jupyter Lab** 中逐格运行即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9cb777-2b7d-4310-9259-f30e8aad9dad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 一、LoRA / QLoRA 简介"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efe3ac9-fde6-4188-a104-6fe1ad57b723",
   "metadata": {},
   "source": [
    "### LoRA（Low-Rank Adaptation）\n",
    "LoRA 是一种 **轻量化模型微调方法**，它的核心思想是：  \n",
    "- 在保持原始预训练模型参数 **冻结不变** 的前提下，只在部分权重矩阵（通常是 Transformer 的注意力层）上引入 **低秩矩阵分解**。  \n",
    "- 用一个低秩的参数矩阵（A、B）来近似原始大矩阵的更新，从而 **大幅减少训练参数量**。  \n",
    "- 优点：  \n",
    "  - **参数高效**：只需训练极少量的新增参数（可低至 0.1%）。  \n",
    "  - **存储友好**：多个下游任务可以共享同一个基础模型，仅保存不同任务的 LoRA 权重。  \n",
    "  - **部署灵活**：推理时直接将 LoRA 权重合并到原模型，无需额外计算开销。\n",
    "\n",
    "> 简单理解：LoRA 就像是在大模型的“固定主干”上，插入一些 **小而聪明的适配器**，让它快速学会新任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec29c50-b0e1-4548-a630-f06b887a8ce1",
   "metadata": {},
   "source": [
    "### QLoRA（Quantized LoRA）本文不涉及\n",
    "QLoRA 是对 LoRA 的进一步优化，它结合了 **量化技术**，使得大模型的微调在 **单卡消费级显卡** 上也可行。  \n",
    "- 核心思路：  \n",
    "  1. 先将大模型的参数进行 **4-bit 量化（NF4 方案）**，降低显存占用。  \n",
    "  2. 在量化后的权重上，应用 **LoRA 适配器** 进行微调。  \n",
    "  3. 训练时仅更新 LoRA 层，而量化权重保持冻结。  \n",
    "\n",
    "- 优点：  \n",
    "  - **极致显存节省**：可在一张 24GB 显存的 GPU 上微调百亿参数模型。  \n",
    "  - **保持性能**：量化后的 QLoRA 与全精度微调效果接近甚至相当。  \n",
    "  - **实用性强**：特别适合个人开发者和中小团队。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d7f704-0602-4eee-974c-f7edbe51c71e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 对比总结\n",
    "| 方法   | 主要手段                   | 显存消耗 | 训练参数量 | 适用场景 |\n",
    "|--------|---------------------------|----------|------------|----------|\n",
    "| LoRA   | 低秩矩阵分解               | 较低     | 千万级别   | 中等规模模型的高效微调 |\n",
    "| QLoRA  | 量化（4-bit） + LoRA 适配 | 极低     | 千万级别   | 超大模型在消费级 GPU 上的微调 |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfe81e3-9470-422f-b479-7b118f1bbd03",
   "metadata": {},
   "source": [
    "## 二、硬件检测与配置环境"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80687ecd-4115-4970-acd4-135208628c36",
   "metadata": {},
   "source": [
    "### 硬件检测（Linux）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b0e2c0-e622-4ee7-bb91-b6e53130fb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import sys\n",
    "\n",
    "print(\"操作系统:\", platform.system())          # Windows / Linux / Darwin (macOS)\n",
    "print(\"系统版本:\", platform.version())         # 内核或版本号\n",
    "print(\"发行版本:\", platform.release())         # 例如 10 / 11 / 22.6.0\n",
    "print(\"详细信息:\", platform.platform())        # 汇总\n",
    "print(\"Python版本:\", sys.version)             # Python 解释器版本\n",
    "print(\"处理器:\", platform.processor())         # CPU 信息\n",
    "print(\"机器类型:\", platform.machine())         # x86_64 / arm64\n",
    "\n",
    "# 安装检测显卡的依赖\n",
    "%pip install gputil\n",
    "\n",
    "import GPUtil\n",
    "\n",
    "gpus = GPUtil.getGPUs()\n",
    "for gpu in gpus:\n",
    "    print(f\"显卡型号: {gpu.name}\")\n",
    "    print(f\"显存总量: {gpu.memoryTotal} MB\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c85d87-cf4c-4461-a35f-ae0ed02b6c11",
   "metadata": {},
   "source": [
    "### 配置环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e562bedc-2f0d-4ee4-9fec-61263667dce9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install torch==2.3.1+cu121\n",
    "%pip install transformers==4.55.4\n",
    "%pip install modelscope==1.29.0\n",
    "%pip install peft==0.17.1\n",
    "%pip install datasets==3.2.0\n",
    "%pip install accelerate==1.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ae0ef2-b29b-47c0-b000-2377b64d2c4e",
   "metadata": {},
   "source": [
    "### 检查环境版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cad49f-9ba7-4f2a-b5b3-51f63735617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📌 打印脚本相关库的版本信息\n",
    "import torch, transformers, modelscope, peft\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "\n",
    "# transformers 是 peft 和 modelscope 依赖的核心库\n",
    "try:\n",
    "    import transformers\n",
    "    print(\"transformers:\", transformers.__version__)\n",
    "except ImportError:\n",
    "    print(\"transformers: 未安装\")\n",
    "\n",
    "try:\n",
    "    import modelscope\n",
    "    print(\"modelscope:\", modelscope.__version__)\n",
    "except ImportError:\n",
    "    print(\"modelscope: 未安装\")\n",
    "\n",
    "try:\n",
    "    import peft\n",
    "    print(\"peft:\", peft.__version__)\n",
    "except ImportError:\n",
    "    print(\"peft: 未安装\")\n",
    "\n",
    "try:\n",
    "    import datasets\n",
    "    print(\"datasets:\", datasets.__version__)\n",
    "except ImportError:\n",
    "    print(\"datasets: 未安装\")\n",
    "\n",
    "try:\n",
    "    import accelerate\n",
    "    print(\"accelerate:\", accelerate.__version__)\n",
    "except ImportError:\n",
    "    print(\"accelerate: 未安装\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97315bf-cb93-4ceb-8bd6-6da5da3748af",
   "metadata": {},
   "source": [
    "## 三、模型与数据集下载"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfaeffd-b73a-491e-8273-a8db6a069535",
   "metadata": {},
   "source": [
    "### Qwen2.5-1.5B模型下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e89730e-3b76-47ed-9c85-06672b867adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"qwen/Qwen2.5-1.5B-Instruct\"  # 可替换\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d92ea8-11cc-49a8-8df6-9cb7bf3df8e5",
   "metadata": {},
   "source": [
    "### alpaca数据集下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea33c99f-f5fa-4115-b4dc-e3000b5f2f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope.msdatasets import MsDataset\n",
    "\n",
    "ds =  MsDataset.load('AI-ModelScope/alpaca-gpt4-data-zh', subset_name='default', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef368533-dbe1-42ed-a8c2-2f07509923d8",
   "metadata": {},
   "source": [
    "## 四、数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eb9f6d-22ef-404d-8649-2910e5e9911b",
   "metadata": {},
   "source": [
    "### 计算max_length\n",
    "根据数据集的token长度来计算最合适的长度，有利于数据预处理的速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4135b13f-365d-4a47-9f6d-67b06e651009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_max_length(dataset, tokenizer, sample_size=5000, quantile=95):\n",
    "    \"\"\"\n",
    "    自动统计 token 长度分布，并推荐 max_length\n",
    "    Args:\n",
    "        dataset: MsDataset 对象\n",
    "        tokenizer: HF AutoTokenizer\n",
    "        sample_size: 抽样数量（避免全量太慢）\n",
    "        quantile: 分位数（默认95）\n",
    "    \"\"\"\n",
    "    total = min(sample_size, len(dataset))\n",
    "    lengths = []\n",
    "\n",
    "    print(f\"开始统计，抽样 {total} 条数据 ...\")\n",
    "\n",
    "    for i in tqdm(range(total)):\n",
    "        ex = dataset[i]\n",
    "        instruction = ex.get(\"instruction\", \"\")\n",
    "        input_text = ex.get(\"input\", \"\") or \"\"\n",
    "        output_text = ex.get(\"output\", \"\")\n",
    "\n",
    "        if input_text.strip():\n",
    "            prompt = f\"指令: {instruction}\\n输入: {input_text}\\n回答:\"\n",
    "        else:\n",
    "            prompt = f\"指令: {instruction}\\n回答:\"\n",
    "\n",
    "        full_text = prompt + output_text\n",
    "        tokenized = tokenizer(full_text, truncation=False)\n",
    "        lengths.append(len(tokenized[\"input_ids\"]))\n",
    "\n",
    "    max_len = max(lengths)\n",
    "    avg_len = np.mean(lengths)\n",
    "    q_len = np.percentile(lengths, quantile)\n",
    "\n",
    "    print(\"\\n=== Token 长度统计结果 ===\")\n",
    "    print(f\"最大长度: {max_len}\")\n",
    "    print(f\"平均长度: {avg_len:.2f}\")\n",
    "    print(f\"{quantile}% 分位数长度: {q_len:.0f}\")\n",
    "    print(\"=========================\")\n",
    "    print(f\"👉 推荐 max_length = {int(min(q_len, tokenizer.model_max_length))}\")\n",
    "    print(f\"(模型支持的最大长度 = {tokenizer.model_max_length})\")\n",
    "\n",
    "    return int(min(q_len, tokenizer.model_max_length))\n",
    "\n",
    "\n",
    "# 计算数据集应该定义的最大长度\n",
    "max_length = recommend_max_length(ds, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53410742-ec2d-4383-ad76-43cb6b997ca4",
   "metadata": {},
   "source": [
    "### 预处理（构造Dataset）\n",
    "需要针对自己的数据集范式来编写，这里只针对alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f40f313-12aa-4dab-a9aa-7a2197706475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义数据处理函数(需要针对自己的数据集范式来编写，这里只针对alpaca)\n",
    "def preprocess(example):\n",
    "    # 丢掉 instruction 或 output 缺失的样本\n",
    "    if not example['instruction'] or not example['output']:\n",
    "        return None\n",
    "\n",
    "    # alpaca 数据有指令、输入、输出三个标签\n",
    "    instruction = example['instruction']\n",
    "    input_text = example.get('input') or \"\"  # input 可能为 None\n",
    "    output_text = example['output']\n",
    "\n",
    "    if input_text.strip():\n",
    "        prompt = f\"指令: {instruction}\\n输入: {input_text}\\n回答:\"\n",
    "    else:\n",
    "        prompt = f\"指令: {instruction}\\n回答:\"\n",
    "\n",
    "    full_text = prompt + output_text\n",
    "\n",
    "    enc = tokenizer(\n",
    "        full_text,  # 需要进行token化的文本\n",
    "        truncation=True,  # 文本过大的时候是否截断\n",
    "        max_length=max_length,  # 根据模型和数据集决定最合适的\n",
    "        padding=\"max_length\",  # 🔹 保证长度一致，DataLoader 堆叠安全\n",
    "        return_tensors=\"pt\"  # 返回的数据类型，pt:pytorch.tensor; tf:tensorflow; np:numpy\n",
    "    )\n",
    "    # 单个样本是字典格式\n",
    "    return {\n",
    "        \"input_ids\": enc[\"input_ids\"][0],\n",
    "        \"labels\": enc[\"input_ids\"][0]\n",
    "    }\n",
    "\n",
    "train_dataset = ds.map(preprocess)\n",
    "train_dataset = train_dataset.filter(lambda x: x is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709d17b5-dcff-4b59-949c-3bdccc212392",
   "metadata": {},
   "source": [
    "### 构造DataLoader\n",
    "将预处理好的Dataset进行Padding后方便拼接成多个batch的向量，为训练作准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee1615a-3038-4ffc-9eb7-7f42a0188d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# 🛠️ 自定义批处理函数 (collate_fn)\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    作用：\n",
    "    - DataLoader 会把一个 batch 的样本（list[dict]）传进来\n",
    "    - 这里需要手动拼接成 tensor，并且对齐长度（pad）\n",
    "    \"\"\"\n",
    "\n",
    "    # 取出每个样本的 input_ids 和 labels，转成 tensor\n",
    "    input_ids = [torch.tensor(item[\"input_ids\"]) for item in batch]\n",
    "    labels = [torch.tensor(item[\"labels\"]) for item in batch]\n",
    "\n",
    "    # 🔹 对 input_ids 做 padding\n",
    "    #   - batch_first=True: 结果形状 (batch_size, seq_len)\n",
    "    #   - padding_value=tokenizer.pad_token_id: 使用 tokenizer 的 pad_token_id 填充\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    # 🔹 对 labels 做 padding\n",
    "    #   - 注意这里 padding_value = -100\n",
    "    #   - 在 PyTorch 的 CrossEntropyLoss 里，-100 会被忽略，不参与 loss 计算\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(\n",
    "        labels, batch_first=True, padding_value=-100\n",
    "    )\n",
    "\n",
    "    # 返回字典，方便直接喂给模型\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "\n",
    "# 在数据预处理后，分割训练集和测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# 获取数据集的总长度\n",
    "total_samples = len(train_dataset)\n",
    "print(f\"总样本数: {total_samples}\")\n",
    "\n",
    "# 设置随机种子确保可重复性\n",
    "np.random.seed(42)\n",
    "\n",
    "# 分割训练集和测试集 (80% 训练, 20% 测试)\n",
    "train_indices, test_indices = train_test_split(\n",
    "    range(total_samples), \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 创建训练集和测试集\n",
    "train_subset = Subset(train_dataset, train_indices)\n",
    "test_subset = Subset(train_dataset, test_indices)\n",
    "\n",
    "print(f\"训练集样本数: {len(train_subset)}\")\n",
    "print(f\"测试集样本数: {len(test_subset)}\")\n",
    "\n",
    "# 为了快速验证，可以只取部分数据\n",
    "small_train_dataset = Subset(train_subset, range(min(2000, len(train_subset))))\n",
    "small_test_dataset = Subset(test_subset, range(min(500, len(test_subset))))\n",
    "\n",
    "print(f\"小训练集样本数: {len(small_train_dataset)}\")\n",
    "print(f\"小测试集样本数: {len(small_test_dataset)}\")\n",
    "\n",
    "# 构建训练和测试的 DataLoader\n",
    "train_loader = DataLoader(\n",
    "    small_train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    small_test_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,  # 测试时不需要打乱\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb3096f-ff18-4add-b275-678d51aea408",
   "metadata": {},
   "source": [
    "## 五、LoRA微调"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76fe51a-3b1d-44d2-aba4-a1bd46903665",
   "metadata": {},
   "source": [
    "### LoRA参数说明"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7645b00-73b5-46f9-83fe-ad0145c55b83",
   "metadata": {},
   "source": [
    "```python\n",
    "lora_config = LoraConfig(\n",
    "    r=4,                          \n",
    "    lora_alpha=16,               \n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  \n",
    "    lora_dropout=0.05,             \n",
    "    bias=\"none\",               \n",
    "    task_type=\"CAUSAL_LM\"         \n",
    ")\n",
    "```\n",
    "\n",
    "r=4\n",
    "- 表示低秩矩阵的秩值（rank），值越大 → 适配能力更强 → 参数量也随之增加。  \n",
    "- 这里选择 `4`，意味着 **轻量级训练**，适合小规模任务或快速实验。  \n",
    "\n",
    "lora_alpha=16\n",
    "- 缩放因子，用于调整 LoRA 的输出幅度。  \n",
    "- 一般经验是 **lora_alpha ≈ 2 × r**，所以这里 `16` 配合 `r=4` 是合理的。  \n",
    "\n",
    "target_modules=[\"q_proj\", \"v_proj\"]\n",
    "- LoRA 只在注意力机制的 **Query** 和 **Value** 投影层中生效。  \n",
    "- 这是最常见的设置，既保证效果，又控制参数量。\n",
    "- [\"q_proj\", \"v_proj\"] → 默认推荐，90% 的场景适用（对话、问答、指令跟随）。\n",
    "- [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"] → 全量 Attention，参数量大一些，适合任务更复杂（如多模态对齐）。\n",
    "- [\"gate_proj\", \"up_proj\", \"down_proj\"] → 改 MLP 层，适合需要调整“知识表达”的场景（例如数学推理）。\n",
    "- [\"q_proj\", \"v_proj\", \"down_proj\"] → 混合方式，有时能进一步提升性能。\n",
    "\n",
    "lora_dropout=0.05\n",
    "- 在 LoRA 层中添加 **5% 的 dropout**，提升泛化能力。  \n",
    "- 数据量很大时可以调低到 `0`；数据少时可以适当调高（如 `0.1`）。  \n",
    "\n",
    "bias=\"none\"\n",
    "- 不训练 bias 参数，保证模型轻量化。  \n",
    "- 大多数场景下用 `\"none\"` 即可。  \n",
    "\n",
    "task_type=\"CAUSAL_LM\"\n",
    "- 表示任务是 **自回归语言建模**（比如 Qwen、GPT 类模型）。  \n",
    "- 必须和任务类型一致，否则 forward 过程会报错。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da25a924-a0ca-4779-9006-59be35ecb388",
   "metadata": {},
   "source": [
    "### 配置 LoRA 训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f547e8-9520-40aa-a9dd-3aeef97f305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=4,            \n",
    "    lora_alpha=16,                \n",
    "    target_modules=[\"q_proj\", \"v_proj\"], \n",
    "    lora_dropout=0.05,             \n",
    "    bias=\"none\",                    \n",
    "    task_type=\"CAUSAL_LM\"           \n",
    ")\n",
    "\n",
    "# 🚀 将基础模型包装为 PEFT 模型\n",
    "model = get_peft_model(model, lora_config)  # 这里默认会冻结非LoRA的参数\n",
    "\n",
    "# 打印当前可训练参数量（仅 LoRA 部分），其余参数被冻结\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# 训练超参数\n",
    "num_train_epochs = 2  # 对完整的数据集训练多少个批次"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be6e3f0-b17b-4001-9884-b41af16157b2",
   "metadata": {},
   "source": [
    "### 确认参数是否冻结\n",
    "这里只需要训练LoRA新增的参数层\n",
    "输出应该只包含 lora_A、lora_B 之类的增量参数名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08575633-4676-4e2d-94dd-aba87aff7062",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:  # 这里只打印有梯度信息的\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb750b2e-e557-4946-90d3-254efded1b21",
   "metadata": {},
   "source": [
    "### 开始微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bd5b04-8876-4438-b711-db19c396adfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "import os\n",
    "\n",
    "# 创建保存目录\n",
    "model_save_path = \"./qwen2.5-finetuned-lora\"\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "# 优化器设置\n",
    "optimizer = AdamW(model.parameters(), lr=2e-4)\n",
    "\n",
    "# 训练循环\n",
    "model.train()\n",
    "total_steps = len(train_loader) * num_train_epochs\n",
    "current_step = 0\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    print(f\"\\n开始第 {epoch + 1}/{num_train_epochs} 轮训练...\")\n",
    "    \n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(model.device)\n",
    "        labels = batch[\"labels\"].to(model.device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        current_step += 1\n",
    "\n",
    "        # 每 100 个 step 打印一次进度\n",
    "        if step % 100 == 0:\n",
    "            avg_loss = epoch_loss / (step + 1)\n",
    "            print(f\"Epoch {epoch + 1}/{num_train_epochs} | Step {step}/{len(train_loader)} | \"\n",
    "                  f\"Loss {loss.item():.4f} | Avg Loss {avg_loss:.4f} | \"\n",
    "                  f\"Progress {current_step}/{total_steps}\")\n",
    "    \n",
    "    # 每轮结束后打印平均损失\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"第 {epoch + 1} 轮训练完成，平均损失: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "print(\"\\n训练完成！\")\n",
    "\n",
    "# 保存微调后的模型\n",
    "print(f\"正在保存模型到: {model_save_path}\")\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "# 保存训练配置信息\n",
    "import json\n",
    "config_info = {\n",
    "    \"model_id\": model_id,\n",
    "    \"lora_config\": {\n",
    "        \"r\": lora_config.r,\n",
    "        \"lora_alpha\": lora_config.lora_alpha,\n",
    "        \"target_modules\": list(lora_config.target_modules),  # 将set转换为list\n",
    "        \"lora_dropout\": lora_config.lora_dropout,\n",
    "        \"bias\": lora_config.bias,\n",
    "        \"task_type\": lora_config.task_type\n",
    "    },\n",
    "    \"training_config\": {\n",
    "        \"num_epochs\": num_train_epochs,\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"batch_size\": 4,\n",
    "        \"total_steps\": total_steps\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(model_save_path, \"training_config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config_info, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"模型和配置已保存到: {model_save_path}\")\n",
    "print(f\"可训练参数数量: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2484e090-c227-419b-a238-a5f5e1e585e6",
   "metadata": {},
   "source": [
    "## 六、模型测试评估"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4243242d-2ce8-453d-920f-1d8236b9bf8e",
   "metadata": {},
   "source": [
    "### 模型加载\n",
    "加载微调后的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b0715e-2d47-499a-98b9-5cf691f40062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# 重新加载基础模型\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 加载 LoRA 权重\n",
    "finetuned_model = PeftModel.from_pretrained(base_model, model_save_path)\n",
    "finetuned_model.eval()  # 设置为评估模式\n",
    "\n",
    "print(\"微调后的模型加载完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41deefc-fed1-4f6d-ac20-e7098efe4580",
   "metadata": {},
   "source": [
    "### 测试\n",
    "用部份未经过训练的数据集进行测试，这里采用的是交叉熵损失（CrossEntropyLoss）  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cae18c1-5a64-494c-8742-9d5d35df3bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "\n",
    "def evaluate_model(model, test_loader, tokenizer):\n",
    "    \"\"\"\n",
    "    评估模型在测试集上的表现\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"测试中\"):\n",
    "            input_ids = batch[\"input_ids\"].to(model.device)\n",
    "            labels = batch[\"labels\"].to(model.device)\n",
    "            \n",
    "            # 计算损失\n",
    "            outputs = model(input_ids=input_ids, labels=labels)\n",
    "            total_loss += outputs.loss.item()\n",
    "            \n",
    "            # 生成预测\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_new_tokens=max_length,  # 最大生成100个新token\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            # 解码预测结果和目标\n",
    "            for i in range(len(generated_ids)):\n",
    "                # 获取输入部分（用于提取指令）\n",
    "                input_text = tokenizer.decode(input_ids[i], skip_special_tokens=True)\n",
    "                \n",
    "                # 获取生成的部分（去掉输入部分）\n",
    "                generated_text = tokenizer.decode(generated_ids[i][len(input_ids[i]):], skip_special_tokens=True)\n",
    "                \n",
    "                # 获取目标答案\n",
    "                target_text = tokenizer.decode(labels[i][labels[i] != -100], skip_special_tokens=True)\n",
    "                \n",
    "                all_predictions.append(generated_text.strip())\n",
    "                all_targets.append(target_text.strip())\n",
    "    \n",
    "    # 计算平均损失\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    return avg_loss, all_predictions, all_targets\n",
    "\n",
    "# 执行测试\n",
    "print(\"开始测试微调后的模型...\")\n",
    "test_loss, predictions, targets = evaluate_model(finetuned_model, test_loader, tokenizer)\n",
    "\n",
    "print(f\"测试集平均损失: {test_loss:.4f}\")\n",
    "print(f\"测试样本数: {len(predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ce1c79-b8fb-491c-93ab-fc38c8cc87f6",
   "metadata": {},
   "source": [
    "### LLM模型其他常见指标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc16187a-deba-4cd8-a422-23a785d0411b",
   "metadata": {},
   "source": [
    "| 指标                   | 说明                  | 适用场景        |\n",
    "| -------------------- | ------------------- | ----------- |\n",
    "| **BLEU**             | n-gram 精确匹配         | 翻译、摘要       |\n",
    "| **ROUGE**            | 关注 recall（覆盖率）      | 摘要生成、问答     |\n",
    "| **METEOR**           | 考虑同义词匹配             | 文本生成        |\n",
    "| **BERTScore**        | 基于语义嵌入比较文本相似度       | 生成任务，语义评价   |\n",
    "| **Exact Match (EM)** | 完全匹配                | 问答、选择题、逻辑输出 |\n",
    "| **Perplexity (困惑度)** | 模型预测 token 的概率反映流畅度 | 文本生成、语言建模   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beba284-8802-4cba-b5b5-44dde5215d2f",
   "metadata": {},
   "source": [
    "### 计算BERTScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b19093e-5068-458e-80fc-fa56fd371b1c",
   "metadata": {},
   "source": [
    "| 指标                | 含义               | 值域     | 越大越好？  |\n",
    "| ----------------- | ---------------- | ------ | ------ |\n",
    "| **Precision (P)** | 生成文本和目标文本语义重叠的比例 | 0 \\~ 1 | ✅ 越大越好 |\n",
    "| **Recall (R)**    | 目标文本中被生成文本覆盖的比例  | 0 \\~ 1 | ✅ 越大越好 |\n",
    "| **F1-score (F1)** | P 和 R 的调和平均      | 0 \\~ 1 | ✅ 越大越好 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2424cfee-7520-4949-88ff-f33b513603da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bert-score\n",
    "\n",
    "from bert_score import score\n",
    "\n",
    "# 假设 predictions 和 targets 已经是你的生成结果和真实答案列表\n",
    "P, R, F1 = score(predictions, targets, lang=\"en\", verbose=True)  # lang 可以根据任务选择 'en' 或 'zh'\n",
    "\n",
    "# P, R, F1 都是 tensor，形状为 [样本数]\n",
    "avg_precision = P.mean().item()\n",
    "avg_recall = R.mean().item()\n",
    "avg_f1 = F1.mean().item()\n",
    "\n",
    "print(f\"BERTScore 平均 Precision: {avg_precision:.4f}\")\n",
    "print(f\"BERTScore 平均 Recall:    {avg_recall:.4f}\")\n",
    "print(f\"BERTScore 平均 F1:        {avg_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
