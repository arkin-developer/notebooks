{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e6565e-d437-46a9-b294-d024e162f573",
   "metadata": {},
   "source": [
    "# 🎯 Qwen2.5系列模型**LoRA/QLoRA** **微调案例**\n",
    "\n",
    "> 说明：先用一个兼容的小模型（例如 `Qwen/Qwen2.5-1.5B-Instruct`）跑通流程，后续将 `MODEL_ID` 替换为你找到的 DeepSeek 模型仓库名即可，代码无需改动。\n",
    "> \n",
    "\n",
    "**目标**：在单卡 A10（24GB）上，以 *小参数量* 的 DeepSeek 系列模型为例（本案例采用ModelScope来替换HuggingFace），用 **LoRA/QLoRA** 跑通一次完整的 *指令微调*（Instruction Tuning）流程。  \n",
    "**硬件建议**：A10 24GB；  \n",
    "**软件建议**：Python 3.10+、CUDA 12.x、PyTorch 2.3+。\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 本教程包括\n",
    "1. LoRA/QLoRA 简介\n",
    "2. 安装依赖与环境检测  \n",
    "3. 选择模型与数据集（以 `Alpaca` 为经典示例）  \n",
    "4. 数据预处理与 `chat_template` 适配  \n",
    "5. 用 `bitsandbytes` + `peft` + `trl` 进行 **LoRA/QLoRA** 微调  \n",
    "6. 保存与合并权重、推理验证  \n",
    "\n",
    "> 注：全流程都在 **Jupyter Lab** 中逐格运行即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9cb777-2b7d-4310-9259-f30e8aad9dad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 一、LoRA / QLoRA 简介"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efe3ac9-fde6-4188-a104-6fe1ad57b723",
   "metadata": {},
   "source": [
    "### LoRA（Low-Rank Adaptation）\n",
    "LoRA 是一种 **轻量化模型微调方法**，它的核心思想是：  \n",
    "- 在保持原始预训练模型参数 **冻结不变** 的前提下，只在部分权重矩阵（通常是 Transformer 的注意力层）上引入 **低秩矩阵分解**。  \n",
    "- 用一个低秩的参数矩阵（A、B）来近似原始大矩阵的更新，从而 **大幅减少训练参数量**。  \n",
    "- 优点：  \n",
    "  - **参数高效**：只需训练极少量的新增参数（可低至 0.1%）。  \n",
    "  - **存储友好**：多个下游任务可以共享同一个基础模型，仅保存不同任务的 LoRA 权重。  \n",
    "  - **部署灵活**：推理时直接将 LoRA 权重合并到原模型，无需额外计算开销。\n",
    "\n",
    "> 简单理解：LoRA 就像是在大模型的“固定主干”上，插入一些 **小而聪明的适配器**，让它快速学会新任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec29c50-b0e1-4548-a630-f06b887a8ce1",
   "metadata": {},
   "source": [
    "### QLoRA（Quantized LoRA）\n",
    "QLoRA 是对 LoRA 的进一步优化，它结合了 **量化技术**，使得大模型的微调在 **单卡消费级显卡** 上也可行。  \n",
    "- 核心思路：  \n",
    "  1. 先将大模型的参数进行 **4-bit 量化（NF4 方案）**，降低显存占用。  \n",
    "  2. 在量化后的权重上，应用 **LoRA 适配器** 进行微调。  \n",
    "  3. 训练时仅更新 LoRA 层，而量化权重保持冻结。  \n",
    "\n",
    "- 优点：  \n",
    "  - **极致显存节省**：可在一张 24GB 显存的 GPU 上微调百亿参数模型。  \n",
    "  - **保持性能**：量化后的 QLoRA 与全精度微调效果接近甚至相当。  \n",
    "  - **实用性强**：特别适合个人开发者和中小团队。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d7f704-0602-4eee-974c-f7edbe51c71e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 对比总结\n",
    "| 方法   | 主要手段                   | 显存消耗 | 训练参数量 | 适用场景 |\n",
    "|--------|---------------------------|----------|------------|----------|\n",
    "| LoRA   | 低秩矩阵分解               | 较低     | 千万级别   | 中等规模模型的高效微调 |\n",
    "| QLoRA  | 量化（4-bit） + LoRA 适配 | 极低     | 千万级别   | 超大模型在消费级 GPU 上的微调 |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfe81e3-9470-422f-b479-7b118f1bbd03",
   "metadata": {},
   "source": [
    "## 二、安装依赖与环境检测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80687ecd-4115-4970-acd4-135208628c36",
   "metadata": {},
   "source": [
    "### 安装依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00940b3-620a-4cde-8a9f-c5bdd0df422b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4ae0ef2-b29b-47c0-b000-2377b64d2c4e",
   "metadata": {},
   "source": [
    "### 环境版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cad49f-9ba7-4f2a-b5b3-51f63735617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📌 打印脚本相关库的版本信息\n",
    "import torch, transformers, modelscope, peft\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "\n",
    "# transformers 是 peft 和 modelscope 依赖的核心库\n",
    "try:\n",
    "    import transformers\n",
    "    print(\"transformers:\", transformers.__version__)\n",
    "except ImportError:\n",
    "    print(\"transformers: 未安装\")\n",
    "\n",
    "try:\n",
    "    import modelscope\n",
    "    print(\"modelscope:\", modelscope.__version__)\n",
    "except ImportError:\n",
    "    print(\"modelscope: 未安装\")\n",
    "\n",
    "try:\n",
    "    import peft\n",
    "    print(\"peft:\", peft.__version__)\n",
    "except ImportError:\n",
    "    print(\"peft: 未安装\")\n",
    "\n",
    "try:\n",
    "    import datasets\n",
    "    print(\"datasets:\", datasets.__version__)\n",
    "except ImportError:\n",
    "    print(\"datasets: 未安装\")\n",
    "\n",
    "try:\n",
    "    import accelerate\n",
    "    print(\"accelerate:\", accelerate.__version__)\n",
    "except ImportError:\n",
    "    print(\"accelerate: 未安装\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97315bf-cb93-4ceb-8bd6-6da5da3748af",
   "metadata": {},
   "source": [
    "## 三、下载模型和数据集（ModelScope版本）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfaeffd-b73a-491e-8273-a8db6a069535",
   "metadata": {},
   "source": [
    "### 模型下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e89730e-3b76-47ed-9c85-06672b867adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%modelscope` not found.\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"qwen/Qwen2.5-1.5B-Instruct\"  # 可替换\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d92ea8-11cc-49a8-8df6-9cb7bf3df8e5",
   "metadata": {},
   "source": [
    "### 数据集下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea33c99f-f5fa-4115-b4dc-e3000b5f2f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope.msdatasets import MsDataset\n",
    "\n",
    "# 选择alpaca的中文数据集\n",
    "ds = MsDataset.load(\"alpaca-gpt4-data-zh\", namespace=\"AI-ModelScope\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef368533-dbe1-42ed-a8c2-2f07509923d8",
   "metadata": {},
   "source": [
    "## 四、数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f40f313-12aa-4dab-a9aa-7a2197706475",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 自定义数据处理函数(需要针对自己的数据集范式来编写，这里只针对alpaca)\n",
    "def preprocess(example):\n",
    "    # 丢掉 instruction 或 output 缺失的样本\n",
    "    if not example['instruction'] or not example['output']:\n",
    "        return None\n",
    "\n",
    "    # alpaca 数据有指令、输入、输出三个标签\n",
    "    instruction = example['instruction']\n",
    "    input_text = example.get('input') or \"\"  # input 可能为 None\n",
    "    output_text = example['output']\n",
    "\n",
    "    if input_text.strip():\n",
    "        prompt = f\"指令: {instruction}\\n输入: {input_text}\\n回答:\"\n",
    "    else:\n",
    "        prompt = f\"指令: {instruction}\\n回答:\"\n",
    "\n",
    "    full_text = prompt + output_text\n",
    "\n",
    "    enc = tokenizer(\n",
    "        full_text,  # 需要进行token化的文本\n",
    "        truncation=True,  # 文本过大的时候是否截断\n",
    "        max_length=16000,  # 根据模型和数据集决定，模型的上下文, 32k甚至更大\n",
    "        padding=\"max_length\",  # 🔹 保证长度一致，DataLoader 堆叠安全\n",
    "        return_tensors=\"pt\"  # 返回的数据类型，pt:pytorch.tensor; tf:tensorflow; np:numpy\n",
    "    )\n",
    "    # 单个样本是字典格式\n",
    "    return {\n",
    "        \"input_ids\": enc[\"input_ids\"][0],\n",
    "        \"labels\": enc[\"input_ids\"][0]\n",
    "    }\n",
    "\n",
    "train_dataset = ds.map(preprocess)\n",
    "train_dataset = train_dataset.filter(lambda x: x is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709d17b5-dcff-4b59-949c-3bdccc212392",
   "metadata": {},
   "source": [
    "### 将Dataset转化成DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee1615a-3038-4ffc-9eb7-7f42a0188d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# 🛠️ 自定义批处理函数 (collate_fn)\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    作用：\n",
    "    - DataLoader 会把一个 batch 的样本（list[dict]）传进来\n",
    "    - 这里需要手动拼接成 tensor，并且对齐长度（pad）\n",
    "    \"\"\"\n",
    "\n",
    "    # 取出每个样本的 input_ids 和 labels，转成 tensor\n",
    "    input_ids = [torch.tensor(item[\"input_ids\"]) for item in batch]\n",
    "    labels = [torch.tensor(item[\"labels\"]) for item in batch]\n",
    "\n",
    "    # 🔹 对 input_ids 做 padding\n",
    "    #   - batch_first=True: 结果形状 (batch_size, seq_len)\n",
    "    #   - padding_value=tokenizer.pad_token_id: 使用 tokenizer 的 pad_token_id 填充\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    # 🔹 对 labels 做 padding\n",
    "    #   - 注意这里 padding_value = -100\n",
    "    #   - 在 PyTorch 的 CrossEntropyLoss 里，-100 会被忽略，不参与 loss 计算\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(\n",
    "        labels, batch_first=True, padding_value=-100\n",
    "    )\n",
    "\n",
    "    # 返回字典，方便直接喂给模型\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "\n",
    "# 📊 数据子集（仅用于测试）\n",
    "# 这里为了快速验证流程，只取前 2000 条样本来训练\n",
    "small_dataset = Subset(train_dataset, range(2000))\n",
    "\n",
    "# 构建 DataLoader\n",
    "train_loader = DataLoader(\n",
    "    small_dataset,\n",
    "    batch_size=4,        # 每次取 4 个样本\n",
    "    shuffle=True,        # 打乱数据顺序\n",
    "    collate_fn=collate_fn  # 使用我们自定义的 batch 拼接逻辑\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb3096f-ff18-4add-b275-678d51aea408",
   "metadata": {},
   "source": [
    "## 五、LoRA微调"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7645b00-73b5-46f9-83fe-ad0145c55b83",
   "metadata": {},
   "source": [
    "### 🔧 配置 LoRA 训练参数\n",
    "\n",
    "```python\n",
    "lora_config = LoraConfig(\n",
    "    r=4,                          \n",
    "    lora_alpha=16,               \n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  \n",
    "    lora_dropout=0.05,             \n",
    "    bias=\"none\",               \n",
    "    task_type=\"CAUSAL_LM\"         \n",
    ")\n",
    "```\n",
    "\n",
    "r=4\n",
    "- 表示低秩矩阵的秩值（rank），值越大 → 适配能力更强 → 参数量也随之增加。  \n",
    "- 这里选择 `4`，意味着 **轻量级训练**，适合小规模任务或快速实验。  \n",
    "\n",
    "lora_alpha=16\n",
    "- 缩放因子，用于调整 LoRA 的输出幅度。  \n",
    "- 一般经验是 **lora_alpha ≈ 2 × r**，所以这里 `16` 配合 `r=4` 是合理的。  \n",
    "\n",
    "target_modules=[\"q_proj\", \"v_proj\"]\n",
    "- LoRA 只在注意力机制的 **Query** 和 **Value** 投影层中生效。  \n",
    "- 这是最常见的设置，既保证效果，又控制参数量。  \n",
    "\n",
    "lora_dropout=0.05\n",
    "- 在 LoRA 层中添加 **5% 的 dropout**，提升泛化能力。  \n",
    "- 数据量很大时可以调低到 `0`；数据少时可以适当调高（如 `0.1`）。  \n",
    "\n",
    "bias=\"none\"\n",
    "- 不训练 bias 参数，保证模型轻量化。  \n",
    "- 大多数场景下用 `\"none\"` 即可。  \n",
    "\n",
    "task_type=\"CAUSAL_LM\"\n",
    "- 表示任务是 **自回归语言建模**（比如 Qwen、GPT 类模型）。  \n",
    "- 必须和任务类型一致，否则 forward 过程会报错。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f547e8-9520-40aa-a9dd-3aeef97f305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=4,            \n",
    "    lora_alpha=16,                \n",
    "    target_modules=[\"q_proj\", \"v_proj\"], \n",
    "    lora_dropout=0.05,             \n",
    "    bias=\"none\",                    \n",
    "    task_type=\"CAUSAL_LM\"           \n",
    ")\n",
    "\n",
    "# 🚀 将基础模型包装为 PEFT 模型\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 打印当前可训练参数量（仅 LoRA 部分），其余参数被冻结\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# 训练超参数\n",
    "num_train_epochs = 2  # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb750b2e-e557-4946-90d3-254efded1b21",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bd5b04-8876-4438-b711-db19c396adfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_train_epochs):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(model.device)\n",
    "        labels = batch[\"labels\"].to(model.device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        loss = outputs.loss··\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 100 == 0:   # 每 100 个 step 打印一次\n",
    "            print(f\"Epoch {epoch} | Step {step} | Loss {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
