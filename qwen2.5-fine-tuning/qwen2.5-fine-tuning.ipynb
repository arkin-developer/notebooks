{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e6565e-d437-46a9-b294-d024e162f573",
   "metadata": {},
   "source": [
    "# ğŸ¯ Qwen2.5ç³»åˆ—æ¨¡å‹**LoRA/QLoRA** **å¾®è°ƒæ¡ˆä¾‹**\n",
    "\n",
    "> è¯´æ˜ï¼šå…ˆç”¨ä¸€ä¸ªå…¼å®¹çš„å°æ¨¡å‹ï¼ˆä¾‹å¦‚ `Qwen/Qwen2.5-1.5B-Instruct`ï¼‰è·‘é€šæµç¨‹ï¼Œåç»­å°† `MODEL_ID` æ›¿æ¢ä¸ºä½ æ‰¾åˆ°çš„ DeepSeek æ¨¡å‹ä»“åº“åå³å¯ï¼Œä»£ç æ— éœ€æ”¹åŠ¨ã€‚\n",
    "> \n",
    "\n",
    "**ç›®æ ‡**ï¼šåœ¨å•å¡ A10ï¼ˆ24GBï¼‰ä¸Šï¼Œä»¥ *å°å‚æ•°é‡* çš„ DeepSeek ç³»åˆ—æ¨¡å‹ä¸ºä¾‹ï¼ˆæœ¬æ¡ˆä¾‹é‡‡ç”¨ModelScopeæ¥æ›¿æ¢HuggingFaceï¼‰ï¼Œç”¨ **LoRA/QLoRA** è·‘é€šä¸€æ¬¡å®Œæ•´çš„ *æŒ‡ä»¤å¾®è°ƒ*ï¼ˆInstruction Tuningï¼‰æµç¨‹ã€‚  \n",
    "**ç¡¬ä»¶å»ºè®®**ï¼šA10 24GBï¼›  \n",
    "**è½¯ä»¶å»ºè®®**ï¼šPython 3.10+ã€CUDA 12.xã€PyTorch 2.3+ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… æœ¬æ•™ç¨‹åŒ…æ‹¬\n",
    "1. LoRA/QLoRA ç®€ä»‹\n",
    "2. å®‰è£…ä¾èµ–ä¸ç¯å¢ƒæ£€æµ‹  \n",
    "3. é€‰æ‹©æ¨¡å‹ä¸æ•°æ®é›†ï¼ˆä»¥ `Alpaca` ä¸ºç»å…¸ç¤ºä¾‹ï¼‰  \n",
    "4. æ•°æ®é¢„å¤„ç†ä¸ `chat_template` é€‚é…  \n",
    "5. ç”¨ `bitsandbytes` + `peft` + `trl` è¿›è¡Œ **LoRA/QLoRA** å¾®è°ƒ  \n",
    "6. ä¿å­˜ä¸åˆå¹¶æƒé‡ã€æ¨ç†éªŒè¯  \n",
    "\n",
    "> æ³¨ï¼šå…¨æµç¨‹éƒ½åœ¨ **Jupyter Lab** ä¸­é€æ ¼è¿è¡Œå³å¯ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9cb777-2b7d-4310-9259-f30e8aad9dad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## ä¸€ã€LoRA / QLoRA ç®€ä»‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efe3ac9-fde6-4188-a104-6fe1ad57b723",
   "metadata": {},
   "source": [
    "### LoRAï¼ˆLow-Rank Adaptationï¼‰\n",
    "LoRA æ˜¯ä¸€ç§ **è½»é‡åŒ–æ¨¡å‹å¾®è°ƒæ–¹æ³•**ï¼Œå®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š  \n",
    "- åœ¨ä¿æŒåŸå§‹é¢„è®­ç»ƒæ¨¡å‹å‚æ•° **å†»ç»“ä¸å˜** çš„å‰æä¸‹ï¼Œåªåœ¨éƒ¨åˆ†æƒé‡çŸ©é˜µï¼ˆé€šå¸¸æ˜¯ Transformer çš„æ³¨æ„åŠ›å±‚ï¼‰ä¸Šå¼•å…¥ **ä½ç§©çŸ©é˜µåˆ†è§£**ã€‚  \n",
    "- ç”¨ä¸€ä¸ªä½ç§©çš„å‚æ•°çŸ©é˜µï¼ˆAã€Bï¼‰æ¥è¿‘ä¼¼åŸå§‹å¤§çŸ©é˜µçš„æ›´æ–°ï¼Œä»è€Œ **å¤§å¹…å‡å°‘è®­ç»ƒå‚æ•°é‡**ã€‚  \n",
    "- ä¼˜ç‚¹ï¼š  \n",
    "  - **å‚æ•°é«˜æ•ˆ**ï¼šåªéœ€è®­ç»ƒæå°‘é‡çš„æ–°å¢å‚æ•°ï¼ˆå¯ä½è‡³ 0.1%ï¼‰ã€‚  \n",
    "  - **å­˜å‚¨å‹å¥½**ï¼šå¤šä¸ªä¸‹æ¸¸ä»»åŠ¡å¯ä»¥å…±äº«åŒä¸€ä¸ªåŸºç¡€æ¨¡å‹ï¼Œä»…ä¿å­˜ä¸åŒä»»åŠ¡çš„ LoRA æƒé‡ã€‚  \n",
    "  - **éƒ¨ç½²çµæ´»**ï¼šæ¨ç†æ—¶ç›´æ¥å°† LoRA æƒé‡åˆå¹¶åˆ°åŸæ¨¡å‹ï¼Œæ— éœ€é¢å¤–è®¡ç®—å¼€é”€ã€‚\n",
    "\n",
    "> ç®€å•ç†è§£ï¼šLoRA å°±åƒæ˜¯åœ¨å¤§æ¨¡å‹çš„â€œå›ºå®šä¸»å¹²â€ä¸Šï¼Œæ’å…¥ä¸€äº› **å°è€Œèªæ˜çš„é€‚é…å™¨**ï¼Œè®©å®ƒå¿«é€Ÿå­¦ä¼šæ–°ä»»åŠ¡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec29c50-b0e1-4548-a630-f06b887a8ce1",
   "metadata": {},
   "source": [
    "### QLoRAï¼ˆQuantized LoRAï¼‰\n",
    "QLoRA æ˜¯å¯¹ LoRA çš„è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œå®ƒç»“åˆäº† **é‡åŒ–æŠ€æœ¯**ï¼Œä½¿å¾—å¤§æ¨¡å‹çš„å¾®è°ƒåœ¨ **å•å¡æ¶ˆè´¹çº§æ˜¾å¡** ä¸Šä¹Ÿå¯è¡Œã€‚  \n",
    "- æ ¸å¿ƒæ€è·¯ï¼š  \n",
    "  1. å…ˆå°†å¤§æ¨¡å‹çš„å‚æ•°è¿›è¡Œ **4-bit é‡åŒ–ï¼ˆNF4 æ–¹æ¡ˆï¼‰**ï¼Œé™ä½æ˜¾å­˜å ç”¨ã€‚  \n",
    "  2. åœ¨é‡åŒ–åçš„æƒé‡ä¸Šï¼Œåº”ç”¨ **LoRA é€‚é…å™¨** è¿›è¡Œå¾®è°ƒã€‚  \n",
    "  3. è®­ç»ƒæ—¶ä»…æ›´æ–° LoRA å±‚ï¼Œè€Œé‡åŒ–æƒé‡ä¿æŒå†»ç»“ã€‚  \n",
    "\n",
    "- ä¼˜ç‚¹ï¼š  \n",
    "  - **æè‡´æ˜¾å­˜èŠ‚çœ**ï¼šå¯åœ¨ä¸€å¼  24GB æ˜¾å­˜çš„ GPU ä¸Šå¾®è°ƒç™¾äº¿å‚æ•°æ¨¡å‹ã€‚  \n",
    "  - **ä¿æŒæ€§èƒ½**ï¼šé‡åŒ–åçš„ QLoRA ä¸å…¨ç²¾åº¦å¾®è°ƒæ•ˆæœæ¥è¿‘ç”šè‡³ç›¸å½“ã€‚  \n",
    "  - **å®ç”¨æ€§å¼º**ï¼šç‰¹åˆ«é€‚åˆä¸ªäººå¼€å‘è€…å’Œä¸­å°å›¢é˜Ÿã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d7f704-0602-4eee-974c-f7edbe51c71e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### å¯¹æ¯”æ€»ç»“\n",
    "| æ–¹æ³•   | ä¸»è¦æ‰‹æ®µ                   | æ˜¾å­˜æ¶ˆè€— | è®­ç»ƒå‚æ•°é‡ | é€‚ç”¨åœºæ™¯ |\n",
    "|--------|---------------------------|----------|------------|----------|\n",
    "| LoRA   | ä½ç§©çŸ©é˜µåˆ†è§£               | è¾ƒä½     | åƒä¸‡çº§åˆ«   | ä¸­ç­‰è§„æ¨¡æ¨¡å‹çš„é«˜æ•ˆå¾®è°ƒ |\n",
    "| QLoRA  | é‡åŒ–ï¼ˆ4-bitï¼‰ + LoRA é€‚é… | æä½     | åƒä¸‡çº§åˆ«   | è¶…å¤§æ¨¡å‹åœ¨æ¶ˆè´¹çº§ GPU ä¸Šçš„å¾®è°ƒ |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfe81e3-9470-422f-b479-7b118f1bbd03",
   "metadata": {},
   "source": [
    "## äºŒã€å®‰è£…ä¾èµ–ä¸ç¯å¢ƒæ£€æµ‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80687ecd-4115-4970-acd4-135208628c36",
   "metadata": {},
   "source": [
    "### å®‰è£…ä¾èµ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00940b3-620a-4cde-8a9f-c5bdd0df422b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4ae0ef2-b29b-47c0-b000-2377b64d2c4e",
   "metadata": {},
   "source": [
    "### ç¯å¢ƒç‰ˆæœ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cad49f-9ba7-4f2a-b5b3-51f63735617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Œ æ‰“å°è„šæœ¬ç›¸å…³åº“çš„ç‰ˆæœ¬ä¿¡æ¯\n",
    "import torch, transformers, modelscope, peft\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "\n",
    "# transformers æ˜¯ peft å’Œ modelscope ä¾èµ–çš„æ ¸å¿ƒåº“\n",
    "try:\n",
    "    import transformers\n",
    "    print(\"transformers:\", transformers.__version__)\n",
    "except ImportError:\n",
    "    print(\"transformers: æœªå®‰è£…\")\n",
    "\n",
    "try:\n",
    "    import modelscope\n",
    "    print(\"modelscope:\", modelscope.__version__)\n",
    "except ImportError:\n",
    "    print(\"modelscope: æœªå®‰è£…\")\n",
    "\n",
    "try:\n",
    "    import peft\n",
    "    print(\"peft:\", peft.__version__)\n",
    "except ImportError:\n",
    "    print(\"peft: æœªå®‰è£…\")\n",
    "\n",
    "try:\n",
    "    import datasets\n",
    "    print(\"datasets:\", datasets.__version__)\n",
    "except ImportError:\n",
    "    print(\"datasets: æœªå®‰è£…\")\n",
    "\n",
    "try:\n",
    "    import accelerate\n",
    "    print(\"accelerate:\", accelerate.__version__)\n",
    "except ImportError:\n",
    "    print(\"accelerate: æœªå®‰è£…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97315bf-cb93-4ceb-8bd6-6da5da3748af",
   "metadata": {},
   "source": [
    "## ä¸‰ã€ä¸‹è½½æ¨¡å‹å’Œæ•°æ®é›†ï¼ˆModelScopeç‰ˆæœ¬ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfaeffd-b73a-491e-8273-a8db6a069535",
   "metadata": {},
   "source": [
    "### æ¨¡å‹ä¸‹è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e89730e-3b76-47ed-9c85-06672b867adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%modelscope` not found.\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"qwen/Qwen2.5-1.5B-Instruct\"  # å¯æ›¿æ¢\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d92ea8-11cc-49a8-8df6-9cb7bf3df8e5",
   "metadata": {},
   "source": [
    "### æ•°æ®é›†ä¸‹è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea33c99f-f5fa-4115-b4dc-e3000b5f2f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope.msdatasets import MsDataset\n",
    "\n",
    "# é€‰æ‹©alpacaçš„ä¸­æ–‡æ•°æ®é›†\n",
    "ds = MsDataset.load(\"alpaca-gpt4-data-zh\", namespace=\"AI-ModelScope\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef368533-dbe1-42ed-a8c2-2f07509923d8",
   "metadata": {},
   "source": [
    "## å››ã€æ•°æ®é¢„å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f40f313-12aa-4dab-a9aa-7a2197706475",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# è‡ªå®šä¹‰æ•°æ®å¤„ç†å‡½æ•°(éœ€è¦é’ˆå¯¹è‡ªå·±çš„æ•°æ®é›†èŒƒå¼æ¥ç¼–å†™ï¼Œè¿™é‡Œåªé’ˆå¯¹alpaca)\n",
    "def preprocess(example):\n",
    "    # ä¸¢æ‰ instruction æˆ– output ç¼ºå¤±çš„æ ·æœ¬\n",
    "    if not example['instruction'] or not example['output']:\n",
    "        return None\n",
    "\n",
    "    # alpaca æ•°æ®æœ‰æŒ‡ä»¤ã€è¾“å…¥ã€è¾“å‡ºä¸‰ä¸ªæ ‡ç­¾\n",
    "    instruction = example['instruction']\n",
    "    input_text = example.get('input') or \"\"  # input å¯èƒ½ä¸º None\n",
    "    output_text = example['output']\n",
    "\n",
    "    if input_text.strip():\n",
    "        prompt = f\"æŒ‡ä»¤: {instruction}\\nè¾“å…¥: {input_text}\\nå›ç­”:\"\n",
    "    else:\n",
    "        prompt = f\"æŒ‡ä»¤: {instruction}\\nå›ç­”:\"\n",
    "\n",
    "    full_text = prompt + output_text\n",
    "\n",
    "    enc = tokenizer(\n",
    "        full_text,  # éœ€è¦è¿›è¡ŒtokenåŒ–çš„æ–‡æœ¬\n",
    "        truncation=True,  # æ–‡æœ¬è¿‡å¤§çš„æ—¶å€™æ˜¯å¦æˆªæ–­\n",
    "        max_length=16000,  # æ ¹æ®æ¨¡å‹å’Œæ•°æ®é›†å†³å®šï¼Œæ¨¡å‹çš„ä¸Šä¸‹æ–‡, 32kç”šè‡³æ›´å¤§\n",
    "        padding=\"max_length\",  # ğŸ”¹ ä¿è¯é•¿åº¦ä¸€è‡´ï¼ŒDataLoader å †å å®‰å…¨\n",
    "        return_tensors=\"pt\"  # è¿”å›çš„æ•°æ®ç±»å‹ï¼Œpt:pytorch.tensor; tf:tensorflow; np:numpy\n",
    "    )\n",
    "    # å•ä¸ªæ ·æœ¬æ˜¯å­—å…¸æ ¼å¼\n",
    "    return {\n",
    "        \"input_ids\": enc[\"input_ids\"][0],\n",
    "        \"labels\": enc[\"input_ids\"][0]\n",
    "    }\n",
    "\n",
    "train_dataset = ds.map(preprocess)\n",
    "train_dataset = train_dataset.filter(lambda x: x is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709d17b5-dcff-4b59-949c-3bdccc212392",
   "metadata": {},
   "source": [
    "### å°†Datasetè½¬åŒ–æˆDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee1615a-3038-4ffc-9eb7-7f42a0188d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# ğŸ› ï¸ è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•° (collate_fn)\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    ä½œç”¨ï¼š\n",
    "    - DataLoader ä¼šæŠŠä¸€ä¸ª batch çš„æ ·æœ¬ï¼ˆlist[dict]ï¼‰ä¼ è¿›æ¥\n",
    "    - è¿™é‡Œéœ€è¦æ‰‹åŠ¨æ‹¼æ¥æˆ tensorï¼Œå¹¶ä¸”å¯¹é½é•¿åº¦ï¼ˆpadï¼‰\n",
    "    \"\"\"\n",
    "\n",
    "    # å–å‡ºæ¯ä¸ªæ ·æœ¬çš„ input_ids å’Œ labelsï¼Œè½¬æˆ tensor\n",
    "    input_ids = [torch.tensor(item[\"input_ids\"]) for item in batch]\n",
    "    labels = [torch.tensor(item[\"labels\"]) for item in batch]\n",
    "\n",
    "    # ğŸ”¹ å¯¹ input_ids åš padding\n",
    "    #   - batch_first=True: ç»“æœå½¢çŠ¶ (batch_size, seq_len)\n",
    "    #   - padding_value=tokenizer.pad_token_id: ä½¿ç”¨ tokenizer çš„ pad_token_id å¡«å……\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    # ğŸ”¹ å¯¹ labels åš padding\n",
    "    #   - æ³¨æ„è¿™é‡Œ padding_value = -100\n",
    "    #   - åœ¨ PyTorch çš„ CrossEntropyLoss é‡Œï¼Œ-100 ä¼šè¢«å¿½ç•¥ï¼Œä¸å‚ä¸ loss è®¡ç®—\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(\n",
    "        labels, batch_first=True, padding_value=-100\n",
    "    )\n",
    "\n",
    "    # è¿”å›å­—å…¸ï¼Œæ–¹ä¾¿ç›´æ¥å–‚ç»™æ¨¡å‹\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "\n",
    "# ğŸ“Š æ•°æ®å­é›†ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰\n",
    "# è¿™é‡Œä¸ºäº†å¿«é€ŸéªŒè¯æµç¨‹ï¼Œåªå–å‰ 2000 æ¡æ ·æœ¬æ¥è®­ç»ƒ\n",
    "small_dataset = Subset(train_dataset, range(2000))\n",
    "\n",
    "# æ„å»º DataLoader\n",
    "train_loader = DataLoader(\n",
    "    small_dataset,\n",
    "    batch_size=4,        # æ¯æ¬¡å– 4 ä¸ªæ ·æœ¬\n",
    "    shuffle=True,        # æ‰“ä¹±æ•°æ®é¡ºåº\n",
    "    collate_fn=collate_fn  # ä½¿ç”¨æˆ‘ä»¬è‡ªå®šä¹‰çš„ batch æ‹¼æ¥é€»è¾‘\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb3096f-ff18-4add-b275-678d51aea408",
   "metadata": {},
   "source": [
    "## äº”ã€LoRAå¾®è°ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7645b00-73b5-46f9-83fe-ad0145c55b83",
   "metadata": {},
   "source": [
    "### ğŸ”§ é…ç½® LoRA è®­ç»ƒå‚æ•°\n",
    "\n",
    "```python\n",
    "lora_config = LoraConfig(\n",
    "    r=4,                          \n",
    "    lora_alpha=16,               \n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  \n",
    "    lora_dropout=0.05,             \n",
    "    bias=\"none\",               \n",
    "    task_type=\"CAUSAL_LM\"         \n",
    ")\n",
    "```\n",
    "\n",
    "r=4\n",
    "- è¡¨ç¤ºä½ç§©çŸ©é˜µçš„ç§©å€¼ï¼ˆrankï¼‰ï¼Œå€¼è¶Šå¤§ â†’ é€‚é…èƒ½åŠ›æ›´å¼º â†’ å‚æ•°é‡ä¹Ÿéšä¹‹å¢åŠ ã€‚  \n",
    "- è¿™é‡Œé€‰æ‹© `4`ï¼Œæ„å‘³ç€ **è½»é‡çº§è®­ç»ƒ**ï¼Œé€‚åˆå°è§„æ¨¡ä»»åŠ¡æˆ–å¿«é€Ÿå®éªŒã€‚  \n",
    "\n",
    "lora_alpha=16\n",
    "- ç¼©æ”¾å› å­ï¼Œç”¨äºè°ƒæ•´ LoRA çš„è¾“å‡ºå¹…åº¦ã€‚  \n",
    "- ä¸€èˆ¬ç»éªŒæ˜¯ **lora_alpha â‰ˆ 2 Ã— r**ï¼Œæ‰€ä»¥è¿™é‡Œ `16` é…åˆ `r=4` æ˜¯åˆç†çš„ã€‚  \n",
    "\n",
    "target_modules=[\"q_proj\", \"v_proj\"]\n",
    "- LoRA åªåœ¨æ³¨æ„åŠ›æœºåˆ¶çš„ **Query** å’Œ **Value** æŠ•å½±å±‚ä¸­ç”Ÿæ•ˆã€‚  \n",
    "- è¿™æ˜¯æœ€å¸¸è§çš„è®¾ç½®ï¼Œæ—¢ä¿è¯æ•ˆæœï¼Œåˆæ§åˆ¶å‚æ•°é‡ã€‚  \n",
    "\n",
    "lora_dropout=0.05\n",
    "- åœ¨ LoRA å±‚ä¸­æ·»åŠ  **5% çš„ dropout**ï¼Œæå‡æ³›åŒ–èƒ½åŠ›ã€‚  \n",
    "- æ•°æ®é‡å¾ˆå¤§æ—¶å¯ä»¥è°ƒä½åˆ° `0`ï¼›æ•°æ®å°‘æ—¶å¯ä»¥é€‚å½“è°ƒé«˜ï¼ˆå¦‚ `0.1`ï¼‰ã€‚  \n",
    "\n",
    "bias=\"none\"\n",
    "- ä¸è®­ç»ƒ bias å‚æ•°ï¼Œä¿è¯æ¨¡å‹è½»é‡åŒ–ã€‚  \n",
    "- å¤§å¤šæ•°åœºæ™¯ä¸‹ç”¨ `\"none\"` å³å¯ã€‚  \n",
    "\n",
    "task_type=\"CAUSAL_LM\"\n",
    "- è¡¨ç¤ºä»»åŠ¡æ˜¯ **è‡ªå›å½’è¯­è¨€å»ºæ¨¡**ï¼ˆæ¯”å¦‚ Qwenã€GPT ç±»æ¨¡å‹ï¼‰ã€‚  \n",
    "- å¿…é¡»å’Œä»»åŠ¡ç±»å‹ä¸€è‡´ï¼Œå¦åˆ™ forward è¿‡ç¨‹ä¼šæŠ¥é”™ã€‚  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f547e8-9520-40aa-a9dd-3aeef97f305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=4,            \n",
    "    lora_alpha=16,                \n",
    "    target_modules=[\"q_proj\", \"v_proj\"], \n",
    "    lora_dropout=0.05,             \n",
    "    bias=\"none\",                    \n",
    "    task_type=\"CAUSAL_LM\"           \n",
    ")\n",
    "\n",
    "# ğŸš€ å°†åŸºç¡€æ¨¡å‹åŒ…è£…ä¸º PEFT æ¨¡å‹\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# æ‰“å°å½“å‰å¯è®­ç»ƒå‚æ•°é‡ï¼ˆä»… LoRA éƒ¨åˆ†ï¼‰ï¼Œå…¶ä½™å‚æ•°è¢«å†»ç»“\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# è®­ç»ƒè¶…å‚æ•°\n",
    "num_train_epochs = 2  # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb750b2e-e557-4946-90d3-254efded1b21",
   "metadata": {},
   "source": [
    "### è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bd5b04-8876-4438-b711-db19c396adfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_train_epochs):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(model.device)\n",
    "        labels = batch[\"labels\"].to(model.device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        loss = outputs.lossÂ·Â·\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 100 == 0:   # æ¯ 100 ä¸ª step æ‰“å°ä¸€æ¬¡\n",
    "            print(f\"Epoch {epoch} | Step {step} | Loss {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
