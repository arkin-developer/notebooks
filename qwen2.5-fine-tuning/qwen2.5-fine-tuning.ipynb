{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e6565e-d437-46a9-b294-d024e162f573",
   "metadata": {},
   "source": [
    "# ğŸ¯ Qwen2.5ç³»åˆ—æ¨¡å‹**LoRA/QLoRA** **å¾®è°ƒæ¡ˆä¾‹**\n",
    "\n",
    "> è¯´æ˜ï¼šå…ˆç”¨ä¸€ä¸ªå…¼å®¹çš„å°æ¨¡å‹ï¼ˆä¾‹å¦‚ `Qwen/Qwen2.5-1.5B-Instruct`ï¼‰è·‘é€šæµç¨‹ï¼Œåç»­å°† `MODEL_ID` æ›¿æ¢ä¸ºä½ æ‰¾åˆ°çš„ DeepSeek æ¨¡å‹ä»“åº“åå³å¯ï¼Œä»£ç æ— éœ€æ”¹åŠ¨ã€‚\n",
    "> \n",
    "\n",
    "**ç›®æ ‡**ï¼šåœ¨å•å¡ A10ï¼ˆ24GBï¼‰ä¸Šï¼Œä»¥ *å°å‚æ•°é‡* çš„ DeepSeek ç³»åˆ—æ¨¡å‹ä¸ºä¾‹ï¼ˆæœ¬æ¡ˆä¾‹é‡‡ç”¨ModelScopeæ¥æ›¿æ¢HuggingFaceï¼‰ï¼Œç”¨ **LoRA/QLoRA** è·‘é€šä¸€æ¬¡å®Œæ•´çš„ *æŒ‡ä»¤å¾®è°ƒ*ï¼ˆInstruction Tuningï¼‰æµç¨‹ã€‚  \n",
    "**ç¡¬ä»¶å»ºè®®**ï¼šA10 24GBï¼›  \n",
    "**è½¯ä»¶å»ºè®®**ï¼šPython 3.10+ã€CUDA 12.xã€PyTorch 2.3+ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… æœ¬æ•™ç¨‹åŒ…æ‹¬\n",
    "1. LoRA/QLoRA ç®€ä»‹\n",
    "2. ç¡¬ä»¶æ£€æµ‹ä¸é…ç½®ç¯å¢ƒ\n",
    "3. æ¨¡å‹ä¸æ•°æ®é›†ä¸‹è½½  \n",
    "4. æ•°æ®é¢„å¤„ç† \n",
    "5. LoRAå¾®è°ƒ\n",
    "6. æ¨¡å‹æµ‹è¯•è¯„ä¼°\n",
    "\n",
    "> æ³¨ï¼šå…¨æµç¨‹éƒ½åœ¨ **Jupyter Lab** ä¸­é€æ ¼è¿è¡Œå³å¯ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9cb777-2b7d-4310-9259-f30e8aad9dad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## ä¸€ã€LoRA / QLoRA ç®€ä»‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efe3ac9-fde6-4188-a104-6fe1ad57b723",
   "metadata": {},
   "source": [
    "### LoRAï¼ˆLow-Rank Adaptationï¼‰\n",
    "LoRA æ˜¯ä¸€ç§ **è½»é‡åŒ–æ¨¡å‹å¾®è°ƒæ–¹æ³•**ï¼Œå®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š  \n",
    "- åœ¨ä¿æŒåŸå§‹é¢„è®­ç»ƒæ¨¡å‹å‚æ•° **å†»ç»“ä¸å˜** çš„å‰æä¸‹ï¼Œåªåœ¨éƒ¨åˆ†æƒé‡çŸ©é˜µï¼ˆé€šå¸¸æ˜¯ Transformer çš„æ³¨æ„åŠ›å±‚ï¼‰ä¸Šå¼•å…¥ **ä½ç§©çŸ©é˜µåˆ†è§£**ã€‚  \n",
    "- ç”¨ä¸€ä¸ªä½ç§©çš„å‚æ•°çŸ©é˜µï¼ˆAã€Bï¼‰æ¥è¿‘ä¼¼åŸå§‹å¤§çŸ©é˜µçš„æ›´æ–°ï¼Œä»è€Œ **å¤§å¹…å‡å°‘è®­ç»ƒå‚æ•°é‡**ã€‚  \n",
    "- ä¼˜ç‚¹ï¼š  \n",
    "  - **å‚æ•°é«˜æ•ˆ**ï¼šåªéœ€è®­ç»ƒæå°‘é‡çš„æ–°å¢å‚æ•°ï¼ˆå¯ä½è‡³ 0.1%ï¼‰ã€‚  \n",
    "  - **å­˜å‚¨å‹å¥½**ï¼šå¤šä¸ªä¸‹æ¸¸ä»»åŠ¡å¯ä»¥å…±äº«åŒä¸€ä¸ªåŸºç¡€æ¨¡å‹ï¼Œä»…ä¿å­˜ä¸åŒä»»åŠ¡çš„ LoRA æƒé‡ã€‚  \n",
    "  - **éƒ¨ç½²çµæ´»**ï¼šæ¨ç†æ—¶ç›´æ¥å°† LoRA æƒé‡åˆå¹¶åˆ°åŸæ¨¡å‹ï¼Œæ— éœ€é¢å¤–è®¡ç®—å¼€é”€ã€‚\n",
    "\n",
    "> ç®€å•ç†è§£ï¼šLoRA å°±åƒæ˜¯åœ¨å¤§æ¨¡å‹çš„â€œå›ºå®šä¸»å¹²â€ä¸Šï¼Œæ’å…¥ä¸€äº› **å°è€Œèªæ˜çš„é€‚é…å™¨**ï¼Œè®©å®ƒå¿«é€Ÿå­¦ä¼šæ–°ä»»åŠ¡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec29c50-b0e1-4548-a630-f06b887a8ce1",
   "metadata": {},
   "source": [
    "### QLoRAï¼ˆQuantized LoRAï¼‰æœ¬æ–‡ä¸æ¶‰åŠ\n",
    "QLoRA æ˜¯å¯¹ LoRA çš„è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œå®ƒç»“åˆäº† **é‡åŒ–æŠ€æœ¯**ï¼Œä½¿å¾—å¤§æ¨¡å‹çš„å¾®è°ƒåœ¨ **å•å¡æ¶ˆè´¹çº§æ˜¾å¡** ä¸Šä¹Ÿå¯è¡Œã€‚  \n",
    "- æ ¸å¿ƒæ€è·¯ï¼š  \n",
    "  1. å…ˆå°†å¤§æ¨¡å‹çš„å‚æ•°è¿›è¡Œ **4-bit é‡åŒ–ï¼ˆNF4 æ–¹æ¡ˆï¼‰**ï¼Œé™ä½æ˜¾å­˜å ç”¨ã€‚  \n",
    "  2. åœ¨é‡åŒ–åçš„æƒé‡ä¸Šï¼Œåº”ç”¨ **LoRA é€‚é…å™¨** è¿›è¡Œå¾®è°ƒã€‚  \n",
    "  3. è®­ç»ƒæ—¶ä»…æ›´æ–° LoRA å±‚ï¼Œè€Œé‡åŒ–æƒé‡ä¿æŒå†»ç»“ã€‚  \n",
    "\n",
    "- ä¼˜ç‚¹ï¼š  \n",
    "  - **æè‡´æ˜¾å­˜èŠ‚çœ**ï¼šå¯åœ¨ä¸€å¼  24GB æ˜¾å­˜çš„ GPU ä¸Šå¾®è°ƒç™¾äº¿å‚æ•°æ¨¡å‹ã€‚  \n",
    "  - **ä¿æŒæ€§èƒ½**ï¼šé‡åŒ–åçš„ QLoRA ä¸å…¨ç²¾åº¦å¾®è°ƒæ•ˆæœæ¥è¿‘ç”šè‡³ç›¸å½“ã€‚  \n",
    "  - **å®ç”¨æ€§å¼º**ï¼šç‰¹åˆ«é€‚åˆä¸ªäººå¼€å‘è€…å’Œä¸­å°å›¢é˜Ÿã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d7f704-0602-4eee-974c-f7edbe51c71e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### å¯¹æ¯”æ€»ç»“\n",
    "| æ–¹æ³•   | ä¸»è¦æ‰‹æ®µ                   | æ˜¾å­˜æ¶ˆè€— | è®­ç»ƒå‚æ•°é‡ | é€‚ç”¨åœºæ™¯ |\n",
    "|--------|---------------------------|----------|------------|----------|\n",
    "| LoRA   | ä½ç§©çŸ©é˜µåˆ†è§£               | è¾ƒä½     | åƒä¸‡çº§åˆ«   | ä¸­ç­‰è§„æ¨¡æ¨¡å‹çš„é«˜æ•ˆå¾®è°ƒ |\n",
    "| QLoRA  | é‡åŒ–ï¼ˆ4-bitï¼‰ + LoRA é€‚é… | æä½     | åƒä¸‡çº§åˆ«   | è¶…å¤§æ¨¡å‹åœ¨æ¶ˆè´¹çº§ GPU ä¸Šçš„å¾®è°ƒ |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfe81e3-9470-422f-b479-7b118f1bbd03",
   "metadata": {},
   "source": [
    "## äºŒã€ç¡¬ä»¶æ£€æµ‹ä¸é…ç½®ç¯å¢ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80687ecd-4115-4970-acd4-135208628c36",
   "metadata": {},
   "source": [
    "### ç¡¬ä»¶æ£€æµ‹ï¼ˆLinuxï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b0e2c0-e622-4ee7-bb91-b6e53130fb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import sys\n",
    "\n",
    "print(\"æ“ä½œç³»ç»Ÿ:\", platform.system())          # Windows / Linux / Darwin (macOS)\n",
    "print(\"ç³»ç»Ÿç‰ˆæœ¬:\", platform.version())         # å†…æ ¸æˆ–ç‰ˆæœ¬å·\n",
    "print(\"å‘è¡Œç‰ˆæœ¬:\", platform.release())         # ä¾‹å¦‚ 10 / 11 / 22.6.0\n",
    "print(\"è¯¦ç»†ä¿¡æ¯:\", platform.platform())        # æ±‡æ€»\n",
    "print(\"Pythonç‰ˆæœ¬:\", sys.version)             # Python è§£é‡Šå™¨ç‰ˆæœ¬\n",
    "print(\"å¤„ç†å™¨:\", platform.processor())         # CPU ä¿¡æ¯\n",
    "print(\"æœºå™¨ç±»å‹:\", platform.machine())         # x86_64 / arm64\n",
    "\n",
    "# å®‰è£…æ£€æµ‹æ˜¾å¡çš„ä¾èµ–\n",
    "%pip install gputil\n",
    "\n",
    "import GPUtil\n",
    "\n",
    "gpus = GPUtil.getGPUs()\n",
    "for gpu in gpus:\n",
    "    print(f\"æ˜¾å¡å‹å·: {gpu.name}\")\n",
    "    print(f\"æ˜¾å­˜æ€»é‡: {gpu.memoryTotal} MB\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c85d87-cf4c-4461-a35f-ae0ed02b6c11",
   "metadata": {},
   "source": [
    "### é…ç½®ç¯å¢ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e562bedc-2f0d-4ee4-9fec-61263667dce9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install torch==2.3.1+cu121\n",
    "%pip install transformers==4.55.4\n",
    "%pip install modelscope==1.29.0\n",
    "%pip install peft==0.17.1\n",
    "%pip install datasets==3.2.0\n",
    "%pip install accelerate==1.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ae0ef2-b29b-47c0-b000-2377b64d2c4e",
   "metadata": {},
   "source": [
    "### æ£€æŸ¥ç¯å¢ƒç‰ˆæœ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cad49f-9ba7-4f2a-b5b3-51f63735617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Œ æ‰“å°è„šæœ¬ç›¸å…³åº“çš„ç‰ˆæœ¬ä¿¡æ¯\n",
    "import torch, transformers, modelscope, peft\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "\n",
    "# transformers æ˜¯ peft å’Œ modelscope ä¾èµ–çš„æ ¸å¿ƒåº“\n",
    "try:\n",
    "    import transformers\n",
    "    print(\"transformers:\", transformers.__version__)\n",
    "except ImportError:\n",
    "    print(\"transformers: æœªå®‰è£…\")\n",
    "\n",
    "try:\n",
    "    import modelscope\n",
    "    print(\"modelscope:\", modelscope.__version__)\n",
    "except ImportError:\n",
    "    print(\"modelscope: æœªå®‰è£…\")\n",
    "\n",
    "try:\n",
    "    import peft\n",
    "    print(\"peft:\", peft.__version__)\n",
    "except ImportError:\n",
    "    print(\"peft: æœªå®‰è£…\")\n",
    "\n",
    "try:\n",
    "    import datasets\n",
    "    print(\"datasets:\", datasets.__version__)\n",
    "except ImportError:\n",
    "    print(\"datasets: æœªå®‰è£…\")\n",
    "\n",
    "try:\n",
    "    import accelerate\n",
    "    print(\"accelerate:\", accelerate.__version__)\n",
    "except ImportError:\n",
    "    print(\"accelerate: æœªå®‰è£…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97315bf-cb93-4ceb-8bd6-6da5da3748af",
   "metadata": {},
   "source": [
    "## ä¸‰ã€æ¨¡å‹ä¸æ•°æ®é›†ä¸‹è½½"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfaeffd-b73a-491e-8273-a8db6a069535",
   "metadata": {},
   "source": [
    "### Qwen2.5-1.5Bæ¨¡å‹ä¸‹è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e89730e-3b76-47ed-9c85-06672b867adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"qwen/Qwen2.5-1.5B-Instruct\"  # å¯æ›¿æ¢\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d92ea8-11cc-49a8-8df6-9cb7bf3df8e5",
   "metadata": {},
   "source": [
    "### alpacaæ•°æ®é›†ä¸‹è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea33c99f-f5fa-4115-b4dc-e3000b5f2f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope.msdatasets import MsDataset\n",
    "\n",
    "ds =  MsDataset.load('AI-ModelScope/alpaca-gpt4-data-zh', subset_name='default', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef368533-dbe1-42ed-a8c2-2f07509923d8",
   "metadata": {},
   "source": [
    "## å››ã€æ•°æ®é¢„å¤„ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eb9f6d-22ef-404d-8649-2910e5e9911b",
   "metadata": {},
   "source": [
    "### è®¡ç®—max_length\n",
    "æ ¹æ®æ•°æ®é›†çš„tokené•¿åº¦æ¥è®¡ç®—æœ€åˆé€‚çš„é•¿åº¦ï¼Œæœ‰åˆ©äºæ•°æ®é¢„å¤„ç†çš„é€Ÿåº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4135b13f-365d-4a47-9f6d-67b06e651009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_max_length(dataset, tokenizer, sample_size=5000, quantile=95):\n",
    "    \"\"\"\n",
    "    è‡ªåŠ¨ç»Ÿè®¡ token é•¿åº¦åˆ†å¸ƒï¼Œå¹¶æ¨è max_length\n",
    "    Args:\n",
    "        dataset: MsDataset å¯¹è±¡\n",
    "        tokenizer: HF AutoTokenizer\n",
    "        sample_size: æŠ½æ ·æ•°é‡ï¼ˆé¿å…å…¨é‡å¤ªæ…¢ï¼‰\n",
    "        quantile: åˆ†ä½æ•°ï¼ˆé»˜è®¤95ï¼‰\n",
    "    \"\"\"\n",
    "    total = min(sample_size, len(dataset))\n",
    "    lengths = []\n",
    "\n",
    "    print(f\"å¼€å§‹ç»Ÿè®¡ï¼ŒæŠ½æ · {total} æ¡æ•°æ® ...\")\n",
    "\n",
    "    for i in tqdm(range(total)):\n",
    "        ex = dataset[i]\n",
    "        instruction = ex.get(\"instruction\", \"\")\n",
    "        input_text = ex.get(\"input\", \"\") or \"\"\n",
    "        output_text = ex.get(\"output\", \"\")\n",
    "\n",
    "        if input_text.strip():\n",
    "            prompt = f\"æŒ‡ä»¤: {instruction}\\nè¾“å…¥: {input_text}\\nå›ç­”:\"\n",
    "        else:\n",
    "            prompt = f\"æŒ‡ä»¤: {instruction}\\nå›ç­”:\"\n",
    "\n",
    "        full_text = prompt + output_text\n",
    "        tokenized = tokenizer(full_text, truncation=False)\n",
    "        lengths.append(len(tokenized[\"input_ids\"]))\n",
    "\n",
    "    max_len = max(lengths)\n",
    "    avg_len = np.mean(lengths)\n",
    "    q_len = np.percentile(lengths, quantile)\n",
    "\n",
    "    print(\"\\n=== Token é•¿åº¦ç»Ÿè®¡ç»“æœ ===\")\n",
    "    print(f\"æœ€å¤§é•¿åº¦: {max_len}\")\n",
    "    print(f\"å¹³å‡é•¿åº¦: {avg_len:.2f}\")\n",
    "    print(f\"{quantile}% åˆ†ä½æ•°é•¿åº¦: {q_len:.0f}\")\n",
    "    print(\"=========================\")\n",
    "    print(f\"ğŸ‘‰ æ¨è max_length = {int(min(q_len, tokenizer.model_max_length))}\")\n",
    "    print(f\"(æ¨¡å‹æ”¯æŒçš„æœ€å¤§é•¿åº¦ = {tokenizer.model_max_length})\")\n",
    "\n",
    "    return int(min(q_len, tokenizer.model_max_length))\n",
    "\n",
    "\n",
    "# è®¡ç®—æ•°æ®é›†åº”è¯¥å®šä¹‰çš„æœ€å¤§é•¿åº¦\n",
    "max_length = recommend_max_length(ds, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53410742-ec2d-4383-ad76-43cb6b997ca4",
   "metadata": {},
   "source": [
    "### é¢„å¤„ç†ï¼ˆæ„é€ Datasetï¼‰\n",
    "éœ€è¦é’ˆå¯¹è‡ªå·±çš„æ•°æ®é›†èŒƒå¼æ¥ç¼–å†™ï¼Œè¿™é‡Œåªé’ˆå¯¹alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f40f313-12aa-4dab-a9aa-7a2197706475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è‡ªå®šä¹‰æ•°æ®å¤„ç†å‡½æ•°(éœ€è¦é’ˆå¯¹è‡ªå·±çš„æ•°æ®é›†èŒƒå¼æ¥ç¼–å†™ï¼Œè¿™é‡Œåªé’ˆå¯¹alpaca)\n",
    "def preprocess(example):\n",
    "    # ä¸¢æ‰ instruction æˆ– output ç¼ºå¤±çš„æ ·æœ¬\n",
    "    if not example['instruction'] or not example['output']:\n",
    "        return None\n",
    "\n",
    "    # alpaca æ•°æ®æœ‰æŒ‡ä»¤ã€è¾“å…¥ã€è¾“å‡ºä¸‰ä¸ªæ ‡ç­¾\n",
    "    instruction = example['instruction']\n",
    "    input_text = example.get('input') or \"\"  # input å¯èƒ½ä¸º None\n",
    "    output_text = example['output']\n",
    "\n",
    "    if input_text.strip():\n",
    "        prompt = f\"æŒ‡ä»¤: {instruction}\\nè¾“å…¥: {input_text}\\nå›ç­”:\"\n",
    "    else:\n",
    "        prompt = f\"æŒ‡ä»¤: {instruction}\\nå›ç­”:\"\n",
    "\n",
    "    full_text = prompt + output_text\n",
    "\n",
    "    enc = tokenizer(\n",
    "        full_text,  # éœ€è¦è¿›è¡ŒtokenåŒ–çš„æ–‡æœ¬\n",
    "        truncation=True,  # æ–‡æœ¬è¿‡å¤§çš„æ—¶å€™æ˜¯å¦æˆªæ–­\n",
    "        max_length=max_length,  # æ ¹æ®æ¨¡å‹å’Œæ•°æ®é›†å†³å®šæœ€åˆé€‚çš„\n",
    "        padding=\"max_length\",  # ğŸ”¹ ä¿è¯é•¿åº¦ä¸€è‡´ï¼ŒDataLoader å †å å®‰å…¨\n",
    "        return_tensors=\"pt\"  # è¿”å›çš„æ•°æ®ç±»å‹ï¼Œpt:pytorch.tensor; tf:tensorflow; np:numpy\n",
    "    )\n",
    "    # å•ä¸ªæ ·æœ¬æ˜¯å­—å…¸æ ¼å¼\n",
    "    return {\n",
    "        \"input_ids\": enc[\"input_ids\"][0],\n",
    "        \"labels\": enc[\"input_ids\"][0]\n",
    "    }\n",
    "\n",
    "train_dataset = ds.map(preprocess)\n",
    "train_dataset = train_dataset.filter(lambda x: x is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709d17b5-dcff-4b59-949c-3bdccc212392",
   "metadata": {},
   "source": [
    "### æ„é€ DataLoader\n",
    "å°†é¢„å¤„ç†å¥½çš„Datasetè¿›è¡ŒPaddingåæ–¹ä¾¿æ‹¼æ¥æˆå¤šä¸ªbatchçš„å‘é‡ï¼Œä¸ºè®­ç»ƒä½œå‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee1615a-3038-4ffc-9eb7-7f42a0188d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# ğŸ› ï¸ è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•° (collate_fn)\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    ä½œç”¨ï¼š\n",
    "    - DataLoader ä¼šæŠŠä¸€ä¸ª batch çš„æ ·æœ¬ï¼ˆlist[dict]ï¼‰ä¼ è¿›æ¥\n",
    "    - è¿™é‡Œéœ€è¦æ‰‹åŠ¨æ‹¼æ¥æˆ tensorï¼Œå¹¶ä¸”å¯¹é½é•¿åº¦ï¼ˆpadï¼‰\n",
    "    \"\"\"\n",
    "\n",
    "    # å–å‡ºæ¯ä¸ªæ ·æœ¬çš„ input_ids å’Œ labelsï¼Œè½¬æˆ tensor\n",
    "    input_ids = [torch.tensor(item[\"input_ids\"]) for item in batch]\n",
    "    labels = [torch.tensor(item[\"labels\"]) for item in batch]\n",
    "\n",
    "    # ğŸ”¹ å¯¹ input_ids åš padding\n",
    "    #   - batch_first=True: ç»“æœå½¢çŠ¶ (batch_size, seq_len)\n",
    "    #   - padding_value=tokenizer.pad_token_id: ä½¿ç”¨ tokenizer çš„ pad_token_id å¡«å……\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    # ğŸ”¹ å¯¹ labels åš padding\n",
    "    #   - æ³¨æ„è¿™é‡Œ padding_value = -100\n",
    "    #   - åœ¨ PyTorch çš„ CrossEntropyLoss é‡Œï¼Œ-100 ä¼šè¢«å¿½ç•¥ï¼Œä¸å‚ä¸ loss è®¡ç®—\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(\n",
    "        labels, batch_first=True, padding_value=-100\n",
    "    )\n",
    "\n",
    "    # è¿”å›å­—å…¸ï¼Œæ–¹ä¾¿ç›´æ¥å–‚ç»™æ¨¡å‹\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "\n",
    "# åœ¨æ•°æ®é¢„å¤„ç†åï¼Œåˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# è·å–æ•°æ®é›†çš„æ€»é•¿åº¦\n",
    "total_samples = len(train_dataset)\n",
    "print(f\"æ€»æ ·æœ¬æ•°: {total_samples}\")\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§\n",
    "np.random.seed(42)\n",
    "\n",
    "# åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›† (80% è®­ç»ƒ, 20% æµ‹è¯•)\n",
    "train_indices, test_indices = train_test_split(\n",
    "    range(total_samples), \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# åˆ›å»ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "train_subset = Subset(train_dataset, train_indices)\n",
    "test_subset = Subset(train_dataset, test_indices)\n",
    "\n",
    "print(f\"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_subset)}\")\n",
    "print(f\"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_subset)}\")\n",
    "\n",
    "# ä¸ºäº†å¿«é€ŸéªŒè¯ï¼Œå¯ä»¥åªå–éƒ¨åˆ†æ•°æ®\n",
    "small_train_dataset = Subset(train_subset, range(min(2000, len(train_subset))))\n",
    "small_test_dataset = Subset(test_subset, range(min(500, len(test_subset))))\n",
    "\n",
    "print(f\"å°è®­ç»ƒé›†æ ·æœ¬æ•°: {len(small_train_dataset)}\")\n",
    "print(f\"å°æµ‹è¯•é›†æ ·æœ¬æ•°: {len(small_test_dataset)}\")\n",
    "\n",
    "# æ„å»ºè®­ç»ƒå’Œæµ‹è¯•çš„ DataLoader\n",
    "train_loader = DataLoader(\n",
    "    small_train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    small_test_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,  # æµ‹è¯•æ—¶ä¸éœ€è¦æ‰“ä¹±\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb3096f-ff18-4add-b275-678d51aea408",
   "metadata": {},
   "source": [
    "## äº”ã€LoRAå¾®è°ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76fe51a-3b1d-44d2-aba4-a1bd46903665",
   "metadata": {},
   "source": [
    "### LoRAå‚æ•°è¯´æ˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7645b00-73b5-46f9-83fe-ad0145c55b83",
   "metadata": {},
   "source": [
    "```python\n",
    "lora_config = LoraConfig(\n",
    "    r=4,                          \n",
    "    lora_alpha=16,               \n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  \n",
    "    lora_dropout=0.05,             \n",
    "    bias=\"none\",               \n",
    "    task_type=\"CAUSAL_LM\"         \n",
    ")\n",
    "```\n",
    "\n",
    "r=4\n",
    "- è¡¨ç¤ºä½ç§©çŸ©é˜µçš„ç§©å€¼ï¼ˆrankï¼‰ï¼Œå€¼è¶Šå¤§ â†’ é€‚é…èƒ½åŠ›æ›´å¼º â†’ å‚æ•°é‡ä¹Ÿéšä¹‹å¢åŠ ã€‚  \n",
    "- è¿™é‡Œé€‰æ‹© `4`ï¼Œæ„å‘³ç€ **è½»é‡çº§è®­ç»ƒ**ï¼Œé€‚åˆå°è§„æ¨¡ä»»åŠ¡æˆ–å¿«é€Ÿå®éªŒã€‚  \n",
    "\n",
    "lora_alpha=16\n",
    "- ç¼©æ”¾å› å­ï¼Œç”¨äºè°ƒæ•´ LoRA çš„è¾“å‡ºå¹…åº¦ã€‚  \n",
    "- ä¸€èˆ¬ç»éªŒæ˜¯ **lora_alpha â‰ˆ 2 Ã— r**ï¼Œæ‰€ä»¥è¿™é‡Œ `16` é…åˆ `r=4` æ˜¯åˆç†çš„ã€‚  \n",
    "\n",
    "target_modules=[\"q_proj\", \"v_proj\"]\n",
    "- LoRA åªåœ¨æ³¨æ„åŠ›æœºåˆ¶çš„ **Query** å’Œ **Value** æŠ•å½±å±‚ä¸­ç”Ÿæ•ˆã€‚  \n",
    "- è¿™æ˜¯æœ€å¸¸è§çš„è®¾ç½®ï¼Œæ—¢ä¿è¯æ•ˆæœï¼Œåˆæ§åˆ¶å‚æ•°é‡ã€‚\n",
    "- [\"q_proj\", \"v_proj\"] â†’ é»˜è®¤æ¨èï¼Œ90% çš„åœºæ™¯é€‚ç”¨ï¼ˆå¯¹è¯ã€é—®ç­”ã€æŒ‡ä»¤è·Ÿéšï¼‰ã€‚\n",
    "- [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"] â†’ å…¨é‡ Attentionï¼Œå‚æ•°é‡å¤§ä¸€äº›ï¼Œé€‚åˆä»»åŠ¡æ›´å¤æ‚ï¼ˆå¦‚å¤šæ¨¡æ€å¯¹é½ï¼‰ã€‚\n",
    "- [\"gate_proj\", \"up_proj\", \"down_proj\"] â†’ æ”¹ MLP å±‚ï¼Œé€‚åˆéœ€è¦è°ƒæ•´â€œçŸ¥è¯†è¡¨è¾¾â€çš„åœºæ™¯ï¼ˆä¾‹å¦‚æ•°å­¦æ¨ç†ï¼‰ã€‚\n",
    "- [\"q_proj\", \"v_proj\", \"down_proj\"] â†’ æ··åˆæ–¹å¼ï¼Œæœ‰æ—¶èƒ½è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚\n",
    "\n",
    "lora_dropout=0.05\n",
    "- åœ¨ LoRA å±‚ä¸­æ·»åŠ  **5% çš„ dropout**ï¼Œæå‡æ³›åŒ–èƒ½åŠ›ã€‚  \n",
    "- æ•°æ®é‡å¾ˆå¤§æ—¶å¯ä»¥è°ƒä½åˆ° `0`ï¼›æ•°æ®å°‘æ—¶å¯ä»¥é€‚å½“è°ƒé«˜ï¼ˆå¦‚ `0.1`ï¼‰ã€‚  \n",
    "\n",
    "bias=\"none\"\n",
    "- ä¸è®­ç»ƒ bias å‚æ•°ï¼Œä¿è¯æ¨¡å‹è½»é‡åŒ–ã€‚  \n",
    "- å¤§å¤šæ•°åœºæ™¯ä¸‹ç”¨ `\"none\"` å³å¯ã€‚  \n",
    "\n",
    "task_type=\"CAUSAL_LM\"\n",
    "- è¡¨ç¤ºä»»åŠ¡æ˜¯ **è‡ªå›å½’è¯­è¨€å»ºæ¨¡**ï¼ˆæ¯”å¦‚ Qwenã€GPT ç±»æ¨¡å‹ï¼‰ã€‚  \n",
    "- å¿…é¡»å’Œä»»åŠ¡ç±»å‹ä¸€è‡´ï¼Œå¦åˆ™ forward è¿‡ç¨‹ä¼šæŠ¥é”™ã€‚  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da25a924-a0ca-4779-9006-59be35ecb388",
   "metadata": {},
   "source": [
    "### é…ç½® LoRA è®­ç»ƒå‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f547e8-9520-40aa-a9dd-3aeef97f305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=4,            \n",
    "    lora_alpha=16,                \n",
    "    target_modules=[\"q_proj\", \"v_proj\"], \n",
    "    lora_dropout=0.05,             \n",
    "    bias=\"none\",                    \n",
    "    task_type=\"CAUSAL_LM\"           \n",
    ")\n",
    "\n",
    "# ğŸš€ å°†åŸºç¡€æ¨¡å‹åŒ…è£…ä¸º PEFT æ¨¡å‹\n",
    "model = get_peft_model(model, lora_config)  # è¿™é‡Œé»˜è®¤ä¼šå†»ç»“éLoRAçš„å‚æ•°\n",
    "\n",
    "# æ‰“å°å½“å‰å¯è®­ç»ƒå‚æ•°é‡ï¼ˆä»… LoRA éƒ¨åˆ†ï¼‰ï¼Œå…¶ä½™å‚æ•°è¢«å†»ç»“\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# è®­ç»ƒè¶…å‚æ•°\n",
    "num_train_epochs = 2  # å¯¹å®Œæ•´çš„æ•°æ®é›†è®­ç»ƒå¤šå°‘ä¸ªæ‰¹æ¬¡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be6e3f0-b17b-4001-9884-b41af16157b2",
   "metadata": {},
   "source": [
    "### ç¡®è®¤å‚æ•°æ˜¯å¦å†»ç»“\n",
    "è¿™é‡Œåªéœ€è¦è®­ç»ƒLoRAæ–°å¢çš„å‚æ•°å±‚\n",
    "è¾“å‡ºåº”è¯¥åªåŒ…å« lora_Aã€lora_B ä¹‹ç±»çš„å¢é‡å‚æ•°å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08575633-4676-4e2d-94dd-aba87aff7062",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:  # è¿™é‡Œåªæ‰“å°æœ‰æ¢¯åº¦ä¿¡æ¯çš„\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb750b2e-e557-4946-90d3-254efded1b21",
   "metadata": {},
   "source": [
    "### å¼€å§‹å¾®è°ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bd5b04-8876-4438-b711-db19c396adfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "import os\n",
    "\n",
    "# åˆ›å»ºä¿å­˜ç›®å½•\n",
    "model_save_path = \"./qwen2.5-finetuned-lora\"\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "# ä¼˜åŒ–å™¨è®¾ç½®\n",
    "optimizer = AdamW(model.parameters(), lr=2e-4)\n",
    "\n",
    "# è®­ç»ƒå¾ªç¯\n",
    "model.train()\n",
    "total_steps = len(train_loader) * num_train_epochs\n",
    "current_step = 0\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    print(f\"\\nå¼€å§‹ç¬¬ {epoch + 1}/{num_train_epochs} è½®è®­ç»ƒ...\")\n",
    "    \n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(model.device)\n",
    "        labels = batch[\"labels\"].to(model.device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        current_step += 1\n",
    "\n",
    "        # æ¯ 100 ä¸ª step æ‰“å°ä¸€æ¬¡è¿›åº¦\n",
    "        if step % 100 == 0:\n",
    "            avg_loss = epoch_loss / (step + 1)\n",
    "            print(f\"Epoch {epoch + 1}/{num_train_epochs} | Step {step}/{len(train_loader)} | \"\n",
    "                  f\"Loss {loss.item():.4f} | Avg Loss {avg_loss:.4f} | \"\n",
    "                  f\"Progress {current_step}/{total_steps}\")\n",
    "    \n",
    "    # æ¯è½®ç»“æŸåæ‰“å°å¹³å‡æŸå¤±\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"ç¬¬ {epoch + 1} è½®è®­ç»ƒå®Œæˆï¼Œå¹³å‡æŸå¤±: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "print(\"\\nè®­ç»ƒå®Œæˆï¼\")\n",
    "\n",
    "# ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹\n",
    "print(f\"æ­£åœ¨ä¿å­˜æ¨¡å‹åˆ°: {model_save_path}\")\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "# ä¿å­˜è®­ç»ƒé…ç½®ä¿¡æ¯\n",
    "import json\n",
    "config_info = {\n",
    "    \"model_id\": model_id,\n",
    "    \"lora_config\": {\n",
    "        \"r\": lora_config.r,\n",
    "        \"lora_alpha\": lora_config.lora_alpha,\n",
    "        \"target_modules\": list(lora_config.target_modules),  # å°†setè½¬æ¢ä¸ºlist\n",
    "        \"lora_dropout\": lora_config.lora_dropout,\n",
    "        \"bias\": lora_config.bias,\n",
    "        \"task_type\": lora_config.task_type\n",
    "    },\n",
    "    \"training_config\": {\n",
    "        \"num_epochs\": num_train_epochs,\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"batch_size\": 4,\n",
    "        \"total_steps\": total_steps\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(model_save_path, \"training_config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config_info, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"æ¨¡å‹å’Œé…ç½®å·²ä¿å­˜åˆ°: {model_save_path}\")\n",
    "print(f\"å¯è®­ç»ƒå‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2484e090-c227-419b-a238-a5f5e1e585e6",
   "metadata": {},
   "source": [
    "## å…­ã€æ¨¡å‹æµ‹è¯•è¯„ä¼°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4243242d-2ce8-453d-920f-1d8236b9bf8e",
   "metadata": {},
   "source": [
    "### æ¨¡å‹åŠ è½½\n",
    "åŠ è½½å¾®è°ƒåçš„æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b0715e-2d47-499a-98b9-5cf691f40062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# é‡æ–°åŠ è½½åŸºç¡€æ¨¡å‹\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# åŠ è½½ LoRA æƒé‡\n",
    "finetuned_model = PeftModel.from_pretrained(base_model, model_save_path)\n",
    "finetuned_model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼\n",
    "\n",
    "print(\"å¾®è°ƒåçš„æ¨¡å‹åŠ è½½å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41deefc-fed1-4f6d-ac20-e7098efe4580",
   "metadata": {},
   "source": [
    "### æµ‹è¯•\n",
    "ç”¨éƒ¨ä»½æœªç»è¿‡è®­ç»ƒçš„æ•°æ®é›†è¿›è¡Œæµ‹è¯•ï¼Œè¿™é‡Œé‡‡ç”¨çš„æ˜¯äº¤å‰ç†µæŸå¤±ï¼ˆCrossEntropyLossï¼‰  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cae18c1-5a64-494c-8742-9d5d35df3bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "\n",
    "def evaluate_model(model, test_loader, tokenizer):\n",
    "    \"\"\"\n",
    "    è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"æµ‹è¯•ä¸­\"):\n",
    "            input_ids = batch[\"input_ids\"].to(model.device)\n",
    "            labels = batch[\"labels\"].to(model.device)\n",
    "            \n",
    "            # è®¡ç®—æŸå¤±\n",
    "            outputs = model(input_ids=input_ids, labels=labels)\n",
    "            total_loss += outputs.loss.item()\n",
    "            \n",
    "            # ç”Ÿæˆé¢„æµ‹\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_new_tokens=max_length,  # æœ€å¤§ç”Ÿæˆ100ä¸ªæ–°token\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            # è§£ç é¢„æµ‹ç»“æœå’Œç›®æ ‡\n",
    "            for i in range(len(generated_ids)):\n",
    "                # è·å–è¾“å…¥éƒ¨åˆ†ï¼ˆç”¨äºæå–æŒ‡ä»¤ï¼‰\n",
    "                input_text = tokenizer.decode(input_ids[i], skip_special_tokens=True)\n",
    "                \n",
    "                # è·å–ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»æ‰è¾“å…¥éƒ¨åˆ†ï¼‰\n",
    "                generated_text = tokenizer.decode(generated_ids[i][len(input_ids[i]):], skip_special_tokens=True)\n",
    "                \n",
    "                # è·å–ç›®æ ‡ç­”æ¡ˆ\n",
    "                target_text = tokenizer.decode(labels[i][labels[i] != -100], skip_special_tokens=True)\n",
    "                \n",
    "                all_predictions.append(generated_text.strip())\n",
    "                all_targets.append(target_text.strip())\n",
    "    \n",
    "    # è®¡ç®—å¹³å‡æŸå¤±\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    return avg_loss, all_predictions, all_targets\n",
    "\n",
    "# æ‰§è¡Œæµ‹è¯•\n",
    "print(\"å¼€å§‹æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹...\")\n",
    "test_loss, predictions, targets = evaluate_model(finetuned_model, test_loader, tokenizer)\n",
    "\n",
    "print(f\"æµ‹è¯•é›†å¹³å‡æŸå¤±: {test_loss:.4f}\")\n",
    "print(f\"æµ‹è¯•æ ·æœ¬æ•°: {len(predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ce1c79-b8fb-491c-93ab-fc38c8cc87f6",
   "metadata": {},
   "source": [
    "### LLMæ¨¡å‹å…¶ä»–å¸¸è§æŒ‡æ ‡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc16187a-deba-4cd8-a422-23a785d0411b",
   "metadata": {},
   "source": [
    "| æŒ‡æ ‡                   | è¯´æ˜                  | é€‚ç”¨åœºæ™¯        |\n",
    "| -------------------- | ------------------- | ----------- |\n",
    "| **BLEU**             | n-gram ç²¾ç¡®åŒ¹é…         | ç¿»è¯‘ã€æ‘˜è¦       |\n",
    "| **ROUGE**            | å…³æ³¨ recallï¼ˆè¦†ç›–ç‡ï¼‰      | æ‘˜è¦ç”Ÿæˆã€é—®ç­”     |\n",
    "| **METEOR**           | è€ƒè™‘åŒä¹‰è¯åŒ¹é…             | æ–‡æœ¬ç”Ÿæˆ        |\n",
    "| **BERTScore**        | åŸºäºè¯­ä¹‰åµŒå…¥æ¯”è¾ƒæ–‡æœ¬ç›¸ä¼¼åº¦       | ç”Ÿæˆä»»åŠ¡ï¼Œè¯­ä¹‰è¯„ä»·   |\n",
    "| **Exact Match (EM)** | å®Œå…¨åŒ¹é…                | é—®ç­”ã€é€‰æ‹©é¢˜ã€é€»è¾‘è¾“å‡º |\n",
    "| **Perplexity (å›°æƒ‘åº¦)** | æ¨¡å‹é¢„æµ‹ token çš„æ¦‚ç‡åæ˜ æµç•…åº¦ | æ–‡æœ¬ç”Ÿæˆã€è¯­è¨€å»ºæ¨¡   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beba284-8802-4cba-b5b5-44dde5215d2f",
   "metadata": {},
   "source": [
    "### è®¡ç®—BERTScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b19093e-5068-458e-80fc-fa56fd371b1c",
   "metadata": {},
   "source": [
    "| æŒ‡æ ‡                | å«ä¹‰               | å€¼åŸŸ     | è¶Šå¤§è¶Šå¥½ï¼Ÿ  |\n",
    "| ----------------- | ---------------- | ------ | ------ |\n",
    "| **Precision (P)** | ç”Ÿæˆæ–‡æœ¬å’Œç›®æ ‡æ–‡æœ¬è¯­ä¹‰é‡å çš„æ¯”ä¾‹ | 0 \\~ 1 | âœ… è¶Šå¤§è¶Šå¥½ |\n",
    "| **Recall (R)**    | ç›®æ ‡æ–‡æœ¬ä¸­è¢«ç”Ÿæˆæ–‡æœ¬è¦†ç›–çš„æ¯”ä¾‹  | 0 \\~ 1 | âœ… è¶Šå¤§è¶Šå¥½ |\n",
    "| **F1-score (F1)** | P å’Œ R çš„è°ƒå’Œå¹³å‡      | 0 \\~ 1 | âœ… è¶Šå¤§è¶Šå¥½ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2424cfee-7520-4949-88ff-f33b513603da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bert-score\n",
    "\n",
    "from bert_score import score\n",
    "\n",
    "# å‡è®¾ predictions å’Œ targets å·²ç»æ˜¯ä½ çš„ç”Ÿæˆç»“æœå’ŒçœŸå®ç­”æ¡ˆåˆ—è¡¨\n",
    "P, R, F1 = score(predictions, targets, lang=\"en\", verbose=True)  # lang å¯ä»¥æ ¹æ®ä»»åŠ¡é€‰æ‹© 'en' æˆ– 'zh'\n",
    "\n",
    "# P, R, F1 éƒ½æ˜¯ tensorï¼Œå½¢çŠ¶ä¸º [æ ·æœ¬æ•°]\n",
    "avg_precision = P.mean().item()\n",
    "avg_recall = R.mean().item()\n",
    "avg_f1 = F1.mean().item()\n",
    "\n",
    "print(f\"BERTScore å¹³å‡ Precision: {avg_precision:.4f}\")\n",
    "print(f\"BERTScore å¹³å‡ Recall:    {avg_recall:.4f}\")\n",
    "print(f\"BERTScore å¹³å‡ F1:        {avg_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
