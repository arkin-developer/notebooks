{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e6565e-d437-46a9-b294-d024e162f573",
   "metadata": {},
   "source": [
    "# 🎯 Qwen2.5系列模型**LoRA/QLoRA** **微调案例**\n",
    "\n",
    "> 说明：先用一个兼容的小模型（例如 `Qwen/Qwen2.5-1.5B-Instruct`）跑通流程，后续将 `MODEL_ID` 替换为你找到的 DeepSeek 模型仓库名即可，代码无需改动。\n",
    "> \n",
    "\n",
    "**目标**：在单卡 A10（24GB）上，以 *小参数量* 的 DeepSeek 系列模型为例（本案例采用ModelScope来替换HuggingFace），用 **LoRA/QLoRA** 跑通一次完整的 *指令微调*（Instruction Tuning）流程。  \n",
    "**硬件建议**：A10 24GB；  \n",
    "**软件建议**：Python 3.10+、CUDA 12.x、PyTorch 2.3+。\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 本教程包括\n",
    "1. LoRA/QLoRA 简介\n",
    "2. 安装依赖与环境检测  \n",
    "3. 选择模型与数据集（以 `Alpaca` 为经典示例）  \n",
    "4. 数据预处理与 `chat_template` 适配  \n",
    "5. 用 `bitsandbytes` + `peft` + `trl` 进行 **LoRA/QLoRA** 微调  \n",
    "6. 保存与合并权重、推理验证  \n",
    "\n",
    "> 注：全流程都在 **Jupyter Lab** 中逐格运行即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9cb777-2b7d-4310-9259-f30e8aad9dad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 一、LoRA / QLoRA 简介"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efe3ac9-fde6-4188-a104-6fe1ad57b723",
   "metadata": {},
   "source": [
    "### LoRA（Low-Rank Adaptation）\n",
    "LoRA 是一种 **轻量化模型微调方法**，它的核心思想是：  \n",
    "- 在保持原始预训练模型参数 **冻结不变** 的前提下，只在部分权重矩阵（通常是 Transformer 的注意力层）上引入 **低秩矩阵分解**。  \n",
    "- 用一个低秩的参数矩阵（A、B）来近似原始大矩阵的更新，从而 **大幅减少训练参数量**。  \n",
    "- 优点：  \n",
    "  - **参数高效**：只需训练极少量的新增参数（可低至 0.1%）。  \n",
    "  - **存储友好**：多个下游任务可以共享同一个基础模型，仅保存不同任务的 LoRA 权重。  \n",
    "  - **部署灵活**：推理时直接将 LoRA 权重合并到原模型，无需额外计算开销。\n",
    "\n",
    "> 简单理解：LoRA 就像是在大模型的“固定主干”上，插入一些 **小而聪明的适配器**，让它快速学会新任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec29c50-b0e1-4548-a630-f06b887a8ce1",
   "metadata": {},
   "source": [
    "### QLoRA（Quantized LoRA）\n",
    "QLoRA 是对 LoRA 的进一步优化，它结合了 **量化技术**，使得大模型的微调在 **单卡消费级显卡** 上也可行。  \n",
    "- 核心思路：  \n",
    "  1. 先将大模型的参数进行 **4-bit 量化（NF4 方案）**，降低显存占用。  \n",
    "  2. 在量化后的权重上，应用 **LoRA 适配器** 进行微调。  \n",
    "  3. 训练时仅更新 LoRA 层，而量化权重保持冻结。  \n",
    "\n",
    "- 优点：  \n",
    "  - **极致显存节省**：可在一张 24GB 显存的 GPU 上微调百亿参数模型。  \n",
    "  - **保持性能**：量化后的 QLoRA 与全精度微调效果接近甚至相当。  \n",
    "  - **实用性强**：特别适合个人开发者和中小团队。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d7f704-0602-4eee-974c-f7edbe51c71e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 对比总结\n",
    "| 方法   | 主要手段                   | 显存消耗 | 训练参数量 | 适用场景 |\n",
    "|--------|---------------------------|----------|------------|----------|\n",
    "| LoRA   | 低秩矩阵分解               | 较低     | 千万级别   | 中等规模模型的高效微调 |\n",
    "| QLoRA  | 量化（4-bit） + LoRA 适配 | 极低     | 千万级别   | 超大模型在消费级 GPU 上的微调 |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfe81e3-9470-422f-b479-7b118f1bbd03",
   "metadata": {},
   "source": [
    "## 二、安装依赖与环境检测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80687ecd-4115-4970-acd4-135208628c36",
   "metadata": {},
   "source": [
    "### 安装依赖"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab25c4f7-8559-4bd4-8a5e-e48bba2c90a3",
   "metadata": {},
   "source": [
    "linux版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3b0e2c0-e622-4ee7-bb91-b6e53130fb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/arkin/anaconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4d0d27-d929-41fd-b1f5-6dfea51f0a93",
   "metadata": {},
   "source": [
    "mac版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e562bedc-2f0d-4ee4-9fec-61263667dce9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/arkin/anaconda3/lib/python3.11/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /Users/arkin/anaconda3/lib/python3.11/site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in /Users/arkin/anaconda3/lib/python3.11/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /Users/arkin/anaconda3/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from torch) (4.13.1)\n",
      "Requirement already satisfied: networkx in /Users/arkin/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/arkin/anaconda3/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /Users/arkin/anaconda3/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: transformers in /Users/arkin/anaconda3/lib/python3.11/site-packages (4.55.4)\n",
      "Requirement already satisfied: filelock in /Users/arkin/anaconda3/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/arkin/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.13.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: modelscope in /Users/arkin/anaconda3/lib/python3.11/site-packages (1.29.1)\n",
      "Requirement already satisfied: filelock in /Users/arkin/anaconda3/lib/python3.11/site-packages (from modelscope) (3.13.1)\n",
      "Requirement already satisfied: requests>=2.25 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from modelscope) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /Users/arkin/anaconda3/lib/python3.11/site-packages (from modelscope) (68.2.2)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from modelscope) (4.65.0)\n",
      "Requirement already satisfied: urllib3>=1.26 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from modelscope) (2.0.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from requests>=2.25->modelscope) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from requests>=2.25->modelscope) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from requests>=2.25->modelscope) (2025.8.3)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: accelerate in /Users/arkin/anaconda3/lib/python3.11/site-packages (1.10.1)\n",
      "Requirement already satisfied: filelock in /Users/arkin/anaconda3/lib/python3.11/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-21.0.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Users/arkin/anaconda3/lib/python3.11/site-packages (from datasets) (2.1.4)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m240.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in /Users/arkin/anaconda3/lib/python3.11/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: psutil in /Users/arkin/anaconda3/lib/python3.11/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.8)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: networkx in /Users/arkin/anaconda3/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.9.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.3)\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m454.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m548.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-21.0.0-cp311-cp311-macosx_12_0_arm64.whl (31.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.2/31.2 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
      "Installing collected packages: xxhash, tqdm, requests, pyarrow, dill, multiprocess, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.65.0\n",
      "    Uninstalling tqdm-4.65.0:\n",
      "      Successfully uninstalled tqdm-4.65.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 14.0.2\n",
      "    Uninstalling pyarrow-14.0.2:\n",
      "      Successfully uninstalled pyarrow-14.0.2\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.7\n",
      "    Uninstalling dill-0.3.7:\n",
      "      Successfully uninstalled dill-0.3.7\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.75 requires requests_mock, which is not installed.\n",
      "conda-repo-cli 1.0.75 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.75 requires requests==2.31.0, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-4.0.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-21.0.0 requests-2.32.5 tqdm-4.67.1 xxhash-3.5.0\n",
      "Requirement already satisfied: peft in /Users/arkin/anaconda3/lib/python3.11/site-packages (0.17.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from peft) (23.1)\n",
      "Requirement already satisfied: psutil in /Users/arkin/anaconda3/lib/python3.11/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /Users/arkin/anaconda3/lib/python3.11/site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from peft) (2.6.0)\n",
      "Requirement already satisfied: transformers in /Users/arkin/anaconda3/lib/python3.11/site-packages (from peft) (4.55.4)\n",
      "Requirement already satisfied: tqdm in /Users/arkin/anaconda3/lib/python3.11/site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from peft) (1.10.1)\n",
      "Requirement already satisfied: safetensors in /Users/arkin/anaconda3/lib/python3.11/site-packages (from peft) (0.6.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from peft) (0.34.4)\n",
      "Requirement already satisfied: filelock in /Users/arkin/anaconda3/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft) (2023.10.0)\n",
      "Requirement already satisfied: requests in /Users/arkin/anaconda3/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft) (2.32.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft) (4.13.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft) (1.1.8)\n",
      "Requirement already satisfied: networkx in /Users/arkin/anaconda3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from transformers->peft) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from transformers->peft) (0.21.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/arkin/anaconda3/lib/python3.11/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install transformers\n",
    "!pip install modelscope\n",
    "!pip install datasets accelerate\n",
    "!pip install peft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ae0ef2-b29b-47c0-b000-2377b64d2c4e",
   "metadata": {},
   "source": [
    "### 环境版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6cad49f-9ba7-4f2a-b5b3-51f63735617e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.6.0\n",
      "transformers: 4.55.4\n",
      "modelscope: 1.29.1\n",
      "peft: 0.17.1\n",
      "datasets: 4.0.0\n",
      "accelerate: 1.10.1\n"
     ]
    }
   ],
   "source": [
    "# 📌 打印脚本相关库的版本信息\n",
    "import torch, transformers, modelscope, peft\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "\n",
    "# transformers 是 peft 和 modelscope 依赖的核心库\n",
    "try:\n",
    "    import transformers\n",
    "    print(\"transformers:\", transformers.__version__)\n",
    "except ImportError:\n",
    "    print(\"transformers: 未安装\")\n",
    "\n",
    "try:\n",
    "    import modelscope\n",
    "    print(\"modelscope:\", modelscope.__version__)\n",
    "except ImportError:\n",
    "    print(\"modelscope: 未安装\")\n",
    "\n",
    "try:\n",
    "    import peft\n",
    "    print(\"peft:\", peft.__version__)\n",
    "except ImportError:\n",
    "    print(\"peft: 未安装\")\n",
    "\n",
    "try:\n",
    "    import datasets\n",
    "    print(\"datasets:\", datasets.__version__)\n",
    "except ImportError:\n",
    "    print(\"datasets: 未安装\")\n",
    "\n",
    "try:\n",
    "    import accelerate\n",
    "    print(\"accelerate:\", accelerate.__version__)\n",
    "except ImportError:\n",
    "    print(\"accelerate: 未安装\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97315bf-cb93-4ceb-8bd6-6da5da3748af",
   "metadata": {},
   "source": [
    "## 三、下载模型和数据集（ModelScope版本）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfaeffd-b73a-491e-8273-a8db6a069535",
   "metadata": {},
   "source": [
    "### 模型下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e89730e-3b76-47ed-9c85-06672b867adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /Users/arkin/.cache/modelscope/hub/models/qwen/Qwen2.5-1.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 23:23:37,263 - modelscope - INFO - Got 7 files, start to download ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec18a152444436486f4e6120ea59a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing 7 items:   0%|          | 0.00/7.00 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f93f19f95ff4a85bcb2e1df6b82f449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer.json]:   0%|          | 0.00/6.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "929036b014534ed9bf86a697968c987f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [vocab.json]:   0%|          | 0.00/2.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73c0f926f034380b0292ac9429362a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [configuration.json]:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59382df398b24963964aa2891ccbd4b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [merges.txt]:   0%|          | 0.00/1.59M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d89baa75fa459d9130f256b5fc23ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [config.json]:   0%|          | 0.00/660 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd40378fec7f4e53aca747a42fd1fdc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer_config.json]:   0%|          | 0.00/7.13k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0533ecd88d6a4f76bb2433b66614b2a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [generation_config.json]:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 23:23:39,021 - modelscope - INFO - Download model 'qwen/Qwen2.5-1.5B-Instruct' successfully.\n",
      "2025-08-25 23:23:39,022 - modelscope - INFO - Creating symbolic link [/Users/arkin/.cache/modelscope/hub/models/qwen/Qwen2.5-1.5B-Instruct].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /Users/arkin/.cache/modelscope/hub/models/qwen/Qwen2.5-1.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 23:23:40,349 - modelscope - INFO - Got 3 files, start to download ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c600ea0df14ba481d4fc0b4ea17a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing 3 items:   0%|          | 0.00/3.00 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9adc388ce35c4fca972a50ba2514ad03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [README.md]:   0%|          | 0.00/4.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e220b47c29a14951ba236a504ee12a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [model.safetensors]:   0%|          | 0.00/2.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f9aed02fb24dabba289e0780f021c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [LICENSE]:   0%|          | 0.00/11.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from modelscope import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"qwen/Qwen2.5-1.5B-Instruct\"  # 可替换\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d92ea8-11cc-49a8-8df6-9cb7bf3df8e5",
   "metadata": {},
   "source": [
    "### 数据集下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea33c99f-f5fa-4115-b4dc-e3000b5f2f2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LargeList' from 'datasets' (/Users/arkin/anaconda3/lib/python3.11/site-packages/datasets/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodelscope\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmsdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MsDataset\n\u001b[1;32m      3\u001b[0m ds \u001b[38;5;241m=\u001b[39m  MsDataset\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAI-ModelScope/alpaca-gpt4-data-zh\u001b[39m\u001b[38;5;124m'\u001b[39m, subset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/modelscope/msdatasets/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Alibaba, Inc. and its affiliates.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodelscope\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmsdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mms_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MsDataset\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/modelscope/msdatasets/ms_dataset.py:25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodelscope\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmsdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_cls\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcustom_datasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \\\n\u001b[1;32m     23\u001b[0m     build_custom_dataset\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodelscope\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmsdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdelete_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetDeleteManager\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodelscope\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmsdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhf_datasets_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset_with_ctx\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodelscope\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmsdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mupload_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetUploadManager\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodelscope\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_preprocessor\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/modelscope/msdatasets/utils/hf_datasets_util.py:17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m urlencode\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (BuilderConfig, Dataset, DatasetBuilder, DatasetDict,\n\u001b[1;32m     18\u001b[0m                       DownloadConfig, DownloadManager, DownloadMode, Features,\n\u001b[1;32m     19\u001b[0m                       IterableDataset, IterableDatasetDict, Split,\n\u001b[1;32m     20\u001b[0m                       VerificationMode, Version, config, data_files, LargeList, Sequence \u001b[38;5;28;01mas\u001b[39;00m SequenceHf)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m features\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _FEATURE_TYPES\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LargeList' from 'datasets' (/Users/arkin/anaconda3/lib/python3.11/site-packages/datasets/__init__.py)"
     ]
    }
   ],
   "source": [
    "from modelscope.msdatasets import MsDataset\n",
    "\n",
    "ds =  MsDataset.load('AI-ModelScope/alpaca-gpt4-data-zh', subset_name='default', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef368533-dbe1-42ed-a8c2-2f07509923d8",
   "metadata": {},
   "source": [
    "## 四、数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f40f313-12aa-4dab-a9aa-7a2197706475",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 自定义数据处理函数(需要针对自己的数据集范式来编写，这里只针对alpaca)\n",
    "def preprocess(example):\n",
    "    # 丢掉 instruction 或 output 缺失的样本\n",
    "    if not example['instruction'] or not example['output']:\n",
    "        return None\n",
    "\n",
    "    # alpaca 数据有指令、输入、输出三个标签\n",
    "    instruction = example['instruction']\n",
    "    input_text = example.get('input') or \"\"  # input 可能为 None\n",
    "    output_text = example['output']\n",
    "\n",
    "    if input_text.strip():\n",
    "        prompt = f\"指令: {instruction}\\n输入: {input_text}\\n回答:\"\n",
    "    else:\n",
    "        prompt = f\"指令: {instruction}\\n回答:\"\n",
    "\n",
    "    full_text = prompt + output_text\n",
    "\n",
    "    enc = tokenizer(\n",
    "        full_text,  # 需要进行token化的文本\n",
    "        truncation=True,  # 文本过大的时候是否截断\n",
    "        max_length=16000,  # 根据模型和数据集决定，模型的上下文, 32k甚至更大\n",
    "        padding=\"max_length\",  # 🔹 保证长度一致，DataLoader 堆叠安全\n",
    "        return_tensors=\"pt\"  # 返回的数据类型，pt:pytorch.tensor; tf:tensorflow; np:numpy\n",
    "    )\n",
    "    # 单个样本是字典格式\n",
    "    return {\n",
    "        \"input_ids\": enc[\"input_ids\"][0],\n",
    "        \"labels\": enc[\"input_ids\"][0]\n",
    "    }\n",
    "\n",
    "train_dataset = ds.map(preprocess)\n",
    "train_dataset = train_dataset.filter(lambda x: x is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709d17b5-dcff-4b59-949c-3bdccc212392",
   "metadata": {},
   "source": [
    "### 将Dataset转化成DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee1615a-3038-4ffc-9eb7-7f42a0188d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# 🛠️ 自定义批处理函数 (collate_fn)\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    作用：\n",
    "    - DataLoader 会把一个 batch 的样本（list[dict]）传进来\n",
    "    - 这里需要手动拼接成 tensor，并且对齐长度（pad）\n",
    "    \"\"\"\n",
    "\n",
    "    # 取出每个样本的 input_ids 和 labels，转成 tensor\n",
    "    input_ids = [torch.tensor(item[\"input_ids\"]) for item in batch]\n",
    "    labels = [torch.tensor(item[\"labels\"]) for item in batch]\n",
    "\n",
    "    # 🔹 对 input_ids 做 padding\n",
    "    #   - batch_first=True: 结果形状 (batch_size, seq_len)\n",
    "    #   - padding_value=tokenizer.pad_token_id: 使用 tokenizer 的 pad_token_id 填充\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    # 🔹 对 labels 做 padding\n",
    "    #   - 注意这里 padding_value = -100\n",
    "    #   - 在 PyTorch 的 CrossEntropyLoss 里，-100 会被忽略，不参与 loss 计算\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(\n",
    "        labels, batch_first=True, padding_value=-100\n",
    "    )\n",
    "\n",
    "    # 返回字典，方便直接喂给模型\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "\n",
    "# 📊 数据子集（仅用于测试）\n",
    "# 这里为了快速验证流程，只取前 2000 条样本来训练\n",
    "small_dataset = Subset(train_dataset, range(2000))\n",
    "\n",
    "# 构建 DataLoader\n",
    "train_loader = DataLoader(\n",
    "    small_dataset,\n",
    "    batch_size=4,        # 每次取 4 个样本\n",
    "    shuffle=True,        # 打乱数据顺序\n",
    "    collate_fn=collate_fn  # 使用我们自定义的 batch 拼接逻辑\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb3096f-ff18-4add-b275-678d51aea408",
   "metadata": {},
   "source": [
    "## 五、LoRA微调"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7645b00-73b5-46f9-83fe-ad0145c55b83",
   "metadata": {},
   "source": [
    "### 🔧 配置 LoRA 训练参数\n",
    "\n",
    "```python\n",
    "lora_config = LoraConfig(\n",
    "    r=4,                          \n",
    "    lora_alpha=16,               \n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  \n",
    "    lora_dropout=0.05,             \n",
    "    bias=\"none\",               \n",
    "    task_type=\"CAUSAL_LM\"         \n",
    ")\n",
    "```\n",
    "\n",
    "r=4\n",
    "- 表示低秩矩阵的秩值（rank），值越大 → 适配能力更强 → 参数量也随之增加。  \n",
    "- 这里选择 `4`，意味着 **轻量级训练**，适合小规模任务或快速实验。  \n",
    "\n",
    "lora_alpha=16\n",
    "- 缩放因子，用于调整 LoRA 的输出幅度。  \n",
    "- 一般经验是 **lora_alpha ≈ 2 × r**，所以这里 `16` 配合 `r=4` 是合理的。  \n",
    "\n",
    "target_modules=[\"q_proj\", \"v_proj\"]\n",
    "- LoRA 只在注意力机制的 **Query** 和 **Value** 投影层中生效。  \n",
    "- 这是最常见的设置，既保证效果，又控制参数量。  \n",
    "\n",
    "lora_dropout=0.05\n",
    "- 在 LoRA 层中添加 **5% 的 dropout**，提升泛化能力。  \n",
    "- 数据量很大时可以调低到 `0`；数据少时可以适当调高（如 `0.1`）。  \n",
    "\n",
    "bias=\"none\"\n",
    "- 不训练 bias 参数，保证模型轻量化。  \n",
    "- 大多数场景下用 `\"none\"` 即可。  \n",
    "\n",
    "task_type=\"CAUSAL_LM\"\n",
    "- 表示任务是 **自回归语言建模**（比如 Qwen、GPT 类模型）。  \n",
    "- 必须和任务类型一致，否则 forward 过程会报错。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f547e8-9520-40aa-a9dd-3aeef97f305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=4,            \n",
    "    lora_alpha=16,                \n",
    "    target_modules=[\"q_proj\", \"v_proj\"], \n",
    "    lora_dropout=0.05,             \n",
    "    bias=\"none\",                    \n",
    "    task_type=\"CAUSAL_LM\"           \n",
    ")\n",
    "\n",
    "# 🚀 将基础模型包装为 PEFT 模型\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 打印当前可训练参数量（仅 LoRA 部分），其余参数被冻结\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# 训练超参数\n",
    "num_train_epochs = 2  # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb750b2e-e557-4946-90d3-254efded1b21",
   "metadata": {},
   "source": [
    "### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bd5b04-8876-4438-b711-db19c396adfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_train_epochs):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(model.device)\n",
    "        labels = batch[\"labels\"].to(model.device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        loss = outputs.loss··\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 100 == 0:   # 每 100 个 step 打印一次\n",
    "            print(f\"Epoch {epoch} | Step {step} | Loss {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
