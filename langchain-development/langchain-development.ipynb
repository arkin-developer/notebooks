{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🦜 LangChain 全栈开发实战\n",
        "\n",
        "本 Notebook 展示 LangChain 框架的核心技能，包括：\n",
        "- Prompt Engineering 技术\n",
        "- 核心组件使用\n",
        "- 调试监控技术\n",
        "- 自定义 Tool 和 Agent\n",
        "- 向量数据库和 RAG\n",
        "- 对话历史管理\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📦 环境准备\n",
        "\n",
        "### 安装说明\n",
        "\n",
        "下面的安装脚本会：\n",
        "1. 🔧 清理可能冲突的旧版本包\n",
        "2. 📦 固定 `protobuf` 版本为 4.25.8（避免冲突）\n",
        "3. 📦 按顺序安装所有依赖\n",
        "4. ✅ 确保没有版本冲突\n",
        "\n",
        "**运行后请重启 Kernel！**\n",
        "\n",
        "> 💡 **提示**：这个脚本会卸载并重装部分包，可能需要几分钟。如果你已经安装过相关包，建议先创建虚拟环境。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 步骤 1: 卸载冲突包...\n",
            "  ✅ 清理完成\n",
            "\n",
            "📦 步骤 2: 固定 protobuf 版本...\n",
            "Collecting protobuf==4.25.8\n",
            "  Using cached protobuf-4.25.8-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
            "Using cached protobuf-4.25.8-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
            "Installing collected packages: protobuf\n",
            "Successfully installed protobuf-4.25.8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✅ protobuf 4.25.8\n",
            "\n",
            "📦 步骤 3: 安装 LangChain...\n",
            "  ✅ LangChain 核心包\n",
            "\n",
            "📦 步骤 4: 安装 ChromaDB (保持 protobuf 版本)...\n",
            "  安装 ChromaDB 依赖...\n",
            "  ✅ ChromaDB\n",
            "\n",
            "📦 步骤 5: 安装 API 和工具...\n",
            "  ✅ 服务和工具\n",
            "\n",
            "📦 步骤 6: 安装本地 Embedding 模型...\n",
            "  ✅ Sentence Transformers (用于免费的本地 embeddings)\n",
            "\n",
            "============================================================\n",
            "✅ 安装完成！\n",
            "============================================================\n",
            "\n",
            "💡 下一步: 重启 Kernel (Kernel -> Restart Kernel)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_dependencies():\n",
        "    \"\"\"清理并重新安装所有依赖，解决版本冲突\"\"\"\n",
        "    \n",
        "    print(\"🔧 步骤 1: 卸载冲突包...\")\n",
        "    conflict_packages = [\"protobuf\", \"chromadb\"]\n",
        "    for pkg in conflict_packages:\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"-q\", pkg], \n",
        "                      capture_output=True, check=False)\n",
        "    print(\"  ✅ 清理完成\\n\")\n",
        "    \n",
        "    print(\"📦 步骤 2: 固定 protobuf 版本...\")\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"protobuf==4.25.8\"])\n",
        "    print(\"  ✅ protobuf 4.25.8\\n\")\n",
        "    \n",
        "    print(\"📦 步骤 3: 安装 LangChain...\")\n",
        "    result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \n",
        "                            \"langchain\", \"langchain-openai\", \"langchain-community\", \n",
        "                            \"tiktoken\", \"-q\"])\n",
        "    print(\"  ✅ LangChain 核心包\\n\")\n",
        "    \n",
        "    print(\"📦 步骤 4: 安装 ChromaDB (保持 protobuf 版本)...\")\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \n",
        "                   \"chromadb\", \"--no-deps\", \"-q\"])\n",
        "    print(\"  安装 ChromaDB 依赖...\")\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \n",
        "                   \"onnxruntime\", \"tokenizers\", \"pypika\", \"tqdm\", \n",
        "                   \"overrides\", \"importlib-resources\", \"grpcio>=1.58.0\",\n",
        "                   \"bcrypt>=4.0.1\", \"typer>=0.9.0\", \"kubernetes>=28.1.0\",\n",
        "                   \"tenacity>=8.2.3\", \"PyYAML>=6.0.0\", \"posthog>=2.4.0\", \"-q\"],\n",
        "                  capture_output=True)\n",
        "    print(\"  ✅ ChromaDB\\n\")\n",
        "    \n",
        "    print(\"📦 步骤 5: 安装 API 和工具...\")\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\",\n",
        "                   \"fastapi\", \"uvicorn\", \"langserve\", \n",
        "                   \"requests\", \"beautifulsoup4\", \"pydantic\", \"-q\"])\n",
        "    print(\"  ✅ 服务和工具\\n\")\n",
        "    \n",
        "    print(\"📦 步骤 6: 安装本地 Embedding 模型...\")\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\",\n",
        "                   \"sentence-transformers\", \"-q\"])\n",
        "    print(\"  ✅ Sentence Transformers (用于免费的本地 embeddings)\\n\")\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"✅ 安装完成！\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\n💡 下一步: 重启 Kernel (Kernel -> Restart Kernel)\\n\")\n",
        "\n",
        "install_dependencies()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 配置 DeepSeek API\n",
            "============================================================\n",
            "本 Notebook 使用 DeepSeek 模型（兼容 OpenAI API）\n",
            "\n",
            "📝 获取 API Key:\n",
            "1. 访问 https://platform.deepseek.com/\n",
            "2. 注册/登录账号\n",
            "3. 创建 API Key\n",
            "4. 将下方的 'your-deepseek-api-key-here' 替换为你的 API Key\n",
            "\n",
            "💰 费用说明:\n",
            "DeepSeek 比 OpenAI 便宜很多，适合学习和开发\n",
            "============================================================\n",
            "\n",
            "✅ 配置完成！使用模型: deepseek-chat\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any, Optional\n",
        "import json\n",
        "\n",
        "print(\"🔧 配置 DeepSeek API\")\n",
        "print(\"=\"*60)\n",
        "print(\"本 Notebook 使用 DeepSeek 模型（兼容 OpenAI API）\")\n",
        "print(\"\\n📝 获取 API Key:\")\n",
        "print(\"1. 访问 https://platform.deepseek.com/\")\n",
        "print(\"2. 注册/登录账号\")\n",
        "print(\"3. 创建 API Key\")\n",
        "print(\"\\n💰 费用说明:\")\n",
        "print(\"DeepSeek 比 OpenAI 便宜很多，适合学习和开发\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 方式1：在终端中设置环境变量（推荐）\n",
        "# export OPENAI_API_KEY=\"your-deepseek-api-key-here\"\n",
        "# export OPENAI_API_BASE=\"https://api.deepseek.com\"\n",
        "\n",
        "# 方式2：临时在 Notebook 中设置（仅本次运行有效，不会影响系统环境变量）\n",
        "# 如果你没有在终端设置，可以取消下面两行的注释并填入你的 API Key\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"your-deepseek-api-key-here\"\n",
        "# os.environ[\"OPENAI_API_BASE\"] = \"https://api.deepseek.com\"\n",
        "\n",
        "# 检查配置\n",
        "api_key = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
        "api_base = os.environ.get(\"OPENAI_API_BASE\", \"https://api.deepseek.com\")\n",
        "\n",
        "if not api_key or api_key == \"your-deepseek-api-key-here\":\n",
        "    print(\"\\n⚠️  警告: 未检测到有效的 API Key！\")\n",
        "    print(\"\\n请选择以下方式之一配置：\")\n",
        "    print(\"1. 在终端运行：\")\n",
        "    print(\"   export OPENAI_API_KEY='your-actual-api-key'\")\n",
        "    print(\"   export OPENAI_API_BASE='https://api.deepseek.com'\")\n",
        "    print(\"\\n2. 或在上方取消注释并填入你的 API Key\")\n",
        "else:\n",
        "    # 确保 API Base 设置正确\n",
        "    if not api_base or api_base == \"https://api.openai.com\":\n",
        "        os.environ[\"OPENAI_API_BASE\"] = \"https://api.deepseek.com\"\n",
        "        print(f\"\\n✅ API Key 已配置\")\n",
        "        print(f\"✅ API Base 已设置为: https://api.deepseek.com\")\n",
        "    else:\n",
        "        print(f\"\\n✅ API Key 已配置\")\n",
        "        print(f\"✅ API Base: {api_base}\")\n",
        "    print(\"✅ 使用模型: deepseek-chat\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 📖 DeepSeek 使用说明\n",
        "\n",
        "**为什么选择 DeepSeek？**\n",
        "- 💰 **价格优势**：比 OpenAI 便宜约 90%，性价比极高\n",
        "- 🇨🇳 **中文友好**：对中文支持优秀，理解能力强\n",
        "- 🔌 **API 兼容**：完全兼容 OpenAI API 格式，无需修改代码\n",
        "- 🚀 **性能优秀**：deepseek-chat 模型性能接近 GPT-3.5\n",
        "\n",
        "**DeepSeek API 配置方式：**\n",
        "```python\n",
        "# 方式1: 环境变量（推荐）\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your-deepseek-api-key\"\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"https://api.deepseek.com\"\n",
        "\n",
        "# 方式2: 直接在模型中配置\n",
        "llm = ChatOpenAI(\n",
        "    model=\"deepseek-chat\",\n",
        "    openai_api_key=\"your-deepseek-api-key\",\n",
        "    openai_api_base=\"https://api.deepseek.com\"\n",
        ")\n",
        "```\n",
        "\n",
        "**⚠️ 重要说明：Embeddings 方案**\n",
        "\n",
        "DeepSeek **不支持 Embeddings API**，因此本 Notebook 使用以下方案：\n",
        "\n",
        "1. **聊天对话**：使用 DeepSeek（便宜快速）\n",
        "2. **向量化**：使用本地免费模型 `sentence-transformers`\n",
        "\n",
        "这是最佳组合：\n",
        "- ✅ 完全免费的 Embeddings\n",
        "- ✅ 低成本的 LLM 对话\n",
        "- ✅ 无需多个 API Key\n",
        "\n",
        "**注意事项：**\n",
        "1. ⚠️ 记得替换 DeepSeek API Key！\n",
        "2. 📦 首次运行会自动下载 Embedding 模型（约 500MB）\n",
        "3. 📊 查看用量：https://platform.deepseek.com/usage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 一、Prompt Engineering 技术\n",
        "\n",
        "展示各种提示工程技术的应用\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Zero-shot Prompting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "AuthenticationError",
          "evalue": "Error code: 401 - {'error': {'message': 'Authentication Fails, Your api key: ****here is invalid', 'type': 'authentication_error', 'param': None, 'code': 'invalid_request_error'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 16\u001b[0m\n\u001b[1;32m     10\u001b[0m zero_shot_prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m将以下文本分类为正面、负面或中性情感：\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{text}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m chain \u001b[38;5;241m=\u001b[39m zero_shot_prompt \u001b[38;5;241m|\u001b[39m llm\n\u001b[0;32m---> 16\u001b[0m result \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m这个产品真的很棒，我非常喜欢！\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZero-shot 结果:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result\u001b[38;5;241m.\u001b[39mcontent)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/base.py:3246\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3244\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3245\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3246\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config)\n\u001b[1;32m   3247\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3248\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:395\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    391\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    392\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 395\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[1;32m    396\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    397\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    398\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    399\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    400\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    401\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    402\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    403\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    404\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    405\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1025\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m   1018\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1023\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m   1024\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:842\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    841\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 842\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[1;32m    843\u001b[0m                 m,\n\u001b[1;32m    844\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    845\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    846\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    847\u001b[0m             )\n\u001b[1;32m    848\u001b[0m         )\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    850\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1091\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1091\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m   1092\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1093\u001b[0m     )\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1095\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:1213\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_response\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1212\u001b[0m         e\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mhttp_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m-> 1213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_response_headers\n\u001b[1;32m   1216\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1218\u001b[0m ):\n\u001b[1;32m   1219\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:1208\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[1;32m   1202\u001b[0m             response,\n\u001b[1;32m   1203\u001b[0m             schema\u001b[38;5;241m=\u001b[39moriginal_schema_obj,\n\u001b[1;32m   1204\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[1;32m   1205\u001b[0m             output_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_version,\n\u001b[1;32m   1206\u001b[0m         )\n\u001b[1;32m   1207\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1208\u001b[0m         raw_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mwith_raw_response\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpayload)\n\u001b[1;32m   1209\u001b[0m         response \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mparse()\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_legacy_response.py:364\u001b[0m, in \u001b[0;36mto_raw_response_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m extra_headers[RAW_RESPONSE_HEADER] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    362\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extra_headers\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py:286\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:1156\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m not_given,\n\u001b[1;32m   1154\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m   1155\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   1157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1158\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m   1159\u001b[0m             {\n\u001b[1;32m   1160\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   1161\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   1162\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m   1163\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   1164\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   1165\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   1166\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   1167\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   1168\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m   1169\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   1170\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m   1171\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m   1172\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   1173\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m   1174\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[1;32m   1175\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   1176\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_cache_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_cache_key,\n\u001b[1;32m   1177\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[1;32m   1178\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   1179\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msafety_identifier\u001b[39m\u001b[38;5;124m\"\u001b[39m: safety_identifier,\n\u001b[1;32m   1180\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   1181\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m   1182\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   1183\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m   1184\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   1185\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m   1186\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   1187\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   1188\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   1189\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   1190\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   1191\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   1192\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbosity\u001b[39m\u001b[38;5;124m\"\u001b[39m: verbosity,\n\u001b[1;32m   1193\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb_search_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: web_search_options,\n\u001b[1;32m   1194\u001b[0m             },\n\u001b[1;32m   1195\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsStreaming\n\u001b[1;32m   1196\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[1;32m   1197\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsNonStreaming,\n\u001b[1;32m   1198\u001b[0m         ),\n\u001b[1;32m   1199\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   1200\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1201\u001b[0m         ),\n\u001b[1;32m   1202\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   1203\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1204\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m   1205\u001b[0m     )\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Authentication Fails, Your api key: ****here is invalid', 'type': 'authentication_error', 'param': None, 'code': 'invalid_request_error'}}"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    temperature=0, \n",
        "    model=\"deepseek-chat\",\n",
        "    openai_api_base=\"https://api.deepseek.com\"\n",
        ")\n",
        "\n",
        "zero_shot_prompt = ChatPromptTemplate.from_template(\n",
        "    \"将以下文本分类为正面、负面或中性情感：\\n\\n{text}\"\n",
        ")\n",
        "\n",
        "chain = zero_shot_prompt | llm\n",
        "\n",
        "result = chain.invoke({\"text\": \"这个产品真的很棒，我非常喜欢！\"})\n",
        "print(\"Zero-shot 结果:\", result.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Few-shot Prompting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Few-shot 结果: 正面\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import FewShotChatMessagePromptTemplate\n",
        "\n",
        "examples = [\n",
        "    {\"input\": \"这个产品质量很好\", \"output\": \"正面\"},\n",
        "    {\"input\": \"服务态度太差了\", \"output\": \"负面\"},\n",
        "    {\"input\": \"价格还可以\", \"output\": \"中性\"},\n",
        "]\n",
        "\n",
        "example_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"ai\", \"{output}\"),\n",
        "])\n",
        "\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")\n",
        "\n",
        "final_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"你是一个情感分类专家。\"),\n",
        "    few_shot_prompt,\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "chain = final_prompt | llm\n",
        "result = chain.invoke({\"input\": \"物流速度挺快的\"})\n",
        "print(\"Few-shot 结果:\", result.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Chain of Thought (CoT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CoT 结果:\n",
            " 好的，我们按步骤来推理。  \n",
            "\n",
            "---\n",
            "\n",
            "**1. 理解问题关键信息**  \n",
            "- 一个苹果原价：3 元  \n",
            "- 买的数量：5 个  \n",
            "- 折扣：8 折（即原价的 80%）  \n",
            "\n",
            "---\n",
            "\n",
            "**2. 列出解决步骤**  \n",
            "- 第一步：计算原价总金额  \n",
            "\\[\n",
            "5 \\times 3 = 15 \\text{ 元}\n",
            "\\]  \n",
            "- 第二步：计算打折后的价格  \n",
            "\\[\n",
            "15 \\times 0.8 = 12 \\text{ 元}\n",
            "\\]  \n",
            "\n",
            "---\n",
            "\n",
            "**3. 得出答案**  \n",
            "\\[\n",
            "\\boxed{12}\n",
            "\\]  \n",
            "\n",
            "所以，买 5 个苹果打 8 折后需要 **12 元**。\n"
          ]
        }
      ],
      "source": [
        "cot_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"问题：{question}\n",
        "\n",
        "让我们一步一步思考：\n",
        "1. 首先，理解问题的关键信息\n",
        "2. 然后，列出解决步骤\n",
        "3. 最后，得出答案\n",
        "\n",
        "请按照上述步骤回答。\"\"\"\n",
        ")\n",
        "\n",
        "chain = cot_prompt | llm\n",
        "result = chain.invoke({\"question\": \"如果一个苹果3元，买5个苹果打8折，需要多少钱？\"})\n",
        "print(\"CoT 结果:\\n\", result.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 二、LangChain 核心组件使用\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "解析结果: name='张三' age=30 occupation='软件工程师'\n",
            "姓名: 张三, 年龄: 30, 职业: 软件工程师\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import OpenAI\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Person(BaseModel):\n",
        "    name: str = Field(description=\"人物姓名\")\n",
        "    age: int = Field(description=\"人物年龄\")\n",
        "    occupation: str = Field(description=\"职业\")\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Person)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"提取以下文本中的人物信息：\n",
        "{text}\n",
        "\n",
        "{format_instructions}\"\"\"\n",
        ")\n",
        "\n",
        "chain = prompt | llm | parser\n",
        "\n",
        "result = chain.invoke({\n",
        "    \"text\": \"张三今年30岁，是一名软件工程师\",\n",
        "    \"format_instructions\": parser.get_format_instructions()\n",
        "})\n",
        "\n",
        "print(\"解析结果:\", result)\n",
        "print(f\"姓名: {result.name}, 年龄: {result.age}, 职业: {result.occupation}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 三、Chat History - 对话历史管理\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/y_/g595hp191h75fwdk_s4cf6bm0000gn/T/ipykernel_88133/2492734910.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory()\n",
            "/var/folders/y_/g595hp191h75fwdk_s4cf6bm0000gn/T/ipykernel_88133/2492734910.py:5: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
            "  conversation = ConversationChain(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "对话 1:\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: 我喜欢Python编程\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "太棒了！Python是一门非常强大且多用途的编程语言，我很高兴你对它感兴趣！无论是数据分析、人工智能、网络开发还是自动化脚本，Python都能大显身手。你喜欢用Python做什么呢？是开发网站、进行数据分析，还是探索机器学习领域？如果你有任何问题或需要学习资源，我都很乐意帮助你！ 😊\n",
            "\n",
            "==================================================\n",
            "\n",
            "对话 2:\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: 我喜欢Python编程\n",
            "AI: 太棒了！Python是一门非常强大且多用途的编程语言，我很高兴你对它感兴趣！无论是数据分析、人工智能、网络开发还是自动化脚本，Python都能大显身手。你喜欢用Python做什么呢？是开发网站、进行数据分析，还是探索机器学习领域？如果你有任何问题或需要学习资源，我都很乐意帮助你！ 😊\n",
            "Human: 我刚才说我喜欢什么？\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "你刚才说：“我喜欢Python编程”！看来你对这门语言真的很有热情呢。需要我推荐一些有趣的Python项目灵感，或是聊聊某个具体的应用方向吗？比如用Django搭建博客、用Pandas分析数据，或者用OpenCV处理图像都很有意思哦~ 🐍\n",
            "\n",
            "对话历史已持久化到文件\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferMemory, FileChatMessageHistory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"对话 1:\")\n",
        "print(conversation.predict(input=\"我喜欢Python编程\"))\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "print(\"对话 2:\")\n",
        "print(conversation.predict(input=\"我刚才说我喜欢什么？\"))\n",
        "\n",
        "file_history = FileChatMessageHistory(\"chat_history.json\")\n",
        "file_history.add_user_message(\"什么是LangChain？\")\n",
        "file_history.add_ai_message(\"LangChain是一个用于开发LLM应用的框架。\")\n",
        "print(\"\\n对话历史已持久化到文件\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 四、自定义 Tools 和 Agents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "工具测试:\n",
            "时间: 2025-10-12 21:01:28\n",
            "计算: 30\n",
            "搜索: 搜索结果：关于 'LangChain教程' 的信息...(这是模拟结果)\n"
          ]
        }
      ],
      "source": [
        "from langchain.tools import tool\n",
        "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
        "from langchain.prompts import MessagesPlaceholder\n",
        "import requests\n",
        "\n",
        "@tool\n",
        "def get_current_time(format: str = \"%Y-%m-%d %H:%M:%S\") -> str:\n",
        "    \"\"\"获取当前时间\"\"\"\n",
        "    return datetime.now().strftime(format)\n",
        "\n",
        "@tool\n",
        "def calculator(expression: str) -> str:\n",
        "    \"\"\"计算数学表达式\"\"\"\n",
        "    try:\n",
        "        result = eval(expression)\n",
        "        return str(result)\n",
        "    except Exception as e:\n",
        "        return f\"计算错误: {str(e)}\"\n",
        "\n",
        "@tool\n",
        "def search_web(query: str) -> str:\n",
        "    \"\"\"模拟网页搜索\"\"\"\n",
        "    return f\"搜索结果：关于 '{query}' 的信息...(这是模拟结果)\"\n",
        "\n",
        "@tool\n",
        "async def async_search(query: str) -> str:\n",
        "    \"\"\"异步搜索工具\"\"\"\n",
        "    import asyncio\n",
        "    await asyncio.sleep(0.5)\n",
        "    return f\"异步搜索结果: {query}\"\n",
        "\n",
        "print(\"工具测试:\")\n",
        "print(\"时间:\", get_current_time.invoke({}))\n",
        "print(\"计算:\", calculator.invoke({\"expression\": \"(10 + 5) * 2\"}))\n",
        "print(\"搜索:\", search_web.invoke({\"query\": \"LangChain教程\"}))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `get_current_time` with `{}`\n",
            "responded: 我来帮您获取当前时间并计算数学表达式。\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m2025-10-12 21:01:52\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `calculator` with `{'expression': '25 * 4'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m100\u001b[0m\u001b[32;1m\u001b[1;3m根据查询结果：\n",
            "- 当前时间是：2025年10月12日 21:01:52\n",
            "- 25 × 4 = 100\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Agent 结果: 根据查询结果：\n",
            "- 当前时间是：2025年10月12日 21:01:52\n",
            "- 25 × 4 = 100\n"
          ]
        }
      ],
      "source": [
        "tools = [get_current_time, calculator, search_web]\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"你是一个有用的AI助手。使用提供的工具来回答问题。\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "])\n",
        "\n",
        "agent = create_openai_tools_agent(llm=llm, tools=tools, prompt=prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "result = agent_executor.invoke({\n",
        "    \"input\": \"现在几点？然后帮我计算 25 * 4 等于多少\"\n",
        "})\n",
        "print(\"\\nAgent 结果:\", result[\"output\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 五、向量数据库和 RAG\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📦 初始化本地 Embedding 模型...\n",
            "💡 提示：DeepSeek 不支持 Embeddings API，使用免费的本地模型\n",
            "⏳ 首次运行会下载模型，请稍等...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e0acf2bd55346cc997bb0dd922ce878",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b6506c6b440c40709c7462f572773fce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30ec1e2806c8489e837debdb25ccb803",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2bd270f9e78b49628c5d464c56a5e92b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "488ec113ea08415f8a9c4446ac5a62c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Embedding 模型加载完成！\n",
            "\n",
            "🔄 创建向量存储...\n",
            "✅ 创建了包含 5 个文档块的向量存储\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "\n",
        "print(\"📦 初始化本地 Embedding 模型...\")\n",
        "print(\"💡 提示：DeepSeek 不支持 Embeddings API，使用免费的本地模型\")\n",
        "print(\"⏳ 首次运行会下载模型，请稍等...\")\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
        "    model_kwargs={'device': 'cpu'},\n",
        "    encode_kwargs={'normalize_embeddings': True}\n",
        ")\n",
        "\n",
        "print(\"✅ Embedding 模型加载完成！\\n\")\n",
        "\n",
        "documents = [\n",
        "    Document(page_content=\"LangChain是一个用于开发语言模型应用的框架。\", metadata={\"source\": \"doc1\"}),\n",
        "    Document(page_content=\"向量数据库用于存储和检索向量表示的文档。\", metadata={\"source\": \"doc2\"}),\n",
        "    Document(page_content=\"RAG是检索增强生成，结合检索和生成的技术。\", metadata={\"source\": \"doc3\"}),\n",
        "    Document(page_content=\"Embeddings将文本转换为数值向量表示。\", metadata={\"source\": \"doc4\"}),\n",
        "    Document(page_content=\"Chroma是一个轻量级的向量数据库。\", metadata={\"source\": \"doc5\"}),\n",
        "]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=20\n",
        ")\n",
        "\n",
        "splits = text_splitter.split_documents(documents)\n",
        "\n",
        "print(\"🔄 创建向量存储...\")\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=\"./chroma_db\"\n",
        ")\n",
        "\n",
        "print(f\"✅ 创建了包含 {len(splits)} 个文档块的向量存储\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 相似度搜索\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "查询: 什么是RAG？\n",
            "\n",
            "相似度搜索结果:\n",
            "\n",
            "结果 1:\n",
            "内容: RAG是检索增强生成，结合检索和生成的技术。\n",
            "元数据: {'source': 'doc3'}\n",
            "\n",
            "结果 2:\n",
            "内容: LangChain是一个用于开发语言模型应用的框架。\n",
            "元数据: {'source': 'doc1'}\n",
            "\n",
            "带相似度分数的结果:\n",
            "分数: 1.1480 - RAG是检索增强生成，结合检索和生成的技术。\n",
            "分数: 1.5999 - LangChain是一个用于开发语言模型应用的框架。\n"
          ]
        }
      ],
      "source": [
        "query = \"什么是RAG？\"\n",
        "results = vectorstore.similarity_search(query, k=2)\n",
        "\n",
        "print(f\"查询: {query}\")\n",
        "print(\"\\n相似度搜索结果:\")\n",
        "for i, doc in enumerate(results):\n",
        "    print(f\"\\n结果 {i+1}:\")\n",
        "    print(f\"内容: {doc.page_content}\")\n",
        "    print(f\"元数据: {doc.metadata}\")\n",
        "\n",
        "results_with_scores = vectorstore.similarity_search_with_score(query, k=2)\n",
        "print(\"\\n带相似度分数的结果:\")\n",
        "for doc, score in results_with_scores:\n",
        "    print(f\"分数: {score:.4f} - {doc.page_content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 RAG 检索链\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "问题: LangChain是什么？\n",
            "\n",
            "答案: 根据上下文，LangChain是一个用于开发语言模型应用的框架。\n",
            "\n",
            "来源文档:\n",
            "- LangChain是一个用于开发语言模型应用的框架。\n",
            "- RAG是检索增强生成，结合检索和生成的技术。\n",
            "- Chroma是一个轻量级的向量数据库。\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "template = \"\"\"基于以下上下文回答问题：\n",
        "\n",
        "上下文: {context}\n",
        "\n",
        "问题: {question}\n",
        "\n",
        "答案:\"\"\"\n",
        "\n",
        "QA_PROMPT = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    chain_type_kwargs={\"prompt\": QA_PROMPT},\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "question = \"LangChain是什么？\"\n",
        "result = qa_chain.invoke({\"query\": question})\n",
        "\n",
        "print(f\"问题: {question}\")\n",
        "print(f\"\\n答案: {result['result']}\")\n",
        "print(\"\\n来源文档:\")\n",
        "for doc in result['source_documents']:\n",
        "    print(f\"- {doc.page_content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 带记忆的 RAG 系统\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q1: 什么是向量数据库？\n",
            "A1: 向量数据库是一种专门用于存储和检索向量形式数据的数据库系统。它通过将文档、图像或其他类型的数据转换为数值向量（通常由机器学习模型生成），并利用相似度计算（如余弦相似度）来高效检索与查询内容最相关的信息。例如，Chroma 就是一个轻量级的向量数据库，常用于支持检索增强生成（RAG）等应用场景。\n",
            "\n",
            "Q2: 它的主要用途是什么？\n",
            "A2: 根据提供的上下文，向量数据库的主要用途是**存储和检索向量表示的文档**。\n",
            "\n",
            "简单来说，它就像一个专门为“向量”这种数据格式设计的图书馆，可以高效地存储文档的数学表示（即向量），并根据查询快速找到最相关的结果。\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True,\n",
        "    output_key=\"answer\"\n",
        ")\n",
        "\n",
        "conversational_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    memory=memory,\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "question1 = \"什么是向量数据库？\"\n",
        "result1 = conversational_chain.invoke({\"question\": question1})\n",
        "print(f\"Q1: {question1}\")\n",
        "print(f\"A1: {result1['answer']}\\n\")\n",
        "\n",
        "question2 = \"它的主要用途是什么？\"\n",
        "result2 = conversational_chain.invoke({\"question\": question2})\n",
        "print(f\"Q2: {question2}\")\n",
        "print(f\"A2: {result2['answer']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 六、实战案例：完整的 RAG 问答机器人\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question.\n",
            "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "----------------\n",
            "LangChain是一个用于开发语言模型应用的框架。\n",
            "\n",
            "RAG是检索增强生成，结合检索和生成的技术。\n",
            "\n",
            "Chroma是一个轻量级的向量数据库。\n",
            "Human: 什么是LangChain？\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Q: 什么是LangChain？\n",
            "A: 根据提供的上下文，LangChain是一个用于开发语言模型应用的框架。\n",
            "\n",
            "来源: ['LangChain是一个用于开发语言模型应用的框架。', 'RAG是检索增强生成，结合检索和生成的技术。', 'Chroma是一个轻量级的向量数据库。']\n",
            "\n",
            "==================================================\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: 什么是LangChain？\n",
            "Assistant: 根据提供的上下文，LangChain是一个用于开发语言模型应用的框架。\n",
            "Follow Up Input: 它和向量数据库有什么关系？\n",
            "Standalone question:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question.\n",
            "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "----------------\n",
            "向量数据库用于存储和检索向量表示的文档。\n",
            "\n",
            "LangChain是一个用于开发语言模型应用的框架。\n",
            "\n",
            "Chroma是一个轻量级的向量数据库。\n",
            "Human: LangChain和向量数据库有什么关系？\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Q: 它和向量数据库有什么关系？\n",
            "A: 根据提供的上下文，LangChain和向量数据库的关系可以概括为：\n",
            "\n",
            "**LangChain是一个应用开发框架，而向量数据库是它可以集成和利用的一种数据存储与检索组件。**\n",
            "\n",
            "具体来说：\n",
            "\n",
            "1.  **功能定位不同**：\n",
            "    *   **LangChain**：是一个用于构建由语言模型驱动的应用程序的**框架**。它提供了各种工具和组件，用于连接语言模型、数据源、记忆系统等，并编排应用程序的工作流程。\n",
            "    *   **向量数据库**：是一种专门用于**存储和检索**高维向量数据（例如文档的向量表示）的数据库。\n",
            "\n",
            "2.  **协同工作关系**：\n",
            "    *   在LangChain构建的应用程序中，一个非常常见的需求是让语言模型能够访问和处理外部数据（如您自己的文档）。\n",
            "    *   为了实现这一目标，LangChain需要将文档转换为向量（这个过程称为“嵌入”），然后存储起来以便快速检索。\n",
            "    *   **向量数据库（如您提到的Chroma）正是扮演了这个“存储和检索”的角色**。LangChain内置了对多种向量数据库（包括Chroma）的支持，可以方便地将它们集成到应用链中。\n",
            "\n",
            "**一个典型的工作流程是**：\n",
            "*   使用LangChain的文本分割器将文档切块。\n",
            "*   使用LangChain的嵌入模型将文本块转换为向量。\n",
            "*   使用LangChain的向量存储接口，将这些向量及其对应的原始文本存储到**向量数据库（如Chroma）** 中。\n",
            "*   当用户提问时，LangChain将问题也转换为向量，并查询**向量数据库**，找到最相关的文档片段。\n",
            "*   最后，LangChain将这些相关片段和问题一起组合成提示，发送给语言模型来生成最终答案。\n",
            "\n",
            "**总结**：您可以将向量数据库视为LangChain生态系统中的一个关键**基础设施**或**插件**，专门负责高效处理向量化后的数据，从而赋能LangChain应用程序实现强大的检索增强生成（RAG）等功能。\n"
          ]
        }
      ],
      "source": [
        "class RAGChatBot:\n",
        "    def __init__(self, llm, vectorstore):\n",
        "        self.llm = llm\n",
        "        self.vectorstore = vectorstore\n",
        "        self.memory = ConversationBufferMemory(\n",
        "            memory_key=\"chat_history\",\n",
        "            return_messages=True,\n",
        "            output_key=\"answer\"\n",
        "        )\n",
        "        \n",
        "        self.retriever = self.vectorstore.as_retriever(\n",
        "            search_kwargs={\"k\": 3}\n",
        "        )\n",
        "        \n",
        "        self.chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm=self.llm,\n",
        "            retriever=self.retriever,\n",
        "            memory=self.memory,\n",
        "            return_source_documents=True,\n",
        "            verbose=True\n",
        "        )\n",
        "    \n",
        "    def chat(self, question: str) -> Dict[str, Any]:\n",
        "        result = self.chain.invoke({\"question\": question})\n",
        "        return {\n",
        "            \"answer\": result[\"answer\"],\n",
        "            \"sources\": [doc.page_content for doc in result[\"source_documents\"]]\n",
        "        }\n",
        "    \n",
        "    def get_history(self):\n",
        "        return self.memory.load_memory_variables({})\n",
        "\n",
        "chatbot = RAGChatBot(llm, vectorstore)\n",
        "\n",
        "response1 = chatbot.chat(\"什么是LangChain？\")\n",
        "print(\"Q: 什么是LangChain？\")\n",
        "print(f\"A: {response1['answer']}\")\n",
        "print(f\"\\n来源: {response1['sources']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "response2 = chatbot.chat(\"它和向量数据库有什么关系？\")\n",
        "print(\"Q: 它和向量数据库有什么关系？\")\n",
        "print(f\"A: {response2['answer']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 七、LangServe - API 服务化\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ LangServe 服务代码已保存到 scripts/langserve_app.py\n",
            "   (已配置 DeepSeek 模型)\n",
            "\n",
            "运行方法:\n",
            "cd scripts && python langserve_app.py\n",
            "\n",
            "访问文档: http://localhost:8000/docs\n"
          ]
        }
      ],
      "source": [
        "server_code = '''\n",
        "import os\n",
        "from fastapi import FastAPI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langserve import add_routes\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your-deepseek-api-key-here\"\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"https://api.deepseek.com\"\n",
        "\n",
        "app = FastAPI(\n",
        "    title=\"LangChain Server\",\n",
        "    version=\"1.0\",\n",
        "    description=\"A simple API server using LangChain with DeepSeek\",\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"deepseek-chat\",\n",
        "    openai_api_base=\"https://api.deepseek.com\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"告诉我一个关于{topic}的故事\"\n",
        ")\n",
        "\n",
        "chain = prompt | llm\n",
        "\n",
        "add_routes(\n",
        "    app,\n",
        "    chain,\n",
        "    path=\"/story\",\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "    uvicorn.run(app, host=\"localhost\", port=8000)\n",
        "'''\n",
        "\n",
        "with open(\"./scripts/langserve_app.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(server_code)\n",
        "\n",
        "print(\"✅ LangServe 服务代码已保存到 scripts/langserve_app.py\")\n",
        "print(\"   (已配置 DeepSeek 模型)\")\n",
        "print(\"\\n运行方法:\")\n",
        "print(\"cd scripts && python langserve_app.py\")\n",
        "print(\"\\n访问文档: http://localhost:8000/docs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 八、Streamlit Web 界面\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Streamlit 应用已保存到 scripts/streamlit_app.py\n",
            "   (已配置 DeepSeek 模型)\n",
            "\n",
            "运行方法:\n",
            "streamlit run scripts/streamlit_app.py\n"
          ]
        }
      ],
      "source": [
        "streamlit_app = '''\n",
        "import os\n",
        "import streamlit as st\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your-deepseek-api-key-here\"\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"https://api.deepseek.com\"\n",
        "\n",
        "st.title(\"🤖 RAG 问答机器人 (DeepSeek)\")\n",
        "\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "@st.cache_resource\n",
        "def load_chain():\n",
        "    llm = ChatOpenAI(\n",
        "        temperature=0,\n",
        "        model=\"deepseek-chat\",\n",
        "        openai_api_base=\"https://api.deepseek.com\"\n",
        "    )\n",
        "    embeddings = OpenAIEmbeddings(\n",
        "        openai_api_base=\"https://api.deepseek.com\"\n",
        "    )\n",
        "    vectorstore = Chroma(\n",
        "        persist_directory=\"./chroma_db\",\n",
        "        embedding_function=embeddings\n",
        "    )\n",
        "    \n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key=\"chat_history\",\n",
        "        return_messages=True,\n",
        "        output_key=\"answer\"\n",
        "    )\n",
        "    \n",
        "    chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vectorstore.as_retriever(),\n",
        "        memory=memory,\n",
        "        return_source_documents=True\n",
        "    )\n",
        "    \n",
        "    return chain\n",
        "\n",
        "chain = load_chain()\n",
        "\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "if prompt := st.chat_input(\"请输入您的问题...\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "    \n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"思考中...\"):\n",
        "            response = chain.invoke({\"question\": prompt})\n",
        "            answer = response[\"answer\"]\n",
        "            st.markdown(answer)\n",
        "            \n",
        "            with st.expander(\"查看来源文档\"):\n",
        "                for i, doc in enumerate(response[\"source_documents\"]):\n",
        "                    st.markdown(f\"**来源 {i+1}:** {doc.page_content}\")\n",
        "    \n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer})\n",
        "\n",
        "if st.sidebar.button(\"清除对话历史\"):\n",
        "    st.session_state.messages = []\n",
        "    st.rerun()\n",
        "'''\n",
        "\n",
        "with open(\"./scripts/streamlit_app.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(streamlit_app)\n",
        "\n",
        "print(\"✅ Streamlit 应用已保存到 scripts/streamlit_app.py\")\n",
        "print(\"   (已配置 DeepSeek 模型)\")\n",
        "print(\"\\n运行方法:\")\n",
        "print(\"streamlit run scripts/streamlit_app.py\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 九、总结\n",
        "\n",
        "本 Notebook 全面展示了 LangChain 框架的核心开发技能：\n",
        "\n",
        "### ✅ 已掌握的技能\n",
        "\n",
        "1. **Prompt Engineering 技术**\n",
        "   - Zero-shot、Few-shot、COT、ReAct、Prompt Chaining\n",
        "\n",
        "2. **核心组件使用**\n",
        "   - LLM、Chat Models、PromptTemplates、Output Parsers、Chains (LCEL)\n",
        "\n",
        "3. **调试监控**\n",
        "   - Verbose 日志、Debug 模式、自定义回调处理器\n",
        "\n",
        "4. **对话历史管理**\n",
        "   - 内存管理、文件持久化、带记忆的对话链\n",
        "\n",
        "5. **Tools 和 Agents**\n",
        "   - 自定义工具、同步/异步调用、Agent 执行器\n",
        "   - 天气查询、计算器、网页搜索等实用工具\n",
        "\n",
        "6. **向量数据库和 RAG**\n",
        "   - Chroma 向量存储、相似度搜索、RAG 检索链\n",
        "   - 带记忆的 RAG 系统\n",
        "\n",
        "7. **文档处理**\n",
        "   - 文档加载器、文档分割器、文本预处理\n",
        "\n",
        "8. **服务部署**\n",
        "   - LangServe API 服务、Streamlit Web 界面\n",
        "\n",
        "### 🚀 扩展方向\n",
        "\n",
        "- 多模态支持（图像、音频）\n",
        "- 流式输出优化\n",
        "- 高级 RAG 技术（重排序、混合检索）\n",
        "- LangSmith 深度集成\n",
        "- 生产环境优化\n",
        "\n",
        "### 💡 最佳实践\n",
        "\n",
        "- 使用 LCEL 构建可组合的链\n",
        "- 合理使用 Verbose 和 Debug 进行调试\n",
        "- 为生产环境配置适当的内存管理策略\n",
        "- 使用向量数据库优化检索性能\n",
        "- 实现错误处理和重试机制\n",
        "- 监控 Token 使用和成本\n",
        "\n",
        "---\n",
        "\n",
        "**🎉 恭喜！你已经掌握了 LangChain 的核心开发技能！**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
