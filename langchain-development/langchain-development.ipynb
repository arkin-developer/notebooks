{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ¦œ LangChain å…¨æ ˆå¼€å‘å®æˆ˜\n",
        "\n",
        "æœ¬ Notebook å±•ç¤º LangChain æ¡†æ¶çš„æ ¸å¿ƒæŠ€èƒ½ï¼ŒåŒ…æ‹¬ï¼š\n",
        "- Prompt Engineering æŠ€æœ¯\n",
        "- æ ¸å¿ƒç»„ä»¶ä½¿ç”¨\n",
        "- è°ƒè¯•ç›‘æ§æŠ€æœ¯\n",
        "- è‡ªå®šä¹‰ Tool å’Œ Agent\n",
        "- å‘é‡æ•°æ®åº“å’Œ RAG\n",
        "- å¯¹è¯å†å²ç®¡ç†\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“¦ ç¯å¢ƒå‡†å¤‡\n",
        "\n",
        "### å®‰è£…è¯´æ˜\n",
        "\n",
        "ä¸‹é¢çš„å®‰è£…è„šæœ¬ä¼šï¼š\n",
        "1. ğŸ”§ æ¸…ç†å¯èƒ½å†²çªçš„æ—§ç‰ˆæœ¬åŒ…\n",
        "2. ğŸ“¦ å›ºå®š `protobuf` ç‰ˆæœ¬ä¸º 4.25.8ï¼ˆé¿å…å†²çªï¼‰\n",
        "3. ğŸ“¦ æŒ‰é¡ºåºå®‰è£…æ‰€æœ‰ä¾èµ–\n",
        "4. âœ… ç¡®ä¿æ²¡æœ‰ç‰ˆæœ¬å†²çª\n",
        "\n",
        "**è¿è¡Œåè¯·é‡å¯ Kernelï¼**\n",
        "\n",
        "> ğŸ’¡ **æç¤º**ï¼šè¿™ä¸ªè„šæœ¬ä¼šå¸è½½å¹¶é‡è£…éƒ¨åˆ†åŒ…ï¼Œå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿã€‚å¦‚æœä½ å·²ç»å®‰è£…è¿‡ç›¸å…³åŒ…ï¼Œå»ºè®®å…ˆåˆ›å»ºè™šæ‹Ÿç¯å¢ƒã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”§ æ­¥éª¤ 1: å¸è½½å†²çªåŒ…...\n",
            "  âœ… æ¸…ç†å®Œæˆ\n",
            "\n",
            "ğŸ“¦ æ­¥éª¤ 2: å›ºå®š protobuf ç‰ˆæœ¬...\n",
            "Collecting protobuf==4.25.8\n",
            "  Using cached protobuf-4.25.8-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
            "Using cached protobuf-4.25.8-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
            "Installing collected packages: protobuf\n",
            "Successfully installed protobuf-4.25.8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  âœ… protobuf 4.25.8\n",
            "\n",
            "ğŸ“¦ æ­¥éª¤ 3: å®‰è£… LangChain...\n",
            "  âœ… LangChain æ ¸å¿ƒåŒ…\n",
            "\n",
            "ğŸ“¦ æ­¥éª¤ 4: å®‰è£… ChromaDB (ä¿æŒ protobuf ç‰ˆæœ¬)...\n",
            "  å®‰è£… ChromaDB ä¾èµ–...\n",
            "  âœ… ChromaDB\n",
            "\n",
            "ğŸ“¦ æ­¥éª¤ 5: å®‰è£… API å’Œå·¥å…·...\n",
            "  âœ… æœåŠ¡å’Œå·¥å…·\n",
            "\n",
            "ğŸ“¦ æ­¥éª¤ 6: å®‰è£…æœ¬åœ° Embedding æ¨¡å‹...\n",
            "  âœ… Sentence Transformers (ç”¨äºå…è´¹çš„æœ¬åœ° embeddings)\n",
            "\n",
            "============================================================\n",
            "âœ… å®‰è£…å®Œæˆï¼\n",
            "============================================================\n",
            "\n",
            "ğŸ’¡ ä¸‹ä¸€æ­¥: é‡å¯ Kernel (Kernel -> Restart Kernel)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_dependencies():\n",
        "    \"\"\"æ¸…ç†å¹¶é‡æ–°å®‰è£…æ‰€æœ‰ä¾èµ–ï¼Œè§£å†³ç‰ˆæœ¬å†²çª\"\"\"\n",
        "    \n",
        "    print(\"ğŸ”§ æ­¥éª¤ 1: å¸è½½å†²çªåŒ…...\")\n",
        "    conflict_packages = [\"protobuf\", \"chromadb\"]\n",
        "    for pkg in conflict_packages:\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"-q\", pkg], \n",
        "                      capture_output=True, check=False)\n",
        "    print(\"  âœ… æ¸…ç†å®Œæˆ\\n\")\n",
        "    \n",
        "    print(\"ğŸ“¦ æ­¥éª¤ 2: å›ºå®š protobuf ç‰ˆæœ¬...\")\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"protobuf==4.25.8\"])\n",
        "    print(\"  âœ… protobuf 4.25.8\\n\")\n",
        "    \n",
        "    print(\"ğŸ“¦ æ­¥éª¤ 3: å®‰è£… LangChain...\")\n",
        "    result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \n",
        "                            \"langchain\", \"langchain-openai\", \"langchain-community\", \n",
        "                            \"tiktoken\", \"-q\"])\n",
        "    print(\"  âœ… LangChain æ ¸å¿ƒåŒ…\\n\")\n",
        "    \n",
        "    print(\"ğŸ“¦ æ­¥éª¤ 4: å®‰è£… ChromaDB (ä¿æŒ protobuf ç‰ˆæœ¬)...\")\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \n",
        "                   \"chromadb\", \"--no-deps\", \"-q\"])\n",
        "    print(\"  å®‰è£… ChromaDB ä¾èµ–...\")\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \n",
        "                   \"onnxruntime\", \"tokenizers\", \"pypika\", \"tqdm\", \n",
        "                   \"overrides\", \"importlib-resources\", \"grpcio>=1.58.0\",\n",
        "                   \"bcrypt>=4.0.1\", \"typer>=0.9.0\", \"kubernetes>=28.1.0\",\n",
        "                   \"tenacity>=8.2.3\", \"PyYAML>=6.0.0\", \"posthog>=2.4.0\", \"-q\"],\n",
        "                  capture_output=True)\n",
        "    print(\"  âœ… ChromaDB\\n\")\n",
        "    \n",
        "    print(\"ğŸ“¦ æ­¥éª¤ 5: å®‰è£… API å’Œå·¥å…·...\")\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\",\n",
        "                   \"fastapi\", \"uvicorn\", \"langserve\", \n",
        "                   \"requests\", \"beautifulsoup4\", \"pydantic\", \"-q\"])\n",
        "    print(\"  âœ… æœåŠ¡å’Œå·¥å…·\\n\")\n",
        "    \n",
        "    print(\"ğŸ“¦ æ­¥éª¤ 6: å®‰è£…æœ¬åœ° Embedding æ¨¡å‹...\")\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\",\n",
        "                   \"sentence-transformers\", \"-q\"])\n",
        "    print(\"  âœ… Sentence Transformers (ç”¨äºå…è´¹çš„æœ¬åœ° embeddings)\\n\")\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"âœ… å®‰è£…å®Œæˆï¼\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\nğŸ’¡ ä¸‹ä¸€æ­¥: é‡å¯ Kernel (Kernel -> Restart Kernel)\\n\")\n",
        "\n",
        "install_dependencies()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”§ é…ç½® DeepSeek API\n",
            "============================================================\n",
            "æœ¬ Notebook ä½¿ç”¨ DeepSeek æ¨¡å‹ï¼ˆå…¼å®¹ OpenAI APIï¼‰\n",
            "\n",
            "ğŸ“ è·å– API Key:\n",
            "1. è®¿é—® https://platform.deepseek.com/\n",
            "2. æ³¨å†Œ/ç™»å½•è´¦å·\n",
            "3. åˆ›å»º API Key\n",
            "4. å°†ä¸‹æ–¹çš„ 'your-deepseek-api-key-here' æ›¿æ¢ä¸ºä½ çš„ API Key\n",
            "\n",
            "ğŸ’° è´¹ç”¨è¯´æ˜:\n",
            "DeepSeek æ¯” OpenAI ä¾¿å®œå¾ˆå¤šï¼Œé€‚åˆå­¦ä¹ å’Œå¼€å‘\n",
            "============================================================\n",
            "\n",
            "âœ… é…ç½®å®Œæˆï¼ä½¿ç”¨æ¨¡å‹: deepseek-chat\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any, Optional\n",
        "import json\n",
        "\n",
        "print(\"ğŸ”§ é…ç½® DeepSeek API\")\n",
        "print(\"=\"*60)\n",
        "print(\"æœ¬ Notebook ä½¿ç”¨ DeepSeek æ¨¡å‹ï¼ˆå…¼å®¹ OpenAI APIï¼‰\")\n",
        "print(\"\\nğŸ“ è·å– API Key:\")\n",
        "print(\"1. è®¿é—® https://platform.deepseek.com/\")\n",
        "print(\"2. æ³¨å†Œ/ç™»å½•è´¦å·\")\n",
        "print(\"3. åˆ›å»º API Key\")\n",
        "print(\"\\nğŸ’° è´¹ç”¨è¯´æ˜:\")\n",
        "print(\"DeepSeek æ¯” OpenAI ä¾¿å®œå¾ˆå¤šï¼Œé€‚åˆå­¦ä¹ å’Œå¼€å‘\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# æ–¹å¼1ï¼šåœ¨ç»ˆç«¯ä¸­è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆæ¨èï¼‰\n",
        "# export OPENAI_API_KEY=\"your-deepseek-api-key-here\"\n",
        "# export OPENAI_API_BASE=\"https://api.deepseek.com\"\n",
        "\n",
        "# æ–¹å¼2ï¼šä¸´æ—¶åœ¨ Notebook ä¸­è®¾ç½®ï¼ˆä»…æœ¬æ¬¡è¿è¡Œæœ‰æ•ˆï¼Œä¸ä¼šå½±å“ç³»ç»Ÿç¯å¢ƒå˜é‡ï¼‰\n",
        "# å¦‚æœä½ æ²¡æœ‰åœ¨ç»ˆç«¯è®¾ç½®ï¼Œå¯ä»¥å–æ¶ˆä¸‹é¢ä¸¤è¡Œçš„æ³¨é‡Šå¹¶å¡«å…¥ä½ çš„ API Key\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"your-deepseek-api-key-here\"\n",
        "# os.environ[\"OPENAI_API_BASE\"] = \"https://api.deepseek.com\"\n",
        "\n",
        "# æ£€æŸ¥é…ç½®\n",
        "api_key = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
        "api_base = os.environ.get(\"OPENAI_API_BASE\", \"https://api.deepseek.com\")\n",
        "\n",
        "if not api_key or api_key == \"your-deepseek-api-key-here\":\n",
        "    print(\"\\nâš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ°æœ‰æ•ˆçš„ API Keyï¼\")\n",
        "    print(\"\\nè¯·é€‰æ‹©ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€é…ç½®ï¼š\")\n",
        "    print(\"1. åœ¨ç»ˆç«¯è¿è¡Œï¼š\")\n",
        "    print(\"   export OPENAI_API_KEY='your-actual-api-key'\")\n",
        "    print(\"   export OPENAI_API_BASE='https://api.deepseek.com'\")\n",
        "    print(\"\\n2. æˆ–åœ¨ä¸Šæ–¹å–æ¶ˆæ³¨é‡Šå¹¶å¡«å…¥ä½ çš„ API Key\")\n",
        "else:\n",
        "    # ç¡®ä¿ API Base è®¾ç½®æ­£ç¡®\n",
        "    if not api_base or api_base == \"https://api.openai.com\":\n",
        "        os.environ[\"OPENAI_API_BASE\"] = \"https://api.deepseek.com\"\n",
        "        print(f\"\\nâœ… API Key å·²é…ç½®\")\n",
        "        print(f\"âœ… API Base å·²è®¾ç½®ä¸º: https://api.deepseek.com\")\n",
        "    else:\n",
        "        print(f\"\\nâœ… API Key å·²é…ç½®\")\n",
        "        print(f\"âœ… API Base: {api_base}\")\n",
        "    print(\"âœ… ä½¿ç”¨æ¨¡å‹: deepseek-chat\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ“– DeepSeek ä½¿ç”¨è¯´æ˜\n",
        "\n",
        "**ä¸ºä»€ä¹ˆé€‰æ‹© DeepSeekï¼Ÿ**\n",
        "- ğŸ’° **ä»·æ ¼ä¼˜åŠ¿**ï¼šæ¯” OpenAI ä¾¿å®œçº¦ 90%ï¼Œæ€§ä»·æ¯”æé«˜\n",
        "- ğŸ‡¨ğŸ‡³ **ä¸­æ–‡å‹å¥½**ï¼šå¯¹ä¸­æ–‡æ”¯æŒä¼˜ç§€ï¼Œç†è§£èƒ½åŠ›å¼º\n",
        "- ğŸ”Œ **API å…¼å®¹**ï¼šå®Œå…¨å…¼å®¹ OpenAI API æ ¼å¼ï¼Œæ— éœ€ä¿®æ”¹ä»£ç \n",
        "- ğŸš€ **æ€§èƒ½ä¼˜ç§€**ï¼šdeepseek-chat æ¨¡å‹æ€§èƒ½æ¥è¿‘ GPT-3.5\n",
        "\n",
        "**DeepSeek API é…ç½®æ–¹å¼ï¼š**\n",
        "```python\n",
        "# æ–¹å¼1: ç¯å¢ƒå˜é‡ï¼ˆæ¨èï¼‰\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your-deepseek-api-key\"\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"https://api.deepseek.com\"\n",
        "\n",
        "# æ–¹å¼2: ç›´æ¥åœ¨æ¨¡å‹ä¸­é…ç½®\n",
        "llm = ChatOpenAI(\n",
        "    model=\"deepseek-chat\",\n",
        "    openai_api_key=\"your-deepseek-api-key\",\n",
        "    openai_api_base=\"https://api.deepseek.com\"\n",
        ")\n",
        "```\n",
        "\n",
        "**âš ï¸ é‡è¦è¯´æ˜ï¼šEmbeddings æ–¹æ¡ˆ**\n",
        "\n",
        "DeepSeek **ä¸æ”¯æŒ Embeddings API**ï¼Œå› æ­¤æœ¬ Notebook ä½¿ç”¨ä»¥ä¸‹æ–¹æ¡ˆï¼š\n",
        "\n",
        "1. **èŠå¤©å¯¹è¯**ï¼šä½¿ç”¨ DeepSeekï¼ˆä¾¿å®œå¿«é€Ÿï¼‰\n",
        "2. **å‘é‡åŒ–**ï¼šä½¿ç”¨æœ¬åœ°å…è´¹æ¨¡å‹ `sentence-transformers`\n",
        "\n",
        "è¿™æ˜¯æœ€ä½³ç»„åˆï¼š\n",
        "- âœ… å®Œå…¨å…è´¹çš„ Embeddings\n",
        "- âœ… ä½æˆæœ¬çš„ LLM å¯¹è¯\n",
        "- âœ… æ— éœ€å¤šä¸ª API Key\n",
        "\n",
        "**æ³¨æ„äº‹é¡¹ï¼š**\n",
        "1. âš ï¸ è®°å¾—æ›¿æ¢ DeepSeek API Keyï¼\n",
        "2. ğŸ“¦ é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½ Embedding æ¨¡å‹ï¼ˆçº¦ 500MBï¼‰\n",
        "3. ğŸ“Š æŸ¥çœ‹ç”¨é‡ï¼šhttps://platform.deepseek.com/usage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ä¸€ã€Prompt Engineering æŠ€æœ¯\n",
        "\n",
        "å±•ç¤ºå„ç§æç¤ºå·¥ç¨‹æŠ€æœ¯çš„åº”ç”¨\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Zero-shot Prompting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "AuthenticationError",
          "evalue": "Error code: 401 - {'error': {'message': 'Authentication Fails, Your api key: ****here is invalid', 'type': 'authentication_error', 'param': None, 'code': 'invalid_request_error'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 16\u001b[0m\n\u001b[1;32m     10\u001b[0m zero_shot_prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124må°†ä»¥ä¸‹æ–‡æœ¬åˆ†ç±»ä¸ºæ­£é¢ã€è´Ÿé¢æˆ–ä¸­æ€§æƒ…æ„Ÿï¼š\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{text}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m chain \u001b[38;5;241m=\u001b[39m zero_shot_prompt \u001b[38;5;241m|\u001b[39m llm\n\u001b[0;32m---> 16\u001b[0m result \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mè¿™ä¸ªäº§å“çœŸçš„å¾ˆæ£’ï¼Œæˆ‘éå¸¸å–œæ¬¢ï¼\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZero-shot ç»“æœ:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result\u001b[38;5;241m.\u001b[39mcontent)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/base.py:3246\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3244\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3245\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3246\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config)\n\u001b[1;32m   3247\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3248\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:395\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    391\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    392\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 395\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[1;32m    396\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    397\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    398\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    399\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    400\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    401\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    402\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    403\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    404\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    405\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1025\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m   1018\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1023\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m   1024\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:842\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    841\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 842\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[1;32m    843\u001b[0m                 m,\n\u001b[1;32m    844\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    845\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    846\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    847\u001b[0m             )\n\u001b[1;32m    848\u001b[0m         )\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    850\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1091\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1091\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m   1092\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1093\u001b[0m     )\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1095\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:1213\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_response\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1212\u001b[0m         e\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mhttp_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m-> 1213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_response_headers\n\u001b[1;32m   1216\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1218\u001b[0m ):\n\u001b[1;32m   1219\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:1208\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[1;32m   1202\u001b[0m             response,\n\u001b[1;32m   1203\u001b[0m             schema\u001b[38;5;241m=\u001b[39moriginal_schema_obj,\n\u001b[1;32m   1204\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[1;32m   1205\u001b[0m             output_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_version,\n\u001b[1;32m   1206\u001b[0m         )\n\u001b[1;32m   1207\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1208\u001b[0m         raw_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mwith_raw_response\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpayload)\n\u001b[1;32m   1209\u001b[0m         response \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mparse()\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_legacy_response.py:364\u001b[0m, in \u001b[0;36mto_raw_response_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m extra_headers[RAW_RESPONSE_HEADER] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    362\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extra_headers\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py:286\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:1156\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m not_given,\n\u001b[1;32m   1154\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m   1155\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   1157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1158\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m   1159\u001b[0m             {\n\u001b[1;32m   1160\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   1161\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   1162\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m   1163\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   1164\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   1165\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   1166\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   1167\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   1168\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m   1169\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   1170\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m   1171\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m   1172\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   1173\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m   1174\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[1;32m   1175\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   1176\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_cache_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_cache_key,\n\u001b[1;32m   1177\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[1;32m   1178\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   1179\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msafety_identifier\u001b[39m\u001b[38;5;124m\"\u001b[39m: safety_identifier,\n\u001b[1;32m   1180\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   1181\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m   1182\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   1183\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m   1184\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   1185\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m   1186\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   1187\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   1188\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   1189\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   1190\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   1191\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   1192\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbosity\u001b[39m\u001b[38;5;124m\"\u001b[39m: verbosity,\n\u001b[1;32m   1193\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb_search_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: web_search_options,\n\u001b[1;32m   1194\u001b[0m             },\n\u001b[1;32m   1195\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsStreaming\n\u001b[1;32m   1196\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[1;32m   1197\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsNonStreaming,\n\u001b[1;32m   1198\u001b[0m         ),\n\u001b[1;32m   1199\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   1200\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1201\u001b[0m         ),\n\u001b[1;32m   1202\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   1203\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1204\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m   1205\u001b[0m     )\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Authentication Fails, Your api key: ****here is invalid', 'type': 'authentication_error', 'param': None, 'code': 'invalid_request_error'}}"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    temperature=0, \n",
        "    model=\"deepseek-chat\",\n",
        "    openai_api_base=\"https://api.deepseek.com\"\n",
        ")\n",
        "\n",
        "zero_shot_prompt = ChatPromptTemplate.from_template(\n",
        "    \"å°†ä»¥ä¸‹æ–‡æœ¬åˆ†ç±»ä¸ºæ­£é¢ã€è´Ÿé¢æˆ–ä¸­æ€§æƒ…æ„Ÿï¼š\\n\\n{text}\"\n",
        ")\n",
        "\n",
        "chain = zero_shot_prompt | llm\n",
        "\n",
        "result = chain.invoke({\"text\": \"è¿™ä¸ªäº§å“çœŸçš„å¾ˆæ£’ï¼Œæˆ‘éå¸¸å–œæ¬¢ï¼\"})\n",
        "print(\"Zero-shot ç»“æœ:\", result.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Few-shot Prompting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Few-shot ç»“æœ: æ­£é¢\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import FewShotChatMessagePromptTemplate\n",
        "\n",
        "examples = [\n",
        "    {\"input\": \"è¿™ä¸ªäº§å“è´¨é‡å¾ˆå¥½\", \"output\": \"æ­£é¢\"},\n",
        "    {\"input\": \"æœåŠ¡æ€åº¦å¤ªå·®äº†\", \"output\": \"è´Ÿé¢\"},\n",
        "    {\"input\": \"ä»·æ ¼è¿˜å¯ä»¥\", \"output\": \"ä¸­æ€§\"},\n",
        "]\n",
        "\n",
        "example_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"ai\", \"{output}\"),\n",
        "])\n",
        "\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")\n",
        "\n",
        "final_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"ä½ æ˜¯ä¸€ä¸ªæƒ…æ„Ÿåˆ†ç±»ä¸“å®¶ã€‚\"),\n",
        "    few_shot_prompt,\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "chain = final_prompt | llm\n",
        "result = chain.invoke({\"input\": \"ç‰©æµé€Ÿåº¦æŒºå¿«çš„\"})\n",
        "print(\"Few-shot ç»“æœ:\", result.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Chain of Thought (CoT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CoT ç»“æœ:\n",
            " å¥½çš„ï¼Œæˆ‘ä»¬æŒ‰æ­¥éª¤æ¥æ¨ç†ã€‚  \n",
            "\n",
            "---\n",
            "\n",
            "**1. ç†è§£é—®é¢˜å…³é”®ä¿¡æ¯**  \n",
            "- ä¸€ä¸ªè‹¹æœåŸä»·ï¼š3 å…ƒ  \n",
            "- ä¹°çš„æ•°é‡ï¼š5 ä¸ª  \n",
            "- æŠ˜æ‰£ï¼š8 æŠ˜ï¼ˆå³åŸä»·çš„ 80%ï¼‰  \n",
            "\n",
            "---\n",
            "\n",
            "**2. åˆ—å‡ºè§£å†³æ­¥éª¤**  \n",
            "- ç¬¬ä¸€æ­¥ï¼šè®¡ç®—åŸä»·æ€»é‡‘é¢  \n",
            "\\[\n",
            "5 \\times 3 = 15 \\text{ å…ƒ}\n",
            "\\]  \n",
            "- ç¬¬äºŒæ­¥ï¼šè®¡ç®—æ‰“æŠ˜åçš„ä»·æ ¼  \n",
            "\\[\n",
            "15 \\times 0.8 = 12 \\text{ å…ƒ}\n",
            "\\]  \n",
            "\n",
            "---\n",
            "\n",
            "**3. å¾—å‡ºç­”æ¡ˆ**  \n",
            "\\[\n",
            "\\boxed{12}\n",
            "\\]  \n",
            "\n",
            "æ‰€ä»¥ï¼Œä¹° 5 ä¸ªè‹¹æœæ‰“ 8 æŠ˜åéœ€è¦ **12 å…ƒ**ã€‚\n"
          ]
        }
      ],
      "source": [
        "cot_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"é—®é¢˜ï¼š{question}\n",
        "\n",
        "è®©æˆ‘ä»¬ä¸€æ­¥ä¸€æ­¥æ€è€ƒï¼š\n",
        "1. é¦–å…ˆï¼Œç†è§£é—®é¢˜çš„å…³é”®ä¿¡æ¯\n",
        "2. ç„¶åï¼Œåˆ—å‡ºè§£å†³æ­¥éª¤\n",
        "3. æœ€åï¼Œå¾—å‡ºç­”æ¡ˆ\n",
        "\n",
        "è¯·æŒ‰ç…§ä¸Šè¿°æ­¥éª¤å›ç­”ã€‚\"\"\"\n",
        ")\n",
        "\n",
        "chain = cot_prompt | llm\n",
        "result = chain.invoke({\"question\": \"å¦‚æœä¸€ä¸ªè‹¹æœ3å…ƒï¼Œä¹°5ä¸ªè‹¹æœæ‰“8æŠ˜ï¼Œéœ€è¦å¤šå°‘é’±ï¼Ÿ\"})\n",
        "print(\"CoT ç»“æœ:\\n\", result.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## äºŒã€LangChain æ ¸å¿ƒç»„ä»¶ä½¿ç”¨\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "è§£æç»“æœ: name='å¼ ä¸‰' age=30 occupation='è½¯ä»¶å·¥ç¨‹å¸ˆ'\n",
            "å§“å: å¼ ä¸‰, å¹´é¾„: 30, èŒä¸š: è½¯ä»¶å·¥ç¨‹å¸ˆ\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import OpenAI\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Person(BaseModel):\n",
        "    name: str = Field(description=\"äººç‰©å§“å\")\n",
        "    age: int = Field(description=\"äººç‰©å¹´é¾„\")\n",
        "    occupation: str = Field(description=\"èŒä¸š\")\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Person)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"æå–ä»¥ä¸‹æ–‡æœ¬ä¸­çš„äººç‰©ä¿¡æ¯ï¼š\n",
        "{text}\n",
        "\n",
        "{format_instructions}\"\"\"\n",
        ")\n",
        "\n",
        "chain = prompt | llm | parser\n",
        "\n",
        "result = chain.invoke({\n",
        "    \"text\": \"å¼ ä¸‰ä»Šå¹´30å²ï¼Œæ˜¯ä¸€åè½¯ä»¶å·¥ç¨‹å¸ˆ\",\n",
        "    \"format_instructions\": parser.get_format_instructions()\n",
        "})\n",
        "\n",
        "print(\"è§£æç»“æœ:\", result)\n",
        "print(f\"å§“å: {result.name}, å¹´é¾„: {result.age}, èŒä¸š: {result.occupation}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ä¸‰ã€Chat History - å¯¹è¯å†å²ç®¡ç†\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/y_/g595hp191h75fwdk_s4cf6bm0000gn/T/ipykernel_88133/2492734910.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory()\n",
            "/var/folders/y_/g595hp191h75fwdk_s4cf6bm0000gn/T/ipykernel_88133/2492734910.py:5: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
            "  conversation = ConversationChain(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "å¯¹è¯ 1:\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: æˆ‘å–œæ¬¢Pythonç¼–ç¨‹\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "å¤ªæ£’äº†ï¼Pythonæ˜¯ä¸€é—¨éå¸¸å¼ºå¤§ä¸”å¤šç”¨é€”çš„ç¼–ç¨‹è¯­è¨€ï¼Œæˆ‘å¾ˆé«˜å…´ä½ å¯¹å®ƒæ„Ÿå…´è¶£ï¼æ— è®ºæ˜¯æ•°æ®åˆ†æã€äººå·¥æ™ºèƒ½ã€ç½‘ç»œå¼€å‘è¿˜æ˜¯è‡ªåŠ¨åŒ–è„šæœ¬ï¼ŒPythonéƒ½èƒ½å¤§æ˜¾èº«æ‰‹ã€‚ä½ å–œæ¬¢ç”¨Pythonåšä»€ä¹ˆå‘¢ï¼Ÿæ˜¯å¼€å‘ç½‘ç«™ã€è¿›è¡Œæ•°æ®åˆ†æï¼Œè¿˜æ˜¯æ¢ç´¢æœºå™¨å­¦ä¹ é¢†åŸŸï¼Ÿå¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å­¦ä¹ èµ„æºï¼Œæˆ‘éƒ½å¾ˆä¹æ„å¸®åŠ©ä½ ï¼ ğŸ˜Š\n",
            "\n",
            "==================================================\n",
            "\n",
            "å¯¹è¯ 2:\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: æˆ‘å–œæ¬¢Pythonç¼–ç¨‹\n",
            "AI: å¤ªæ£’äº†ï¼Pythonæ˜¯ä¸€é—¨éå¸¸å¼ºå¤§ä¸”å¤šç”¨é€”çš„ç¼–ç¨‹è¯­è¨€ï¼Œæˆ‘å¾ˆé«˜å…´ä½ å¯¹å®ƒæ„Ÿå…´è¶£ï¼æ— è®ºæ˜¯æ•°æ®åˆ†æã€äººå·¥æ™ºèƒ½ã€ç½‘ç»œå¼€å‘è¿˜æ˜¯è‡ªåŠ¨åŒ–è„šæœ¬ï¼ŒPythonéƒ½èƒ½å¤§æ˜¾èº«æ‰‹ã€‚ä½ å–œæ¬¢ç”¨Pythonåšä»€ä¹ˆå‘¢ï¼Ÿæ˜¯å¼€å‘ç½‘ç«™ã€è¿›è¡Œæ•°æ®åˆ†æï¼Œè¿˜æ˜¯æ¢ç´¢æœºå™¨å­¦ä¹ é¢†åŸŸï¼Ÿå¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å­¦ä¹ èµ„æºï¼Œæˆ‘éƒ½å¾ˆä¹æ„å¸®åŠ©ä½ ï¼ ğŸ˜Š\n",
            "Human: æˆ‘åˆšæ‰è¯´æˆ‘å–œæ¬¢ä»€ä¹ˆï¼Ÿ\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "ä½ åˆšæ‰è¯´ï¼šâ€œæˆ‘å–œæ¬¢Pythonç¼–ç¨‹â€ï¼çœ‹æ¥ä½ å¯¹è¿™é—¨è¯­è¨€çœŸçš„å¾ˆæœ‰çƒ­æƒ…å‘¢ã€‚éœ€è¦æˆ‘æ¨èä¸€äº›æœ‰è¶£çš„Pythoné¡¹ç›®çµæ„Ÿï¼Œæˆ–æ˜¯èŠèŠæŸä¸ªå…·ä½“çš„åº”ç”¨æ–¹å‘å—ï¼Ÿæ¯”å¦‚ç”¨Djangoæ­å»ºåšå®¢ã€ç”¨Pandasåˆ†ææ•°æ®ï¼Œæˆ–è€…ç”¨OpenCVå¤„ç†å›¾åƒéƒ½å¾ˆæœ‰æ„æ€å“¦~ ğŸ\n",
            "\n",
            "å¯¹è¯å†å²å·²æŒä¹…åŒ–åˆ°æ–‡ä»¶\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferMemory, FileChatMessageHistory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"å¯¹è¯ 1:\")\n",
        "print(conversation.predict(input=\"æˆ‘å–œæ¬¢Pythonç¼–ç¨‹\"))\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "print(\"å¯¹è¯ 2:\")\n",
        "print(conversation.predict(input=\"æˆ‘åˆšæ‰è¯´æˆ‘å–œæ¬¢ä»€ä¹ˆï¼Ÿ\"))\n",
        "\n",
        "file_history = FileChatMessageHistory(\"chat_history.json\")\n",
        "file_history.add_user_message(\"ä»€ä¹ˆæ˜¯LangChainï¼Ÿ\")\n",
        "file_history.add_ai_message(\"LangChainæ˜¯ä¸€ä¸ªç”¨äºå¼€å‘LLMåº”ç”¨çš„æ¡†æ¶ã€‚\")\n",
        "print(\"\\nå¯¹è¯å†å²å·²æŒä¹…åŒ–åˆ°æ–‡ä»¶\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## å››ã€è‡ªå®šä¹‰ Tools å’Œ Agents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "å·¥å…·æµ‹è¯•:\n",
            "æ—¶é—´: 2025-10-12 21:01:28\n",
            "è®¡ç®—: 30\n",
            "æœç´¢: æœç´¢ç»“æœï¼šå…³äº 'LangChainæ•™ç¨‹' çš„ä¿¡æ¯...(è¿™æ˜¯æ¨¡æ‹Ÿç»“æœ)\n"
          ]
        }
      ],
      "source": [
        "from langchain.tools import tool\n",
        "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
        "from langchain.prompts import MessagesPlaceholder\n",
        "import requests\n",
        "\n",
        "@tool\n",
        "def get_current_time(format: str = \"%Y-%m-%d %H:%M:%S\") -> str:\n",
        "    \"\"\"è·å–å½“å‰æ—¶é—´\"\"\"\n",
        "    return datetime.now().strftime(format)\n",
        "\n",
        "@tool\n",
        "def calculator(expression: str) -> str:\n",
        "    \"\"\"è®¡ç®—æ•°å­¦è¡¨è¾¾å¼\"\"\"\n",
        "    try:\n",
        "        result = eval(expression)\n",
        "        return str(result)\n",
        "    except Exception as e:\n",
        "        return f\"è®¡ç®—é”™è¯¯: {str(e)}\"\n",
        "\n",
        "@tool\n",
        "def search_web(query: str) -> str:\n",
        "    \"\"\"æ¨¡æ‹Ÿç½‘é¡µæœç´¢\"\"\"\n",
        "    return f\"æœç´¢ç»“æœï¼šå…³äº '{query}' çš„ä¿¡æ¯...(è¿™æ˜¯æ¨¡æ‹Ÿç»“æœ)\"\n",
        "\n",
        "@tool\n",
        "async def async_search(query: str) -> str:\n",
        "    \"\"\"å¼‚æ­¥æœç´¢å·¥å…·\"\"\"\n",
        "    import asyncio\n",
        "    await asyncio.sleep(0.5)\n",
        "    return f\"å¼‚æ­¥æœç´¢ç»“æœ: {query}\"\n",
        "\n",
        "print(\"å·¥å…·æµ‹è¯•:\")\n",
        "print(\"æ—¶é—´:\", get_current_time.invoke({}))\n",
        "print(\"è®¡ç®—:\", calculator.invoke({\"expression\": \"(10 + 5) * 2\"}))\n",
        "print(\"æœç´¢:\", search_web.invoke({\"query\": \"LangChainæ•™ç¨‹\"}))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `get_current_time` with `{}`\n",
            "responded: æˆ‘æ¥å¸®æ‚¨è·å–å½“å‰æ—¶é—´å¹¶è®¡ç®—æ•°å­¦è¡¨è¾¾å¼ã€‚\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m2025-10-12 21:01:52\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `calculator` with `{'expression': '25 * 4'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m100\u001b[0m\u001b[32;1m\u001b[1;3mæ ¹æ®æŸ¥è¯¢ç»“æœï¼š\n",
            "- å½“å‰æ—¶é—´æ˜¯ï¼š2025å¹´10æœˆ12æ—¥ 21:01:52\n",
            "- 25 Ã— 4 = 100\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Agent ç»“æœ: æ ¹æ®æŸ¥è¯¢ç»“æœï¼š\n",
            "- å½“å‰æ—¶é—´æ˜¯ï¼š2025å¹´10æœˆ12æ—¥ 21:01:52\n",
            "- 25 Ã— 4 = 100\n"
          ]
        }
      ],
      "source": [
        "tools = [get_current_time, calculator, search_web]\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚ä½¿ç”¨æä¾›çš„å·¥å…·æ¥å›ç­”é—®é¢˜ã€‚\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "])\n",
        "\n",
        "agent = create_openai_tools_agent(llm=llm, tools=tools, prompt=prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "result = agent_executor.invoke({\n",
        "    \"input\": \"ç°åœ¨å‡ ç‚¹ï¼Ÿç„¶åå¸®æˆ‘è®¡ç®— 25 * 4 ç­‰äºå¤šå°‘\"\n",
        "})\n",
        "print(\"\\nAgent ç»“æœ:\", result[\"output\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## äº”ã€å‘é‡æ•°æ®åº“å’Œ RAG\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“¦ åˆå§‹åŒ–æœ¬åœ° Embedding æ¨¡å‹...\n",
            "ğŸ’¡ æç¤ºï¼šDeepSeek ä¸æ”¯æŒ Embeddings APIï¼Œä½¿ç”¨å…è´¹çš„æœ¬åœ°æ¨¡å‹\n",
            "â³ é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ¨¡å‹ï¼Œè¯·ç¨ç­‰...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e0acf2bd55346cc997bb0dd922ce878",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b6506c6b440c40709c7462f572773fce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30ec1e2806c8489e837debdb25ccb803",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2bd270f9e78b49628c5d464c56a5e92b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "488ec113ea08415f8a9c4446ac5a62c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Embedding æ¨¡å‹åŠ è½½å®Œæˆï¼\n",
            "\n",
            "ğŸ”„ åˆ›å»ºå‘é‡å­˜å‚¨...\n",
            "âœ… åˆ›å»ºäº†åŒ…å« 5 ä¸ªæ–‡æ¡£å—çš„å‘é‡å­˜å‚¨\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "\n",
        "print(\"ğŸ“¦ åˆå§‹åŒ–æœ¬åœ° Embedding æ¨¡å‹...\")\n",
        "print(\"ğŸ’¡ æç¤ºï¼šDeepSeek ä¸æ”¯æŒ Embeddings APIï¼Œä½¿ç”¨å…è´¹çš„æœ¬åœ°æ¨¡å‹\")\n",
        "print(\"â³ é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ¨¡å‹ï¼Œè¯·ç¨ç­‰...\")\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
        "    model_kwargs={'device': 'cpu'},\n",
        "    encode_kwargs={'normalize_embeddings': True}\n",
        ")\n",
        "\n",
        "print(\"âœ… Embedding æ¨¡å‹åŠ è½½å®Œæˆï¼\\n\")\n",
        "\n",
        "documents = [\n",
        "    Document(page_content=\"LangChainæ˜¯ä¸€ä¸ªç”¨äºå¼€å‘è¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶ã€‚\", metadata={\"source\": \"doc1\"}),\n",
        "    Document(page_content=\"å‘é‡æ•°æ®åº“ç”¨äºå­˜å‚¨å’Œæ£€ç´¢å‘é‡è¡¨ç¤ºçš„æ–‡æ¡£ã€‚\", metadata={\"source\": \"doc2\"}),\n",
        "    Document(page_content=\"RAGæ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„æŠ€æœ¯ã€‚\", metadata={\"source\": \"doc3\"}),\n",
        "    Document(page_content=\"Embeddingså°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å€¼å‘é‡è¡¨ç¤ºã€‚\", metadata={\"source\": \"doc4\"}),\n",
        "    Document(page_content=\"Chromaæ˜¯ä¸€ä¸ªè½»é‡çº§çš„å‘é‡æ•°æ®åº“ã€‚\", metadata={\"source\": \"doc5\"}),\n",
        "]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=20\n",
        ")\n",
        "\n",
        "splits = text_splitter.split_documents(documents)\n",
        "\n",
        "print(\"ğŸ”„ åˆ›å»ºå‘é‡å­˜å‚¨...\")\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=\"./chroma_db\"\n",
        ")\n",
        "\n",
        "print(f\"âœ… åˆ›å»ºäº†åŒ…å« {len(splits)} ä¸ªæ–‡æ¡£å—çš„å‘é‡å­˜å‚¨\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 ç›¸ä¼¼åº¦æœç´¢\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "æŸ¥è¯¢: ä»€ä¹ˆæ˜¯RAGï¼Ÿ\n",
            "\n",
            "ç›¸ä¼¼åº¦æœç´¢ç»“æœ:\n",
            "\n",
            "ç»“æœ 1:\n",
            "å†…å®¹: RAGæ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„æŠ€æœ¯ã€‚\n",
            "å…ƒæ•°æ®: {'source': 'doc3'}\n",
            "\n",
            "ç»“æœ 2:\n",
            "å†…å®¹: LangChainæ˜¯ä¸€ä¸ªç”¨äºå¼€å‘è¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶ã€‚\n",
            "å…ƒæ•°æ®: {'source': 'doc1'}\n",
            "\n",
            "å¸¦ç›¸ä¼¼åº¦åˆ†æ•°çš„ç»“æœ:\n",
            "åˆ†æ•°: 1.1480 - RAGæ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„æŠ€æœ¯ã€‚\n",
            "åˆ†æ•°: 1.5999 - LangChainæ˜¯ä¸€ä¸ªç”¨äºå¼€å‘è¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶ã€‚\n"
          ]
        }
      ],
      "source": [
        "query = \"ä»€ä¹ˆæ˜¯RAGï¼Ÿ\"\n",
        "results = vectorstore.similarity_search(query, k=2)\n",
        "\n",
        "print(f\"æŸ¥è¯¢: {query}\")\n",
        "print(\"\\nç›¸ä¼¼åº¦æœç´¢ç»“æœ:\")\n",
        "for i, doc in enumerate(results):\n",
        "    print(f\"\\nç»“æœ {i+1}:\")\n",
        "    print(f\"å†…å®¹: {doc.page_content}\")\n",
        "    print(f\"å…ƒæ•°æ®: {doc.metadata}\")\n",
        "\n",
        "results_with_scores = vectorstore.similarity_search_with_score(query, k=2)\n",
        "print(\"\\nå¸¦ç›¸ä¼¼åº¦åˆ†æ•°çš„ç»“æœ:\")\n",
        "for doc, score in results_with_scores:\n",
        "    print(f\"åˆ†æ•°: {score:.4f} - {doc.page_content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 RAG æ£€ç´¢é“¾\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "é—®é¢˜: LangChainæ˜¯ä»€ä¹ˆï¼Ÿ\n",
            "\n",
            "ç­”æ¡ˆ: æ ¹æ®ä¸Šä¸‹æ–‡ï¼ŒLangChainæ˜¯ä¸€ä¸ªç”¨äºå¼€å‘è¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶ã€‚\n",
            "\n",
            "æ¥æºæ–‡æ¡£:\n",
            "- LangChainæ˜¯ä¸€ä¸ªç”¨äºå¼€å‘è¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶ã€‚\n",
            "- RAGæ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„æŠ€æœ¯ã€‚\n",
            "- Chromaæ˜¯ä¸€ä¸ªè½»é‡çº§çš„å‘é‡æ•°æ®åº“ã€‚\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "template = \"\"\"åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š\n",
        "\n",
        "ä¸Šä¸‹æ–‡: {context}\n",
        "\n",
        "é—®é¢˜: {question}\n",
        "\n",
        "ç­”æ¡ˆ:\"\"\"\n",
        "\n",
        "QA_PROMPT = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    chain_type_kwargs={\"prompt\": QA_PROMPT},\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "question = \"LangChainæ˜¯ä»€ä¹ˆï¼Ÿ\"\n",
        "result = qa_chain.invoke({\"query\": question})\n",
        "\n",
        "print(f\"é—®é¢˜: {question}\")\n",
        "print(f\"\\nç­”æ¡ˆ: {result['result']}\")\n",
        "print(\"\\næ¥æºæ–‡æ¡£:\")\n",
        "for doc in result['source_documents']:\n",
        "    print(f\"- {doc.page_content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 å¸¦è®°å¿†çš„ RAG ç³»ç»Ÿ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q1: ä»€ä¹ˆæ˜¯å‘é‡æ•°æ®åº“ï¼Ÿ\n",
            "A1: å‘é‡æ•°æ®åº“æ˜¯ä¸€ç§ä¸“é—¨ç”¨äºå­˜å‚¨å’Œæ£€ç´¢å‘é‡å½¢å¼æ•°æ®çš„æ•°æ®åº“ç³»ç»Ÿã€‚å®ƒé€šè¿‡å°†æ–‡æ¡£ã€å›¾åƒæˆ–å…¶ä»–ç±»å‹çš„æ•°æ®è½¬æ¢ä¸ºæ•°å€¼å‘é‡ï¼ˆé€šå¸¸ç”±æœºå™¨å­¦ä¹ æ¨¡å‹ç”Ÿæˆï¼‰ï¼Œå¹¶åˆ©ç”¨ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆå¦‚ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰æ¥é«˜æ•ˆæ£€ç´¢ä¸æŸ¥è¯¢å†…å®¹æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼ŒChroma å°±æ˜¯ä¸€ä¸ªè½»é‡çº§çš„å‘é‡æ•°æ®åº“ï¼Œå¸¸ç”¨äºæ”¯æŒæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç­‰åº”ç”¨åœºæ™¯ã€‚\n",
            "\n",
            "Q2: å®ƒçš„ä¸»è¦ç”¨é€”æ˜¯ä»€ä¹ˆï¼Ÿ\n",
            "A2: æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ï¼Œå‘é‡æ•°æ®åº“çš„ä¸»è¦ç”¨é€”æ˜¯**å­˜å‚¨å’Œæ£€ç´¢å‘é‡è¡¨ç¤ºçš„æ–‡æ¡£**ã€‚\n",
            "\n",
            "ç®€å•æ¥è¯´ï¼Œå®ƒå°±åƒä¸€ä¸ªä¸“é—¨ä¸ºâ€œå‘é‡â€è¿™ç§æ•°æ®æ ¼å¼è®¾è®¡çš„å›¾ä¹¦é¦†ï¼Œå¯ä»¥é«˜æ•ˆåœ°å­˜å‚¨æ–‡æ¡£çš„æ•°å­¦è¡¨ç¤ºï¼ˆå³å‘é‡ï¼‰ï¼Œå¹¶æ ¹æ®æŸ¥è¯¢å¿«é€Ÿæ‰¾åˆ°æœ€ç›¸å…³çš„ç»“æœã€‚\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True,\n",
        "    output_key=\"answer\"\n",
        ")\n",
        "\n",
        "conversational_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    memory=memory,\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "question1 = \"ä»€ä¹ˆæ˜¯å‘é‡æ•°æ®åº“ï¼Ÿ\"\n",
        "result1 = conversational_chain.invoke({\"question\": question1})\n",
        "print(f\"Q1: {question1}\")\n",
        "print(f\"A1: {result1['answer']}\\n\")\n",
        "\n",
        "question2 = \"å®ƒçš„ä¸»è¦ç”¨é€”æ˜¯ä»€ä¹ˆï¼Ÿ\"\n",
        "result2 = conversational_chain.invoke({\"question\": question2})\n",
        "print(f\"Q2: {question2}\")\n",
        "print(f\"A2: {result2['answer']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## å…­ã€å®æˆ˜æ¡ˆä¾‹ï¼šå®Œæ•´çš„ RAG é—®ç­”æœºå™¨äºº\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question.\n",
            "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "----------------\n",
            "LangChainæ˜¯ä¸€ä¸ªç”¨äºå¼€å‘è¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶ã€‚\n",
            "\n",
            "RAGæ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„æŠ€æœ¯ã€‚\n",
            "\n",
            "Chromaæ˜¯ä¸€ä¸ªè½»é‡çº§çš„å‘é‡æ•°æ®åº“ã€‚\n",
            "Human: ä»€ä¹ˆæ˜¯LangChainï¼Ÿ\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Q: ä»€ä¹ˆæ˜¯LangChainï¼Ÿ\n",
            "A: æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ï¼ŒLangChainæ˜¯ä¸€ä¸ªç”¨äºå¼€å‘è¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶ã€‚\n",
            "\n",
            "æ¥æº: ['LangChainæ˜¯ä¸€ä¸ªç”¨äºå¼€å‘è¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶ã€‚', 'RAGæ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„æŠ€æœ¯ã€‚', 'Chromaæ˜¯ä¸€ä¸ªè½»é‡çº§çš„å‘é‡æ•°æ®åº“ã€‚']\n",
            "\n",
            "==================================================\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: ä»€ä¹ˆæ˜¯LangChainï¼Ÿ\n",
            "Assistant: æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ï¼ŒLangChainæ˜¯ä¸€ä¸ªç”¨äºå¼€å‘è¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶ã€‚\n",
            "Follow Up Input: å®ƒå’Œå‘é‡æ•°æ®åº“æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ\n",
            "Standalone question:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question.\n",
            "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "----------------\n",
            "å‘é‡æ•°æ®åº“ç”¨äºå­˜å‚¨å’Œæ£€ç´¢å‘é‡è¡¨ç¤ºçš„æ–‡æ¡£ã€‚\n",
            "\n",
            "LangChainæ˜¯ä¸€ä¸ªç”¨äºå¼€å‘è¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶ã€‚\n",
            "\n",
            "Chromaæ˜¯ä¸€ä¸ªè½»é‡çº§çš„å‘é‡æ•°æ®åº“ã€‚\n",
            "Human: LangChainå’Œå‘é‡æ•°æ®åº“æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Q: å®ƒå’Œå‘é‡æ•°æ®åº“æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ\n",
            "A: æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ï¼ŒLangChainå’Œå‘é‡æ•°æ®åº“çš„å…³ç³»å¯ä»¥æ¦‚æ‹¬ä¸ºï¼š\n",
            "\n",
            "**LangChainæ˜¯ä¸€ä¸ªåº”ç”¨å¼€å‘æ¡†æ¶ï¼Œè€Œå‘é‡æ•°æ®åº“æ˜¯å®ƒå¯ä»¥é›†æˆå’Œåˆ©ç”¨çš„ä¸€ç§æ•°æ®å­˜å‚¨ä¸æ£€ç´¢ç»„ä»¶ã€‚**\n",
            "\n",
            "å…·ä½“æ¥è¯´ï¼š\n",
            "\n",
            "1.  **åŠŸèƒ½å®šä½ä¸åŒ**ï¼š\n",
            "    *   **LangChain**ï¼šæ˜¯ä¸€ä¸ªç”¨äºæ„å»ºç”±è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åº”ç”¨ç¨‹åºçš„**æ¡†æ¶**ã€‚å®ƒæä¾›äº†å„ç§å·¥å…·å’Œç»„ä»¶ï¼Œç”¨äºè¿æ¥è¯­è¨€æ¨¡å‹ã€æ•°æ®æºã€è®°å¿†ç³»ç»Ÿç­‰ï¼Œå¹¶ç¼–æ’åº”ç”¨ç¨‹åºçš„å·¥ä½œæµç¨‹ã€‚\n",
            "    *   **å‘é‡æ•°æ®åº“**ï¼šæ˜¯ä¸€ç§ä¸“é—¨ç”¨äº**å­˜å‚¨å’Œæ£€ç´¢**é«˜ç»´å‘é‡æ•°æ®ï¼ˆä¾‹å¦‚æ–‡æ¡£çš„å‘é‡è¡¨ç¤ºï¼‰çš„æ•°æ®åº“ã€‚\n",
            "\n",
            "2.  **ååŒå·¥ä½œå…³ç³»**ï¼š\n",
            "    *   åœ¨LangChainæ„å»ºçš„åº”ç”¨ç¨‹åºä¸­ï¼Œä¸€ä¸ªéå¸¸å¸¸è§çš„éœ€æ±‚æ˜¯è®©è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè®¿é—®å’Œå¤„ç†å¤–éƒ¨æ•°æ®ï¼ˆå¦‚æ‚¨è‡ªå·±çš„æ–‡æ¡£ï¼‰ã€‚\n",
            "    *   ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼ŒLangChainéœ€è¦å°†æ–‡æ¡£è½¬æ¢ä¸ºå‘é‡ï¼ˆè¿™ä¸ªè¿‡ç¨‹ç§°ä¸ºâ€œåµŒå…¥â€ï¼‰ï¼Œç„¶åå­˜å‚¨èµ·æ¥ä»¥ä¾¿å¿«é€Ÿæ£€ç´¢ã€‚\n",
            "    *   **å‘é‡æ•°æ®åº“ï¼ˆå¦‚æ‚¨æåˆ°çš„Chromaï¼‰æ­£æ˜¯æ‰®æ¼”äº†è¿™ä¸ªâ€œå­˜å‚¨å’Œæ£€ç´¢â€çš„è§’è‰²**ã€‚LangChainå†…ç½®äº†å¯¹å¤šç§å‘é‡æ•°æ®åº“ï¼ˆåŒ…æ‹¬Chromaï¼‰çš„æ”¯æŒï¼Œå¯ä»¥æ–¹ä¾¿åœ°å°†å®ƒä»¬é›†æˆåˆ°åº”ç”¨é“¾ä¸­ã€‚\n",
            "\n",
            "**ä¸€ä¸ªå…¸å‹çš„å·¥ä½œæµç¨‹æ˜¯**ï¼š\n",
            "*   ä½¿ç”¨LangChainçš„æ–‡æœ¬åˆ†å‰²å™¨å°†æ–‡æ¡£åˆ‡å—ã€‚\n",
            "*   ä½¿ç”¨LangChainçš„åµŒå…¥æ¨¡å‹å°†æ–‡æœ¬å—è½¬æ¢ä¸ºå‘é‡ã€‚\n",
            "*   ä½¿ç”¨LangChainçš„å‘é‡å­˜å‚¨æ¥å£ï¼Œå°†è¿™äº›å‘é‡åŠå…¶å¯¹åº”çš„åŸå§‹æ–‡æœ¬å­˜å‚¨åˆ°**å‘é‡æ•°æ®åº“ï¼ˆå¦‚Chromaï¼‰** ä¸­ã€‚\n",
            "*   å½“ç”¨æˆ·æé—®æ—¶ï¼ŒLangChainå°†é—®é¢˜ä¹Ÿè½¬æ¢ä¸ºå‘é‡ï¼Œå¹¶æŸ¥è¯¢**å‘é‡æ•°æ®åº“**ï¼Œæ‰¾åˆ°æœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µã€‚\n",
            "*   æœ€åï¼ŒLangChainå°†è¿™äº›ç›¸å…³ç‰‡æ®µå’Œé—®é¢˜ä¸€èµ·ç»„åˆæˆæç¤ºï¼Œå‘é€ç»™è¯­è¨€æ¨¡å‹æ¥ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚\n",
            "\n",
            "**æ€»ç»“**ï¼šæ‚¨å¯ä»¥å°†å‘é‡æ•°æ®åº“è§†ä¸ºLangChainç”Ÿæ€ç³»ç»Ÿä¸­çš„ä¸€ä¸ªå…³é”®**åŸºç¡€è®¾æ–½**æˆ–**æ’ä»¶**ï¼Œä¸“é—¨è´Ÿè´£é«˜æ•ˆå¤„ç†å‘é‡åŒ–åçš„æ•°æ®ï¼Œä»è€Œèµ‹èƒ½LangChainåº”ç”¨ç¨‹åºå®ç°å¼ºå¤§çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç­‰åŠŸèƒ½ã€‚\n"
          ]
        }
      ],
      "source": [
        "class RAGChatBot:\n",
        "    def __init__(self, llm, vectorstore):\n",
        "        self.llm = llm\n",
        "        self.vectorstore = vectorstore\n",
        "        self.memory = ConversationBufferMemory(\n",
        "            memory_key=\"chat_history\",\n",
        "            return_messages=True,\n",
        "            output_key=\"answer\"\n",
        "        )\n",
        "        \n",
        "        self.retriever = self.vectorstore.as_retriever(\n",
        "            search_kwargs={\"k\": 3}\n",
        "        )\n",
        "        \n",
        "        self.chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm=self.llm,\n",
        "            retriever=self.retriever,\n",
        "            memory=self.memory,\n",
        "            return_source_documents=True,\n",
        "            verbose=True\n",
        "        )\n",
        "    \n",
        "    def chat(self, question: str) -> Dict[str, Any]:\n",
        "        result = self.chain.invoke({\"question\": question})\n",
        "        return {\n",
        "            \"answer\": result[\"answer\"],\n",
        "            \"sources\": [doc.page_content for doc in result[\"source_documents\"]]\n",
        "        }\n",
        "    \n",
        "    def get_history(self):\n",
        "        return self.memory.load_memory_variables({})\n",
        "\n",
        "chatbot = RAGChatBot(llm, vectorstore)\n",
        "\n",
        "response1 = chatbot.chat(\"ä»€ä¹ˆæ˜¯LangChainï¼Ÿ\")\n",
        "print(\"Q: ä»€ä¹ˆæ˜¯LangChainï¼Ÿ\")\n",
        "print(f\"A: {response1['answer']}\")\n",
        "print(f\"\\næ¥æº: {response1['sources']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "response2 = chatbot.chat(\"å®ƒå’Œå‘é‡æ•°æ®åº“æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ\")\n",
        "print(\"Q: å®ƒå’Œå‘é‡æ•°æ®åº“æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ\")\n",
        "print(f\"A: {response2['answer']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ä¸ƒã€LangServe - API æœåŠ¡åŒ–\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… LangServe æœåŠ¡ä»£ç å·²ä¿å­˜åˆ° scripts/langserve_app.py\n",
            "   (å·²é…ç½® DeepSeek æ¨¡å‹)\n",
            "\n",
            "è¿è¡Œæ–¹æ³•:\n",
            "cd scripts && python langserve_app.py\n",
            "\n",
            "è®¿é—®æ–‡æ¡£: http://localhost:8000/docs\n"
          ]
        }
      ],
      "source": [
        "server_code = '''\n",
        "import os\n",
        "from fastapi import FastAPI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langserve import add_routes\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your-deepseek-api-key-here\"\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"https://api.deepseek.com\"\n",
        "\n",
        "app = FastAPI(\n",
        "    title=\"LangChain Server\",\n",
        "    version=\"1.0\",\n",
        "    description=\"A simple API server using LangChain with DeepSeek\",\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"deepseek-chat\",\n",
        "    openai_api_base=\"https://api.deepseek.com\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"å‘Šè¯‰æˆ‘ä¸€ä¸ªå…³äº{topic}çš„æ•…äº‹\"\n",
        ")\n",
        "\n",
        "chain = prompt | llm\n",
        "\n",
        "add_routes(\n",
        "    app,\n",
        "    chain,\n",
        "    path=\"/story\",\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "    uvicorn.run(app, host=\"localhost\", port=8000)\n",
        "'''\n",
        "\n",
        "with open(\"./scripts/langserve_app.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(server_code)\n",
        "\n",
        "print(\"âœ… LangServe æœåŠ¡ä»£ç å·²ä¿å­˜åˆ° scripts/langserve_app.py\")\n",
        "print(\"   (å·²é…ç½® DeepSeek æ¨¡å‹)\")\n",
        "print(\"\\nè¿è¡Œæ–¹æ³•:\")\n",
        "print(\"cd scripts && python langserve_app.py\")\n",
        "print(\"\\nè®¿é—®æ–‡æ¡£: http://localhost:8000/docs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## å…«ã€Streamlit Web ç•Œé¢\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Streamlit åº”ç”¨å·²ä¿å­˜åˆ° scripts/streamlit_app.py\n",
            "   (å·²é…ç½® DeepSeek æ¨¡å‹)\n",
            "\n",
            "è¿è¡Œæ–¹æ³•:\n",
            "streamlit run scripts/streamlit_app.py\n"
          ]
        }
      ],
      "source": [
        "streamlit_app = '''\n",
        "import os\n",
        "import streamlit as st\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your-deepseek-api-key-here\"\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"https://api.deepseek.com\"\n",
        "\n",
        "st.title(\"ğŸ¤– RAG é—®ç­”æœºå™¨äºº (DeepSeek)\")\n",
        "\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "@st.cache_resource\n",
        "def load_chain():\n",
        "    llm = ChatOpenAI(\n",
        "        temperature=0,\n",
        "        model=\"deepseek-chat\",\n",
        "        openai_api_base=\"https://api.deepseek.com\"\n",
        "    )\n",
        "    embeddings = OpenAIEmbeddings(\n",
        "        openai_api_base=\"https://api.deepseek.com\"\n",
        "    )\n",
        "    vectorstore = Chroma(\n",
        "        persist_directory=\"./chroma_db\",\n",
        "        embedding_function=embeddings\n",
        "    )\n",
        "    \n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key=\"chat_history\",\n",
        "        return_messages=True,\n",
        "        output_key=\"answer\"\n",
        "    )\n",
        "    \n",
        "    chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vectorstore.as_retriever(),\n",
        "        memory=memory,\n",
        "        return_source_documents=True\n",
        "    )\n",
        "    \n",
        "    return chain\n",
        "\n",
        "chain = load_chain()\n",
        "\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "if prompt := st.chat_input(\"è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "    \n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"æ€è€ƒä¸­...\"):\n",
        "            response = chain.invoke({\"question\": prompt})\n",
        "            answer = response[\"answer\"]\n",
        "            st.markdown(answer)\n",
        "            \n",
        "            with st.expander(\"æŸ¥çœ‹æ¥æºæ–‡æ¡£\"):\n",
        "                for i, doc in enumerate(response[\"source_documents\"]):\n",
        "                    st.markdown(f\"**æ¥æº {i+1}:** {doc.page_content}\")\n",
        "    \n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer})\n",
        "\n",
        "if st.sidebar.button(\"æ¸…é™¤å¯¹è¯å†å²\"):\n",
        "    st.session_state.messages = []\n",
        "    st.rerun()\n",
        "'''\n",
        "\n",
        "with open(\"./scripts/streamlit_app.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(streamlit_app)\n",
        "\n",
        "print(\"âœ… Streamlit åº”ç”¨å·²ä¿å­˜åˆ° scripts/streamlit_app.py\")\n",
        "print(\"   (å·²é…ç½® DeepSeek æ¨¡å‹)\")\n",
        "print(\"\\nè¿è¡Œæ–¹æ³•:\")\n",
        "print(\"streamlit run scripts/streamlit_app.py\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ä¹ã€æ€»ç»“\n",
        "\n",
        "æœ¬ Notebook å…¨é¢å±•ç¤ºäº† LangChain æ¡†æ¶çš„æ ¸å¿ƒå¼€å‘æŠ€èƒ½ï¼š\n",
        "\n",
        "### âœ… å·²æŒæ¡çš„æŠ€èƒ½\n",
        "\n",
        "1. **Prompt Engineering æŠ€æœ¯**\n",
        "   - Zero-shotã€Few-shotã€COTã€ReActã€Prompt Chaining\n",
        "\n",
        "2. **æ ¸å¿ƒç»„ä»¶ä½¿ç”¨**\n",
        "   - LLMã€Chat Modelsã€PromptTemplatesã€Output Parsersã€Chains (LCEL)\n",
        "\n",
        "3. **è°ƒè¯•ç›‘æ§**\n",
        "   - Verbose æ—¥å¿—ã€Debug æ¨¡å¼ã€è‡ªå®šä¹‰å›è°ƒå¤„ç†å™¨\n",
        "\n",
        "4. **å¯¹è¯å†å²ç®¡ç†**\n",
        "   - å†…å­˜ç®¡ç†ã€æ–‡ä»¶æŒä¹…åŒ–ã€å¸¦è®°å¿†çš„å¯¹è¯é“¾\n",
        "\n",
        "5. **Tools å’Œ Agents**\n",
        "   - è‡ªå®šä¹‰å·¥å…·ã€åŒæ­¥/å¼‚æ­¥è°ƒç”¨ã€Agent æ‰§è¡Œå™¨\n",
        "   - å¤©æ°”æŸ¥è¯¢ã€è®¡ç®—å™¨ã€ç½‘é¡µæœç´¢ç­‰å®ç”¨å·¥å…·\n",
        "\n",
        "6. **å‘é‡æ•°æ®åº“å’Œ RAG**\n",
        "   - Chroma å‘é‡å­˜å‚¨ã€ç›¸ä¼¼åº¦æœç´¢ã€RAG æ£€ç´¢é“¾\n",
        "   - å¸¦è®°å¿†çš„ RAG ç³»ç»Ÿ\n",
        "\n",
        "7. **æ–‡æ¡£å¤„ç†**\n",
        "   - æ–‡æ¡£åŠ è½½å™¨ã€æ–‡æ¡£åˆ†å‰²å™¨ã€æ–‡æœ¬é¢„å¤„ç†\n",
        "\n",
        "8. **æœåŠ¡éƒ¨ç½²**\n",
        "   - LangServe API æœåŠ¡ã€Streamlit Web ç•Œé¢\n",
        "\n",
        "### ğŸš€ æ‰©å±•æ–¹å‘\n",
        "\n",
        "- å¤šæ¨¡æ€æ”¯æŒï¼ˆå›¾åƒã€éŸ³é¢‘ï¼‰\n",
        "- æµå¼è¾“å‡ºä¼˜åŒ–\n",
        "- é«˜çº§ RAG æŠ€æœ¯ï¼ˆé‡æ’åºã€æ··åˆæ£€ç´¢ï¼‰\n",
        "- LangSmith æ·±åº¦é›†æˆ\n",
        "- ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–\n",
        "\n",
        "### ğŸ’¡ æœ€ä½³å®è·µ\n",
        "\n",
        "- ä½¿ç”¨ LCEL æ„å»ºå¯ç»„åˆçš„é“¾\n",
        "- åˆç†ä½¿ç”¨ Verbose å’Œ Debug è¿›è¡Œè°ƒè¯•\n",
        "- ä¸ºç”Ÿäº§ç¯å¢ƒé…ç½®é€‚å½“çš„å†…å­˜ç®¡ç†ç­–ç•¥\n",
        "- ä½¿ç”¨å‘é‡æ•°æ®åº“ä¼˜åŒ–æ£€ç´¢æ€§èƒ½\n",
        "- å®ç°é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶\n",
        "- ç›‘æ§ Token ä½¿ç”¨å’Œæˆæœ¬\n",
        "\n",
        "---\n",
        "\n",
        "**ğŸ‰ æ­å–œï¼ä½ å·²ç»æŒæ¡äº† LangChain çš„æ ¸å¿ƒå¼€å‘æŠ€èƒ½ï¼**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
