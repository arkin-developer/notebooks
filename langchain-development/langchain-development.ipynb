{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🦜 LangChain 全栈开发实战\n",
    "\n",
    "本 Notebook 展示 LangChain 框架的核心技能，包括：\n",
    "- Prompt Engineering 技术\n",
    "- 核心组件使用\n",
    "- 调试监控技术\n",
    "- 自定义 Tool 和 Agent\n",
    "- 向量数据库和 RAG\n",
    "- 对话历史管理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 环境准备\n",
    "\n",
    "### 安装说明\n",
    "\n",
    "下面的安装脚本会：\n",
    "1. 🔧 清理可能冲突的旧版本包\n",
    "2. 📦 固定 `protobuf` 版本为 4.25.8（避免冲突）\n",
    "3. 📦 按顺序安装所有依赖\n",
    "4. ✅ 确保没有版本冲突\n",
    "\n",
    "**运行后请重启 Kernel！**\n",
    "\n",
    "> 💡 **提示**：这个脚本会卸载并重装部分包，可能需要几分钟。如果你已经安装过相关包，建议先创建虚拟环境。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 步骤 1: 卸载冲突包...\n",
      "  ✅ 清理完成\n",
      "\n",
      "📦 步骤 2: 固定 protobuf 版本...\n",
      "Collecting protobuf==4.25.8\n",
      "  Using cached protobuf-4.25.8-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Using cached protobuf-4.25.8-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "Installing collected packages: protobuf\n",
      "Successfully installed protobuf-4.25.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ protobuf 4.25.8\n",
      "\n",
      "📦 步骤 3: 安装 LangChain...\n",
      "  ✅ LangChain 核心包\n",
      "\n",
      "📦 步骤 4: 安装 ChromaDB (保持 protobuf 版本)...\n",
      "  安装 ChromaDB 依赖...\n",
      "  ✅ ChromaDB\n",
      "\n",
      "📦 步骤 5: 安装 API 和工具...\n",
      "  ✅ 服务和工具\n",
      "\n",
      "📦 步骤 6: 安装本地 Embedding 模型...\n",
      "  ✅ Sentence Transformers (用于免费的本地 embeddings)\n",
      "\n",
      "============================================================\n",
      "✅ 安装完成！\n",
      "============================================================\n",
      "\n",
      "💡 下一步: 重启 Kernel (Kernel -> Restart Kernel)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_dependencies():\n",
    "    \"\"\"清理并重新安装所有依赖，解决版本冲突\"\"\"\n",
    "    \n",
    "    print(\"🔧 步骤 1: 卸载冲突包...\")\n",
    "    conflict_packages = [\"protobuf\", \"chromadb\"]\n",
    "    for pkg in conflict_packages:\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"-q\", pkg], \n",
    "                      capture_output=True, check=False)\n",
    "    print(\"  ✅ 清理完成\\n\")\n",
    "    \n",
    "    print(\"📦 步骤 2: 固定 protobuf 版本...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"protobuf==4.25.8\"])\n",
    "    print(\"  ✅ protobuf 4.25.8\\n\")\n",
    "    \n",
    "    print(\"📦 步骤 3: 安装 LangChain...\")\n",
    "    result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \n",
    "                            \"langchain\", \"langchain-openai\", \"langchain-community\", \n",
    "                            \"tiktoken\", \"-q\"])\n",
    "    print(\"  ✅ LangChain 核心包\\n\")\n",
    "    \n",
    "    print(\"📦 步骤 4: 安装 ChromaDB (保持 protobuf 版本)...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \n",
    "                   \"chromadb\", \"--no-deps\", \"-q\"])\n",
    "    print(\"  安装 ChromaDB 依赖...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \n",
    "                   \"onnxruntime\", \"tokenizers\", \"pypika\", \"tqdm\", \n",
    "                   \"overrides\", \"importlib-resources\", \"grpcio>=1.58.0\",\n",
    "                   \"bcrypt>=4.0.1\", \"typer>=0.9.0\", \"kubernetes>=28.1.0\",\n",
    "                   \"tenacity>=8.2.3\", \"PyYAML>=6.0.0\", \"posthog>=2.4.0\", \"-q\"],\n",
    "                  capture_output=True)\n",
    "    print(\"  ✅ ChromaDB\\n\")\n",
    "    \n",
    "    print(\"📦 步骤 5: 安装 API 和工具...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\",\n",
    "                   \"fastapi\", \"uvicorn\", \"langserve\", \n",
    "                   \"requests\", \"beautifulsoup4\", \"pydantic\", \"-q\"])\n",
    "    print(\"  ✅ 服务和工具\\n\")\n",
    "    \n",
    "    print(\"📦 步骤 6: 安装本地 Embedding 模型...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\",\n",
    "                   \"sentence-transformers\", \"-q\"])\n",
    "    print(\"  ✅ Sentence Transformers (用于免费的本地 embeddings)\\n\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"✅ 安装完成！\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\n💡 下一步: 重启 Kernel (Kernel -> Restart Kernel)\\n\")\n",
    "\n",
    "install_dependencies()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 配置 DeepSeek API\n",
      "============================================================\n",
      "本 Notebook 使用 DeepSeek 模型（兼容 OpenAI API）\n",
      "\n",
      "📝 获取 API Key:\n",
      "1. 访问 https://platform.deepseek.com/\n",
      "2. 注册/登录账号\n",
      "3. 创建 API Key\n",
      "\n",
      "💰 费用说明:\n",
      "DeepSeek 比 OpenAI 便宜很多，适合学习和开发\n",
      "============================================================\n",
      "\n",
      "✅ API Key 已配置\n",
      "✅ API Base: https://api.deepseek.com\n",
      "✅ 使用模型: deepseek-chat\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "import json\n",
    "\n",
    "print(\"🔧 配置 DeepSeek API\")\n",
    "print(\"=\"*60)\n",
    "print(\"本 Notebook 使用 DeepSeek 模型（兼容 OpenAI API）\")\n",
    "print(\"\\n📝 获取 API Key:\")\n",
    "print(\"1. 访问 https://platform.deepseek.com/\")\n",
    "print(\"2. 注册/登录账号\")\n",
    "print(\"3. 创建 API Key\")\n",
    "print(\"\\n💰 费用说明:\")\n",
    "print(\"DeepSeek 比 OpenAI 便宜很多，适合学习和开发\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 方式1：在终端中设置环境变量（推荐）\n",
    "# export OPENAI_API_KEY=\"your-deepseek-api-key-here\"\n",
    "# export OPENAI_API_BASE=\"https://api.deepseek.com\"\n",
    "\n",
    "# 方式2：临时在 Notebook 中设置（仅本次运行有效，不会影响系统环境变量）\n",
    "# 如果你没有在终端设置，可以取消下面两行的注释并填入你的 API Key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-deepseek-api-key-here\"\n",
    "# os.environ[\"OPENAI_API_BASE\"] = \"https://api.deepseek.com\"\n",
    "\n",
    "# 检查配置\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
    "api_base = os.environ.get(\"OPENAI_API_BASE\", \"https://api.deepseek.com\")\n",
    "\n",
    "if not api_key or api_key == \"your-deepseek-api-key-here\":\n",
    "    print(\"\\n⚠️  警告: 未检测到有效的 API Key！\")\n",
    "    print(\"\\n请选择以下方式之一配置：\")\n",
    "    print(\"1. 在终端运行：\")\n",
    "    print(\"   export OPENAI_API_KEY='your-actual-api-key'\")\n",
    "    print(\"   export OPENAI_API_BASE='https://api.deepseek.com'\")\n",
    "    print(\"\\n2. 或在上方取消注释并填入你的 API Key\")\n",
    "else:\n",
    "    # 确保 API Base 设置正确\n",
    "    if not api_base or api_base == \"https://api.openai.com\":\n",
    "        os.environ[\"OPENAI_API_BASE\"] = \"https://api.deepseek.com\"\n",
    "        print(f\"\\n✅ API Key 已配置\")\n",
    "        print(f\"✅ API Base 已设置为: https://api.deepseek.com\")\n",
    "    else:\n",
    "        print(f\"\\n✅ API Key 已配置\")\n",
    "        print(f\"✅ API Base: {api_base}\")\n",
    "    print(\"✅ 使用模型: deepseek-chat\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📖 DeepSeek 使用说明\n",
    "\n",
    "**为什么选择 DeepSeek？**\n",
    "- 💰 **价格优势**：比 OpenAI 便宜约 90%，性价比极高\n",
    "- 🇨🇳 **中文友好**：对中文支持优秀，理解能力强\n",
    "- 🔌 **API 兼容**：完全兼容 OpenAI API 格式，无需修改代码\n",
    "- 🚀 **性能优秀**：deepseek-chat 模型性能接近 GPT-3.5\n",
    "\n",
    "**DeepSeek API 配置方式：**\n",
    "```python\n",
    "# 方式1: 环境变量（推荐）\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-deepseek-api-key\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://api.deepseek.com\"\n",
    "\n",
    "# 方式2: 直接在模型中配置\n",
    "llm = ChatOpenAI(\n",
    "    model=\"deepseek-chat\",\n",
    "    openai_api_key=\"your-deepseek-api-key\",\n",
    "    openai_api_base=\"https://api.deepseek.com\"\n",
    ")\n",
    "```\n",
    "\n",
    "**⚠️ 重要说明：Embeddings 方案**\n",
    "\n",
    "DeepSeek **不支持 Embeddings API**，因此本 Notebook 使用以下方案：\n",
    "\n",
    "1. **聊天对话**：使用 DeepSeek（便宜快速）\n",
    "2. **向量化**：使用本地免费模型 `sentence-transformers`\n",
    "\n",
    "这是最佳组合：\n",
    "- ✅ 完全免费的 Embeddings\n",
    "- ✅ 低成本的 LLM 对话\n",
    "- ✅ 无需多个 API Key\n",
    "\n",
    "**注意事项：**\n",
    "1. ⚠️ 记得替换 DeepSeek API Key！\n",
    "2. 📦 首次运行会自动下载 Embedding 模型（约 500MB）\n",
    "3. 📊 查看用量：https://platform.deepseek.com/usage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、Prompt Engineering 技术\n",
    "\n",
    "展示各种提示工程技术的应用\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Zero-shot Prompting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot 结果: 正面情感\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0, \n",
    "    model=\"deepseek-chat\",\n",
    "    openai_api_base=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "zero_shot_prompt = ChatPromptTemplate.from_template(\n",
    "    \"将以下文本分类为正面、负面或中性情感：\\n\\n{text}\"\n",
    ")\n",
    "\n",
    "chain = zero_shot_prompt | llm\n",
    "\n",
    "result = chain.invoke({\"text\": \"这个产品真的很棒，我非常喜欢！\"})\n",
    "print(\"Zero-shot 结果:\", result.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Few-shot Prompting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-shot 结果: 正面\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"这个产品质量很好\", \"output\": \"正面\"},\n",
    "    {\"input\": \"服务态度太差了\", \"output\": \"负面\"},\n",
    "    {\"input\": \"价格还可以\", \"output\": \"中性\"},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\"),\n",
    "])\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是一个情感分类专家。\"),\n",
    "    few_shot_prompt,\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "chain = final_prompt | llm\n",
    "result = chain.invoke({\"input\": \"物流速度挺快的\"})\n",
    "print(\"Few-shot 结果:\", result.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Chain of Thought (CoT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoT 结果:\n",
      " 好的，我们按步骤来推理。  \n",
      "\n",
      "---\n",
      "\n",
      "**1. 理解问题关键信息**  \n",
      "- 一个苹果原价：3 元  \n",
      "- 买的数量：5 个  \n",
      "- 折扣：8 折（即原价的 80%）  \n",
      "\n",
      "---\n",
      "\n",
      "**2. 列出解决步骤**  \n",
      "① 先算原价总金额：  \n",
      "\\[\n",
      "5 \\times 3 = 15 \\text{ 元}\n",
      "\\]  \n",
      "\n",
      "② 再计算打折后的价格（8 折 = 乘以 0.8）：  \n",
      "\\[\n",
      "15 \\times 0.8 = 12 \\text{ 元}\n",
      "\\]  \n",
      "\n",
      "---\n",
      "\n",
      "**3. 得出答案**  \n",
      "\\[\n",
      "\\boxed{12}\n",
      "\\]  \n",
      "\n",
      "所以，买 5 个苹果打 8 折需要 **12 元**。\n"
     ]
    }
   ],
   "source": [
    "cot_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"问题：{question}\n",
    "\n",
    "让我们一步一步思考：\n",
    "1. 首先，理解问题的关键信息\n",
    "2. 然后，列出解决步骤\n",
    "3. 最后，得出答案\n",
    "\n",
    "请按照上述步骤回答。\"\"\"\n",
    ")\n",
    "\n",
    "chain = cot_prompt | llm\n",
    "result = chain.invoke({\"question\": \"如果一个苹果3元，买5个苹果打8折，需要多少钱？\"})\n",
    "print(\"CoT 结果:\\n\", result.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、LangChain 核心组件使用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "解析结果: name='张三' age=30 occupation='软件工程师'\n",
      "姓名: 张三, 年龄: 30, 职业: 软件工程师\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Person(BaseModel):\n",
    "    name: str = Field(description=\"人物姓名\")\n",
    "    age: int = Field(description=\"人物年龄\")\n",
    "    occupation: str = Field(description=\"职业\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Person)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"提取以下文本中的人物信息：\n",
    "{text}\n",
    "\n",
    "{format_instructions}\"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "result = chain.invoke({\n",
    "    \"text\": \"张三今年30岁，是一名软件工程师\",\n",
    "    \"format_instructions\": parser.get_format_instructions()\n",
    "})\n",
    "\n",
    "print(\"解析结果:\", result)\n",
    "print(f\"姓名: {result.name}, 年龄: {result.age}, 职业: {result.occupation}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、Chat History - 对话历史管理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对话 1:\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 我喜欢Python编程\n",
      "AI:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y_/g595hp191h75fwdk_s4cf6bm0000gn/T/ipykernel_99216/2492734910.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "/var/folders/y_/g595hp191h75fwdk_s4cf6bm0000gn/T/ipykernel_99216/2492734910.py:5: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "太棒了！Python是一门非常强大且多用途的编程语言。它的语法简洁清晰，非常适合初学者入门，同时也被广泛应用于数据科学、机器学习、Web开发、自动化脚本等领域。你喜欢用Python做什么呢？是开发网站、进行数据分析，还是写一些小工具？如果你有任何关于Python的问题，比如如何使用某些库（例如Pandas、NumPy、Django或Flask），或者想了解最佳实践，我都很乐意帮助你！\n",
      "\n",
      "==================================================\n",
      "\n",
      "对话 2:\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 我喜欢Python编程\n",
      "AI: 太棒了！Python是一门非常强大且多用途的编程语言。它的语法简洁清晰，非常适合初学者入门，同时也被广泛应用于数据科学、机器学习、Web开发、自动化脚本等领域。你喜欢用Python做什么呢？是开发网站、进行数据分析，还是写一些小工具？如果你有任何关于Python的问题，比如如何使用某些库（例如Pandas、NumPy、Django或Flask），或者想了解最佳实践，我都很乐意帮助你！\n",
      "Human: 我刚才说我喜欢什么？\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "您刚才说的是：“我喜欢Python编程”。您对Python的热情很棒呢！需要我为您推荐一些有趣的Python项目方向，或是探讨某个特定库的使用技巧吗？\n",
      "\n",
      "对话历史已持久化到文件\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory, FileChatMessageHistory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"对话 1:\")\n",
    "print(conversation.predict(input=\"我喜欢Python编程\"))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"对话 2:\")\n",
    "print(conversation.predict(input=\"我刚才说我喜欢什么？\"))\n",
    "\n",
    "file_history = FileChatMessageHistory(\"chat_history.json\")\n",
    "file_history.add_user_message(\"什么是LangChain？\")\n",
    "file_history.add_ai_message(\"LangChain是一个用于开发LLM应用的框架。\")\n",
    "print(\"\\n对话历史已持久化到文件\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、自定义 Tools 和 Agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "工具测试:\n",
      "时间: 2025-10-12 22:00:42\n",
      "计算: 30\n",
      "搜索: 搜索结果：关于 'LangChain教程' 的信息...(这是模拟结果)\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "import requests\n",
    "\n",
    "@tool\n",
    "def get_current_time(format: str = \"%Y-%m-%d %H:%M:%S\") -> str:\n",
    "    \"\"\"获取当前时间\"\"\"\n",
    "    return datetime.now().strftime(format)\n",
    "\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"计算数学表达式\"\"\"\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"计算错误: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def search_web(query: str) -> str:\n",
    "    \"\"\"模拟网页搜索\"\"\"\n",
    "    return f\"搜索结果：关于 '{query}' 的信息...(这是模拟结果)\"\n",
    "\n",
    "@tool\n",
    "async def async_search(query: str) -> str:\n",
    "    \"\"\"异步搜索工具\"\"\"\n",
    "    import asyncio\n",
    "    await asyncio.sleep(0.5)\n",
    "    return f\"异步搜索结果: {query}\"\n",
    "\n",
    "print(\"工具测试:\")\n",
    "print(\"时间:\", get_current_time.invoke({}))\n",
    "print(\"计算:\", calculator.invoke({\"expression\": \"(10 + 5) * 2\"}))\n",
    "print(\"搜索:\", search_web.invoke({\"query\": \"LangChain教程\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_current_time` with `{}`\n",
      "responded: 我来帮您获取当前时间并计算数学表达式。\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m2025-10-12 22:00:44\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `calculator` with `{'expression': '25 * 4'}`\n",
      "responded: 我来帮您获取当前时间并计算数学表达式。\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m100\u001b[0m\u001b[32;1m\u001b[1;3m根据查询结果：\n",
      "\n",
      "- **当前时间**：2025年10月12日 22:00:44\n",
      "- **25 × 4 的计算结果**：100\n",
      "\n",
      "所以，现在是2025年10月12日晚上10点整，25乘以4等于100。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Agent 结果: 根据查询结果：\n",
      "\n",
      "- **当前时间**：2025年10月12日 22:00:44\n",
      "- **25 × 4 的计算结果**：100\n",
      "\n",
      "所以，现在是2025年10月12日晚上10点整，25乘以4等于100。\n"
     ]
    }
   ],
   "source": [
    "tools = [get_current_time, calculator, search_web]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是一个有用的AI助手。使用提供的工具来回答问题。\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "agent = create_openai_tools_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "result = agent_executor.invoke({\n",
    "    \"input\": \"现在几点？然后帮我计算 25 * 4 等于多少\"\n",
    "})\n",
    "print(\"\\nAgent 结果:\", result[\"output\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、向量数据库和 RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 初始化本地 Embedding 模型...\n",
      "💡 提示：DeepSeek 不支持 Embeddings API，使用免费的本地模型\n",
      "⏳ 首次运行会下载模型，请稍等...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y_/g595hp191h75fwdk_s4cf6bm0000gn/T/ipykernel_99216/2417001478.py:10: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embedding 模型加载完成！\n",
      "\n",
      "🔄 创建向量存储...\n",
      "✅ 创建了包含 5 个文档块的向量存储\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "print(\"📦 初始化本地 Embedding 模型...\")\n",
    "print(\"💡 提示：DeepSeek 不支持 Embeddings API，使用免费的本地模型\")\n",
    "print(\"⏳ 首次运行会下载模型，请稍等...\")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "print(\"✅ Embedding 模型加载完成！\\n\")\n",
    "\n",
    "documents = [\n",
    "    Document(page_content=\"LangChain是一个用于开发语言模型应用的框架。\", metadata={\"source\": \"doc1\"}),\n",
    "    Document(page_content=\"向量数据库用于存储和检索向量表示的文档。\", metadata={\"source\": \"doc2\"}),\n",
    "    Document(page_content=\"RAG是检索增强生成，结合检索和生成的技术。\", metadata={\"source\": \"doc3\"}),\n",
    "    Document(page_content=\"Embeddings将文本转换为数值向量表示。\", metadata={\"source\": \"doc4\"}),\n",
    "    Document(page_content=\"Chroma是一个轻量级的向量数据库。\", metadata={\"source\": \"doc5\"}),\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "print(\"🔄 创建向量存储...\")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "print(f\"✅ 创建了包含 {len(splits)} 个文档块的向量存储\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 相似度搜索\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查询: 什么是RAG？\n",
      "\n",
      "相似度搜索结果:\n",
      "\n",
      "结果 1:\n",
      "内容: RAG是检索增强生成，结合检索和生成的技术。\n",
      "元数据: {'source': 'doc3'}\n",
      "\n",
      "结果 2:\n",
      "内容: RAG是检索增强生成，结合检索和生成的技术。\n",
      "元数据: {'source': 'doc3'}\n",
      "\n",
      "带相似度分数的结果:\n",
      "分数: 1.1480 - RAG是检索增强生成，结合检索和生成的技术。\n",
      "分数: 1.1480 - RAG是检索增强生成，结合检索和生成的技术。\n"
     ]
    }
   ],
   "source": [
    "query = \"什么是RAG？\"\n",
    "results = vectorstore.similarity_search(query, k=2)\n",
    "\n",
    "print(f\"查询: {query}\")\n",
    "print(\"\\n相似度搜索结果:\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n结果 {i+1}:\")\n",
    "    print(f\"内容: {doc.page_content}\")\n",
    "    print(f\"元数据: {doc.metadata}\")\n",
    "\n",
    "results_with_scores = vectorstore.similarity_search_with_score(query, k=2)\n",
    "print(\"\\n带相似度分数的结果:\")\n",
    "for doc, score in results_with_scores:\n",
    "    print(f\"分数: {score:.4f} - {doc.page_content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 RAG 检索链\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "问题: LangChain是什么？\n",
      "\n",
      "答案: 根据上下文，LangChain是一个用于开发语言模型应用的框架。\n",
      "\n",
      "来源文档:\n",
      "- LangChain是一个用于开发语言模型应用的框架。\n",
      "- LangChain是一个用于开发语言模型应用的框架。\n",
      "- RAG是检索增强生成，结合检索和生成的技术。\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "template = \"\"\"基于以下上下文回答问题：\n",
    "\n",
    "上下文: {context}\n",
    "\n",
    "问题: {question}\n",
    "\n",
    "答案:\"\"\"\n",
    "\n",
    "QA_PROMPT = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": QA_PROMPT},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "question = \"LangChain是什么？\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "\n",
    "print(f\"问题: {question}\")\n",
    "print(f\"\\n答案: {result['result']}\")\n",
    "print(\"\\n来源文档:\")\n",
    "for doc in result['source_documents']:\n",
    "    print(f\"- {doc.page_content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 带记忆的 RAG 系统\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: 什么是向量数据库？\n",
      "A1: 根据提供的上下文，向量数据库是一种用于存储和检索以向量形式表示的文档的数据库。它专门设计用于处理高维向量数据，通常用于相似性搜索、推荐系统或自然语言处理等应用场景。例如，Chroma 就是一个轻量级的向量数据库实现。\n",
      "\n",
      "Q2: 它的主要用途是什么？\n",
      "A2: 根据提供的上下文，向量数据库的主要用途是**存储和检索向量表示的文档**。\n",
      "\n",
      "这意味着它专门用于处理以向量（即一组数值）形式表示的数据，例如由机器学习模型（如大型语言模型）生成的文本、图像或音频的嵌入向量。其主要优势在于能够高效地根据向量的相似性来搜索和检索相关信息。\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    output_key=\"answer\"\n",
    ")\n",
    "\n",
    "conversational_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "question1 = \"什么是向量数据库？\"\n",
    "result1 = conversational_chain.invoke({\"question\": question1})\n",
    "print(f\"Q1: {question1}\")\n",
    "print(f\"A1: {result1['answer']}\\n\")\n",
    "\n",
    "question2 = \"它的主要用途是什么？\"\n",
    "result2 = conversational_chain.invoke({\"question\": question2})\n",
    "print(f\"Q2: {question2}\")\n",
    "print(f\"A2: {result2['answer']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 六、实战案例：完整的 RAG 问答机器人\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "LangChain是一个用于开发语言模型应用的框架。\n",
      "\n",
      "LangChain是一个用于开发语言模型应用的框架。\n",
      "\n",
      "RAG是检索增强生成，结合检索和生成的技术。\n",
      "Human: 什么是LangChain？\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Q: 什么是LangChain？\n",
      "A: 根据提供的上下文，LangChain是一个用于开发语言模型应用的框架。\n",
      "\n",
      "来源: ['LangChain是一个用于开发语言模型应用的框架。', 'LangChain是一个用于开发语言模型应用的框架。', 'RAG是检索增强生成，结合检索和生成的技术。']\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: 什么是LangChain？\n",
      "Assistant: 根据提供的上下文，LangChain是一个用于开发语言模型应用的框架。\n",
      "Follow Up Input: 它和向量数据库有什么关系？\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "向量数据库用于存储和检索向量表示的文档。\n",
      "\n",
      "向量数据库用于存储和检索向量表示的文档。\n",
      "\n",
      "LangChain是一个用于开发语言模型应用的框架。\n",
      "Human: LangChain和向量数据库有什么关系？\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Q: 它和向量数据库有什么关系？\n",
      "A: 根据提供的上下文，LangChain和向量数据库的关系可以概括如下：\n",
      "\n",
      "- **LangChain** 是一个用于开发语言模型（如GPT）应用的框架，它帮助构建基于语言模型的工具或系统。\n",
      "- **向量数据库** 用于存储和检索以向量形式表示的文档（例如，文本嵌入），这些向量可以高效地用于相似性搜索。\n",
      "\n",
      "**关系**：在LangChain框架中，向量数据库常被集成作为组件，用于存储文档的向量表示，并支持检索相关文档。例如，LangChain可以利用向量数据库来增强语言模型的上下文感知能力，比如通过检索与用户查询相似的文档片段，以提供更准确的回答。\n",
      "\n",
      "简而言之，LangChain使用向量数据库作为其基础设施的一部分，以实现高效的文档存储和检索功能，从而提升语言模型应用的性能。\n"
     ]
    }
   ],
   "source": [
    "class RAGChatBot:\n",
    "    def __init__(self, llm, vectorstore):\n",
    "        self.llm = llm\n",
    "        self.vectorstore = vectorstore\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True,\n",
    "            output_key=\"answer\"\n",
    "        )\n",
    "        \n",
    "        self.retriever = self.vectorstore.as_retriever(\n",
    "            search_kwargs={\"k\": 3}\n",
    "        )\n",
    "        \n",
    "        self.chain = ConversationalRetrievalChain.from_llm(\n",
    "            llm=self.llm,\n",
    "            retriever=self.retriever,\n",
    "            memory=self.memory,\n",
    "            return_source_documents=True,\n",
    "            verbose=True\n",
    "        )\n",
    "    \n",
    "    def chat(self, question: str) -> Dict[str, Any]:\n",
    "        result = self.chain.invoke({\"question\": question})\n",
    "        return {\n",
    "            \"answer\": result[\"answer\"],\n",
    "            \"sources\": [doc.page_content for doc in result[\"source_documents\"]]\n",
    "        }\n",
    "    \n",
    "    def get_history(self):\n",
    "        return self.memory.load_memory_variables({})\n",
    "\n",
    "chatbot = RAGChatBot(llm, vectorstore)\n",
    "\n",
    "response1 = chatbot.chat(\"什么是LangChain？\")\n",
    "print(\"Q: 什么是LangChain？\")\n",
    "print(f\"A: {response1['answer']}\")\n",
    "print(f\"\\n来源: {response1['sources']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "response2 = chatbot.chat(\"它和向量数据库有什么关系？\")\n",
    "print(\"Q: 它和向量数据库有什么关系？\")\n",
    "print(f\"A: {response2['answer']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七、LangServe - API 服务化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LangServe 服务代码已保存到 scripts/langserve_app.py\n",
      "   (已配置 DeepSeek 模型)\n",
      "\n",
      "运行方法:\n",
      "cd scripts && python langserve_app.py\n",
      "\n",
      "访问文档: http://localhost:8000/docs\n"
     ]
    }
   ],
   "source": [
    "server_code = '''\n",
    "import os\n",
    "from fastapi import FastAPI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langserve import add_routes\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-deepseek-api-key-here\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://api.deepseek.com\"\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"LangChain Server\",\n",
    "    version=\"1.0\",\n",
    "    description=\"A simple API server using LangChain with DeepSeek\",\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"deepseek-chat\",\n",
    "    openai_api_base=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"告诉我一个关于{topic}的故事\"\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    chain,\n",
    "    path=\"/story\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"localhost\", port=8000)\n",
    "'''\n",
    "\n",
    "with open(\"./scripts/langserve_app.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(server_code)\n",
    "\n",
    "print(\"✅ LangServe 服务代码已保存到 scripts/langserve_app.py\")\n",
    "print(\"   (已配置 DeepSeek 模型)\")\n",
    "print(\"\\n运行方法:\")\n",
    "print(\"cd scripts && python langserve_app.py\")\n",
    "print(\"\\n访问文档: http://localhost:8000/docs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 八、Streamlit Web 界面\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Streamlit 应用已保存到 scripts/streamlit_app.py\n",
      "   (已配置 DeepSeek 模型)\n",
      "\n",
      "运行方法:\n",
      "streamlit run scripts/streamlit_app.py\n"
     ]
    }
   ],
   "source": [
    "streamlit_app = '''\n",
    "import os\n",
    "import streamlit as st\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-deepseek-api-key-here\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://api.deepseek.com\"\n",
    "\n",
    "st.title(\"🤖 RAG 问答机器人 (DeepSeek)\")\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "@st.cache_resource\n",
    "def load_chain():\n",
    "    llm = ChatOpenAI(\n",
    "        temperature=0,\n",
    "        model=\"deepseek-chat\",\n",
    "        openai_api_base=\"https://api.deepseek.com\"\n",
    "    )\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        openai_api_base=\"https://api.deepseek.com\"\n",
    "    )\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=\"./chroma_db\",\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "    \n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True,\n",
    "        output_key=\"answer\"\n",
    "    )\n",
    "    \n",
    "    chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        memory=memory,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    return chain\n",
    "\n",
    "chain = load_chain()\n",
    "\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "if prompt := st.chat_input(\"请输入您的问题...\"):\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "    \n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.spinner(\"思考中...\"):\n",
    "            response = chain.invoke({\"question\": prompt})\n",
    "            answer = response[\"answer\"]\n",
    "            st.markdown(answer)\n",
    "            \n",
    "            with st.expander(\"查看来源文档\"):\n",
    "                for i, doc in enumerate(response[\"source_documents\"]):\n",
    "                    st.markdown(f\"**来源 {i+1}:** {doc.page_content}\")\n",
    "    \n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "if st.sidebar.button(\"清除对话历史\"):\n",
    "    st.session_state.messages = []\n",
    "    st.rerun()\n",
    "'''\n",
    "\n",
    "with open(\"./scripts/streamlit_app.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(streamlit_app)\n",
    "\n",
    "print(\"✅ Streamlit 应用已保存到 scripts/streamlit_app.py\")\n",
    "print(\"   (已配置 DeepSeek 模型)\")\n",
    "print(\"\\n运行方法:\")\n",
    "print(\"streamlit run scripts/streamlit_app.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 九、总结\n",
    "\n",
    "本 Notebook 全面展示了 LangChain 框架的核心开发技能：\n",
    "\n",
    "### ✅ 已掌握的技能\n",
    "\n",
    "1. **Prompt Engineering 技术**\n",
    "   - Zero-shot、Few-shot、COT、ReAct、Prompt Chaining\n",
    "\n",
    "2. **核心组件使用**\n",
    "   - LLM、Chat Models、PromptTemplates、Output Parsers、Chains (LCEL)\n",
    "\n",
    "3. **调试监控**\n",
    "   - Verbose 日志、Debug 模式、自定义回调处理器\n",
    "\n",
    "4. **对话历史管理**\n",
    "   - 内存管理、文件持久化、带记忆的对话链\n",
    "\n",
    "5. **Tools 和 Agents**\n",
    "   - 自定义工具、同步/异步调用、Agent 执行器\n",
    "   - 天气查询、计算器、网页搜索等实用工具\n",
    "\n",
    "6. **向量数据库和 RAG**\n",
    "   - Chroma 向量存储、相似度搜索、RAG 检索链\n",
    "   - 带记忆的 RAG 系统\n",
    "\n",
    "7. **文档处理**\n",
    "   - 文档加载器、文档分割器、文本预处理\n",
    "\n",
    "8. **服务部署**\n",
    "   - LangServe API 服务、Streamlit Web 界面\n",
    "\n",
    "### 🚀 扩展方向\n",
    "\n",
    "- 多模态支持（图像、音频）\n",
    "- 流式输出优化\n",
    "- 高级 RAG 技术（重排序、混合检索）\n",
    "- LangSmith 深度集成\n",
    "- 生产环境优化\n",
    "\n",
    "### 💡 最佳实践\n",
    "\n",
    "- 使用 LCEL 构建可组合的链\n",
    "- 合理使用 Verbose 和 Debug 进行调试\n",
    "- 为生产环境配置适当的内存管理策略\n",
    "- 使用向量数据库优化检索性能\n",
    "- 实现错误处理和重试机制\n",
    "- 监控 Token 使用和成本\n",
    "\n",
    "---\n",
    "\n",
    "**🎉 恭喜！你已经掌握了 LangChain 的核心开发技能！**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
