{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🦜 LangChain 全栈开发实战\n",
        "\n",
        "本 Notebook 展示 LangChain 框架的核心技能，包括：\n",
        "- Prompt Engineering 技术\n",
        "- 核心组件使用\n",
        "- 调试监控技术\n",
        "- 自定义 Tool 和 Agent\n",
        "- 向量数据库和 RAG\n",
        "- 对话历史管理\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📦 环境准备\n",
        "\n",
        "### 安装说明\n",
        "\n",
        "下面的安装脚本会：\n",
        "1. 🔧 清理可能冲突的旧版本包\n",
        "2. 📦 固定 `protobuf` 版本为 4.25.8（避免冲突）\n",
        "3. 📦 按顺序安装所有依赖\n",
        "4. ✅ 确保没有版本冲突\n",
        "\n",
        "**运行后请重启 Kernel！**\n",
        "\n",
        "> 💡 **提示**：这个脚本会卸载并重装部分包，可能需要几分钟。如果你已经安装过相关包，建议先创建虚拟环境。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 步骤 1: 卸载冲突包...\n",
            "  ✅ 清理完成\n",
            "\n",
            "📦 步骤 2: 固定 protobuf 版本...\n",
            "Collecting protobuf==4.25.8\n",
            "  Using cached protobuf-4.25.8-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
            "Using cached protobuf-4.25.8-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
            "Installing collected packages: protobuf\n",
            "Successfully installed protobuf-4.25.8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✅ protobuf 4.25.8\n",
            "\n",
            "📦 步骤 3: 安装 LangChain...\n",
            "  ✅ LangChain 核心包\n",
            "\n",
            "📦 步骤 4: 安装 ChromaDB (保持 protobuf 版本)...\n",
            "  安装 ChromaDB 依赖...\n",
            "  ✅ ChromaDB\n",
            "\n",
            "📦 步骤 5: 安装 API 和工具...\n",
            "  ✅ 服务和工具\n",
            "\n",
            "============================================================\n",
            "✅ 安装完成！\n",
            "============================================================\n",
            "\n",
            "💡 下一步: 重启 Kernel (Kernel -> Restart Kernel)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_dependencies():\n",
        "    \"\"\"清理并重新安装所有依赖，解决版本冲突\"\"\"\n",
        "    \n",
        "    print(\"🔧 步骤 1: 卸载冲突包...\")\n",
        "    conflict_packages = [\"protobuf\", \"chromadb\"]\n",
        "    for pkg in conflict_packages:\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"-q\", pkg], \n",
        "                      capture_output=True, check=False)\n",
        "    print(\"  ✅ 清理完成\\n\")\n",
        "    \n",
        "    print(\"📦 步骤 2: 固定 protobuf 版本...\")\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"protobuf==4.25.8\"])\n",
        "    print(\"  ✅ protobuf 4.25.8\\n\")\n",
        "    \n",
        "    print(\"📦 步骤 3: 安装 LangChain...\")\n",
        "    result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \n",
        "                            \"langchain\", \"langchain-openai\", \"langchain-community\", \n",
        "                            \"tiktoken\", \"-q\"])\n",
        "    print(\"  ✅ LangChain 核心包\\n\")\n",
        "    \n",
        "    print(\"📦 步骤 4: 安装 ChromaDB (保持 protobuf 版本)...\")\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \n",
        "                   \"chromadb\", \"--no-deps\", \"-q\"])\n",
        "    print(\"  安装 ChromaDB 依赖...\")\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \n",
        "                   \"onnxruntime\", \"tokenizers\", \"pypika\", \"tqdm\", \n",
        "                   \"overrides\", \"importlib-resources\", \"grpcio>=1.58.0\",\n",
        "                   \"bcrypt>=4.0.1\", \"typer>=0.9.0\", \"kubernetes>=28.1.0\",\n",
        "                   \"tenacity>=8.2.3\", \"PyYAML>=6.0.0\", \"posthog>=2.4.0\", \"-q\"],\n",
        "                  capture_output=True)\n",
        "    print(\"  ✅ ChromaDB\\n\")\n",
        "    \n",
        "    print(\"📦 步骤 5: 安装 API 和工具...\")\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\",\n",
        "                   \"fastapi\", \"uvicorn\", \"langserve\", \n",
        "                   \"requests\", \"beautifulsoup4\", \"pydantic\", \"-q\"])\n",
        "    print(\"  ✅ 服务和工具\\n\")\n",
        "    \n",
        "    print(\"📦 步骤 6: 安装本地 Embedding 模型...\")\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\",\n",
        "                   \"sentence-transformers\", \"-q\"])\n",
        "    print(\"  ✅ Sentence Transformers (用于免费的本地 embeddings)\\n\")\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"✅ 安装完成！\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\n💡 下一步: 重启 Kernel (Kernel -> Restart Kernel)\\n\")\n",
        "\n",
        "install_dependencies()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 配置 DeepSeek API\n",
            "============================================================\n",
            "本 Notebook 使用 DeepSeek 模型（兼容 OpenAI API）\n",
            "\n",
            "📝 获取 API Key:\n",
            "1. 访问 https://platform.deepseek.com/\n",
            "2. 注册/登录账号\n",
            "3. 创建 API Key\n",
            "4. 将下方的 'your-deepseek-api-key-here' 替换为你的 API Key\n",
            "\n",
            "💰 费用说明:\n",
            "DeepSeek 比 OpenAI 便宜很多，适合学习和开发\n",
            "============================================================\n",
            "\n",
            "✅ 配置完成！使用模型: deepseek-chat\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any, Optional\n",
        "import json\n",
        "\n",
        "print(\"🔧 配置 DeepSeek API\")\n",
        "print(\"=\"*60)\n",
        "print(\"本 Notebook 使用 DeepSeek 模型（兼容 OpenAI API）\")\n",
        "print(\"\\n📝 获取 API Key:\")\n",
        "print(\"1. 访问 https://platform.deepseek.com/\")\n",
        "print(\"2. 注册/登录账号\")\n",
        "print(\"3. 创建 API Key\")\n",
        "print(\"4. 将下方的 'your-deepseek-api-key-here' 替换为你的 API Key\")\n",
        "print(\"\\n💰 费用说明:\")\n",
        "print(\"DeepSeek 比 OpenAI 便宜很多，适合学习和开发\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-22fd815ca17a48f08d788e5a5dd454a1\"\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"https://api.deepseek.com\"\n",
        "\n",
        "print(\"\\n✅ 配置完成！使用模型: deepseek-chat\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 📖 DeepSeek 使用说明\n",
        "\n",
        "**为什么选择 DeepSeek？**\n",
        "- 💰 **价格优势**：比 OpenAI 便宜约 90%，性价比极高\n",
        "- 🇨🇳 **中文友好**：对中文支持优秀，理解能力强\n",
        "- 🔌 **API 兼容**：完全兼容 OpenAI API 格式，无需修改代码\n",
        "- 🚀 **性能优秀**：deepseek-chat 模型性能接近 GPT-3.5\n",
        "\n",
        "**DeepSeek API 配置方式：**\n",
        "```python\n",
        "# 方式1: 环境变量（推荐）\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your-deepseek-api-key\"\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"https://api.deepseek.com\"\n",
        "\n",
        "# 方式2: 直接在模型中配置\n",
        "llm = ChatOpenAI(\n",
        "    model=\"deepseek-chat\",\n",
        "    openai_api_key=\"your-deepseek-api-key\",\n",
        "    openai_api_base=\"https://api.deepseek.com\"\n",
        ")\n",
        "```\n",
        "\n",
        "**⚠️ 重要说明：Embeddings 方案**\n",
        "\n",
        "DeepSeek **不支持 Embeddings API**，因此本 Notebook 使用以下方案：\n",
        "\n",
        "1. **聊天对话**：使用 DeepSeek（便宜快速）\n",
        "2. **向量化**：使用本地免费模型 `sentence-transformers`\n",
        "\n",
        "这是最佳组合：\n",
        "- ✅ 完全免费的 Embeddings\n",
        "- ✅ 低成本的 LLM 对话\n",
        "- ✅ 无需多个 API Key\n",
        "\n",
        "**注意事项：**\n",
        "1. ⚠️ 记得替换 DeepSeek API Key！\n",
        "2. 📦 首次运行会自动下载 Embedding 模型（约 500MB）\n",
        "3. 📊 查看用量：https://platform.deepseek.com/usage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 一、Prompt Engineering 技术\n",
        "\n",
        "展示各种提示工程技术的应用\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Zero-shot Prompting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Zero-shot 结果: 正面\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    temperature=0, \n",
        "    model=\"deepseek-chat\",\n",
        "    openai_api_base=\"https://api.deepseek.com\"\n",
        ")\n",
        "\n",
        "zero_shot_prompt = ChatPromptTemplate.from_template(\n",
        "    \"将以下文本分类为正面、负面或中性情感：\\n\\n{text}\"\n",
        ")\n",
        "\n",
        "chain = zero_shot_prompt | llm\n",
        "\n",
        "result = chain.invoke({\"text\": \"这个产品真的很棒，我非常喜欢！\"})\n",
        "print(\"Zero-shot 结果:\", result.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Few-shot Prompting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Few-shot 结果: 正面\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import FewShotChatMessagePromptTemplate\n",
        "\n",
        "examples = [\n",
        "    {\"input\": \"这个产品质量很好\", \"output\": \"正面\"},\n",
        "    {\"input\": \"服务态度太差了\", \"output\": \"负面\"},\n",
        "    {\"input\": \"价格还可以\", \"output\": \"中性\"},\n",
        "]\n",
        "\n",
        "example_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"ai\", \"{output}\"),\n",
        "])\n",
        "\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")\n",
        "\n",
        "final_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"你是一个情感分类专家。\"),\n",
        "    few_shot_prompt,\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "chain = final_prompt | llm\n",
        "result = chain.invoke({\"input\": \"物流速度挺快的\"})\n",
        "print(\"Few-shot 结果:\", result.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Chain of Thought (CoT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CoT 结果:\n",
            " 好的，我们按步骤来推理。  \n",
            "\n",
            "---\n",
            "\n",
            "**1. 理解问题关键信息**  \n",
            "- 一个苹果原价：3 元  \n",
            "- 买的数量：5 个  \n",
            "- 折扣：8 折（即原价的 80%）  \n",
            "\n",
            "---\n",
            "\n",
            "**2. 列出解决步骤**  \n",
            "- 第一步：计算原价总金额  \n",
            "\\[\n",
            "5 \\times 3 = 15 \\text{ 元}\n",
            "\\]  \n",
            "- 第二步：计算打折后的价格  \n",
            "\\[\n",
            "15 \\times 0.8 = 12 \\text{ 元}\n",
            "\\]  \n",
            "\n",
            "---\n",
            "\n",
            "**3. 得出答案**  \n",
            "\\[\n",
            "\\boxed{12}\n",
            "\\]  \n",
            "\n",
            "所以，买 5 个苹果打 8 折后需要 **12 元**。\n"
          ]
        }
      ],
      "source": [
        "cot_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"问题：{question}\n",
        "\n",
        "让我们一步一步思考：\n",
        "1. 首先，理解问题的关键信息\n",
        "2. 然后，列出解决步骤\n",
        "3. 最后，得出答案\n",
        "\n",
        "请按照上述步骤回答。\"\"\"\n",
        ")\n",
        "\n",
        "chain = cot_prompt | llm\n",
        "result = chain.invoke({\"question\": \"如果一个苹果3元，买5个苹果打8折，需要多少钱？\"})\n",
        "print(\"CoT 结果:\\n\", result.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 二、LangChain 核心组件使用\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "解析结果: name='张三' age=30 occupation='软件工程师'\n",
            "姓名: 张三, 年龄: 30, 职业: 软件工程师\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import OpenAI\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Person(BaseModel):\n",
        "    name: str = Field(description=\"人物姓名\")\n",
        "    age: int = Field(description=\"人物年龄\")\n",
        "    occupation: str = Field(description=\"职业\")\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Person)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"提取以下文本中的人物信息：\n",
        "{text}\n",
        "\n",
        "{format_instructions}\"\"\"\n",
        ")\n",
        "\n",
        "chain = prompt | llm | parser\n",
        "\n",
        "result = chain.invoke({\n",
        "    \"text\": \"张三今年30岁，是一名软件工程师\",\n",
        "    \"format_instructions\": parser.get_format_instructions()\n",
        "})\n",
        "\n",
        "print(\"解析结果:\", result)\n",
        "print(f\"姓名: {result.name}, 年龄: {result.age}, 职业: {result.occupation}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 三、Chat History - 对话历史管理\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/y_/g595hp191h75fwdk_s4cf6bm0000gn/T/ipykernel_88133/2492734910.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory()\n",
            "/var/folders/y_/g595hp191h75fwdk_s4cf6bm0000gn/T/ipykernel_88133/2492734910.py:5: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
            "  conversation = ConversationChain(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "对话 1:\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: 我喜欢Python编程\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "太棒了！Python是一门非常强大且多用途的编程语言，我很高兴你对它感兴趣！无论是数据分析、人工智能、网络开发还是自动化脚本，Python都能大显身手。你喜欢用Python做什么呢？是开发网站、进行数据分析，还是探索机器学习领域？如果你有任何问题或需要学习资源，我都很乐意帮助你！ 😊\n",
            "\n",
            "==================================================\n",
            "\n",
            "对话 2:\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: 我喜欢Python编程\n",
            "AI: 太棒了！Python是一门非常强大且多用途的编程语言，我很高兴你对它感兴趣！无论是数据分析、人工智能、网络开发还是自动化脚本，Python都能大显身手。你喜欢用Python做什么呢？是开发网站、进行数据分析，还是探索机器学习领域？如果你有任何问题或需要学习资源，我都很乐意帮助你！ 😊\n",
            "Human: 我刚才说我喜欢什么？\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "你刚才说：“我喜欢Python编程”！看来你对这门语言真的很有热情呢。需要我推荐一些有趣的Python项目灵感，或是聊聊某个具体的应用方向吗？比如用Django搭建博客、用Pandas分析数据，或者用OpenCV处理图像都很有意思哦~ 🐍\n",
            "\n",
            "对话历史已持久化到文件\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferMemory, FileChatMessageHistory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"对话 1:\")\n",
        "print(conversation.predict(input=\"我喜欢Python编程\"))\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "print(\"对话 2:\")\n",
        "print(conversation.predict(input=\"我刚才说我喜欢什么？\"))\n",
        "\n",
        "file_history = FileChatMessageHistory(\"chat_history.json\")\n",
        "file_history.add_user_message(\"什么是LangChain？\")\n",
        "file_history.add_ai_message(\"LangChain是一个用于开发LLM应用的框架。\")\n",
        "print(\"\\n对话历史已持久化到文件\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 四、自定义 Tools 和 Agents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "工具测试:\n",
            "时间: 2025-10-12 21:01:28\n",
            "计算: 30\n",
            "搜索: 搜索结果：关于 'LangChain教程' 的信息...(这是模拟结果)\n"
          ]
        }
      ],
      "source": [
        "from langchain.tools import tool\n",
        "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
        "from langchain.prompts import MessagesPlaceholder\n",
        "import requests\n",
        "\n",
        "@tool\n",
        "def get_current_time(format: str = \"%Y-%m-%d %H:%M:%S\") -> str:\n",
        "    \"\"\"获取当前时间\"\"\"\n",
        "    return datetime.now().strftime(format)\n",
        "\n",
        "@tool\n",
        "def calculator(expression: str) -> str:\n",
        "    \"\"\"计算数学表达式\"\"\"\n",
        "    try:\n",
        "        result = eval(expression)\n",
        "        return str(result)\n",
        "    except Exception as e:\n",
        "        return f\"计算错误: {str(e)}\"\n",
        "\n",
        "@tool\n",
        "def search_web(query: str) -> str:\n",
        "    \"\"\"模拟网页搜索\"\"\"\n",
        "    return f\"搜索结果：关于 '{query}' 的信息...(这是模拟结果)\"\n",
        "\n",
        "@tool\n",
        "async def async_search(query: str) -> str:\n",
        "    \"\"\"异步搜索工具\"\"\"\n",
        "    import asyncio\n",
        "    await asyncio.sleep(0.5)\n",
        "    return f\"异步搜索结果: {query}\"\n",
        "\n",
        "print(\"工具测试:\")\n",
        "print(\"时间:\", get_current_time.invoke({}))\n",
        "print(\"计算:\", calculator.invoke({\"expression\": \"(10 + 5) * 2\"}))\n",
        "print(\"搜索:\", search_web.invoke({\"query\": \"LangChain教程\"}))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `get_current_time` with `{}`\n",
            "responded: 我来帮您获取当前时间并计算数学表达式。\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m2025-10-12 21:01:52\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `calculator` with `{'expression': '25 * 4'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m100\u001b[0m\u001b[32;1m\u001b[1;3m根据查询结果：\n",
            "- 当前时间是：2025年10月12日 21:01:52\n",
            "- 25 × 4 = 100\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Agent 结果: 根据查询结果：\n",
            "- 当前时间是：2025年10月12日 21:01:52\n",
            "- 25 × 4 = 100\n"
          ]
        }
      ],
      "source": [
        "tools = [get_current_time, calculator, search_web]\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"你是一个有用的AI助手。使用提供的工具来回答问题。\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "])\n",
        "\n",
        "agent = create_openai_tools_agent(llm=llm, tools=tools, prompt=prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "result = agent_executor.invoke({\n",
        "    \"input\": \"现在几点？然后帮我计算 25 * 4 等于多少\"\n",
        "})\n",
        "print(\"\\nAgent 结果:\", result[\"output\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 五、向量数据库和 RAG\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📦 初始化本地 Embedding 模型...\n",
            "💡 提示：DeepSeek 不支持 Embeddings API，使用免费的本地模型\n",
            "⏳ 首次运行会下载模型，请稍等...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e0acf2bd55346cc997bb0dd922ce878",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b6506c6b440c40709c7462f572773fce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30ec1e2806c8489e837debdb25ccb803",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2bd270f9e78b49628c5d464c56a5e92b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "488ec113ea08415f8a9c4446ac5a62c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Embedding 模型加载完成！\n",
            "\n",
            "🔄 创建向量存储...\n",
            "✅ 创建了包含 5 个文档块的向量存储\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "\n",
        "print(\"📦 初始化本地 Embedding 模型...\")\n",
        "print(\"💡 提示：DeepSeek 不支持 Embeddings API，使用免费的本地模型\")\n",
        "print(\"⏳ 首次运行会下载模型，请稍等...\")\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
        "    model_kwargs={'device': 'cpu'},\n",
        "    encode_kwargs={'normalize_embeddings': True}\n",
        ")\n",
        "\n",
        "print(\"✅ Embedding 模型加载完成！\\n\")\n",
        "\n",
        "documents = [\n",
        "    Document(page_content=\"LangChain是一个用于开发语言模型应用的框架。\", metadata={\"source\": \"doc1\"}),\n",
        "    Document(page_content=\"向量数据库用于存储和检索向量表示的文档。\", metadata={\"source\": \"doc2\"}),\n",
        "    Document(page_content=\"RAG是检索增强生成，结合检索和生成的技术。\", metadata={\"source\": \"doc3\"}),\n",
        "    Document(page_content=\"Embeddings将文本转换为数值向量表示。\", metadata={\"source\": \"doc4\"}),\n",
        "    Document(page_content=\"Chroma是一个轻量级的向量数据库。\", metadata={\"source\": \"doc5\"}),\n",
        "]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=20\n",
        ")\n",
        "\n",
        "splits = text_splitter.split_documents(documents)\n",
        "\n",
        "print(\"🔄 创建向量存储...\")\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=\"./chroma_db\"\n",
        ")\n",
        "\n",
        "print(f\"✅ 创建了包含 {len(splits)} 个文档块的向量存储\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 相似度搜索\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "查询: 什么是RAG？\n",
            "\n",
            "相似度搜索结果:\n",
            "\n",
            "结果 1:\n",
            "内容: RAG是检索增强生成，结合检索和生成的技术。\n",
            "元数据: {'source': 'doc3'}\n",
            "\n",
            "结果 2:\n",
            "内容: LangChain是一个用于开发语言模型应用的框架。\n",
            "元数据: {'source': 'doc1'}\n",
            "\n",
            "带相似度分数的结果:\n",
            "分数: 1.1480 - RAG是检索增强生成，结合检索和生成的技术。\n",
            "分数: 1.5999 - LangChain是一个用于开发语言模型应用的框架。\n"
          ]
        }
      ],
      "source": [
        "query = \"什么是RAG？\"\n",
        "results = vectorstore.similarity_search(query, k=2)\n",
        "\n",
        "print(f\"查询: {query}\")\n",
        "print(\"\\n相似度搜索结果:\")\n",
        "for i, doc in enumerate(results):\n",
        "    print(f\"\\n结果 {i+1}:\")\n",
        "    print(f\"内容: {doc.page_content}\")\n",
        "    print(f\"元数据: {doc.metadata}\")\n",
        "\n",
        "results_with_scores = vectorstore.similarity_search_with_score(query, k=2)\n",
        "print(\"\\n带相似度分数的结果:\")\n",
        "for doc, score in results_with_scores:\n",
        "    print(f\"分数: {score:.4f} - {doc.page_content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 RAG 检索链\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "问题: LangChain是什么？\n",
            "\n",
            "答案: 根据上下文，LangChain是一个用于开发语言模型应用的框架。\n",
            "\n",
            "来源文档:\n",
            "- LangChain是一个用于开发语言模型应用的框架。\n",
            "- RAG是检索增强生成，结合检索和生成的技术。\n",
            "- Chroma是一个轻量级的向量数据库。\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "template = \"\"\"基于以下上下文回答问题：\n",
        "\n",
        "上下文: {context}\n",
        "\n",
        "问题: {question}\n",
        "\n",
        "答案:\"\"\"\n",
        "\n",
        "QA_PROMPT = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    chain_type_kwargs={\"prompt\": QA_PROMPT},\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "question = \"LangChain是什么？\"\n",
        "result = qa_chain.invoke({\"query\": question})\n",
        "\n",
        "print(f\"问题: {question}\")\n",
        "print(f\"\\n答案: {result['result']}\")\n",
        "print(\"\\n来源文档:\")\n",
        "for doc in result['source_documents']:\n",
        "    print(f\"- {doc.page_content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 带记忆的 RAG 系统\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q1: 什么是向量数据库？\n",
            "A1: 向量数据库是一种专门用于存储和检索向量形式数据的数据库系统。它通过将文档、图像或其他类型的数据转换为数值向量（通常由机器学习模型生成），并利用相似度计算（如余弦相似度）来高效检索与查询内容最相关的信息。例如，Chroma 就是一个轻量级的向量数据库，常用于支持检索增强生成（RAG）等应用场景。\n",
            "\n",
            "Q2: 它的主要用途是什么？\n",
            "A2: 根据提供的上下文，向量数据库的主要用途是**存储和检索向量表示的文档**。\n",
            "\n",
            "简单来说，它就像一个专门为“向量”这种数据格式设计的图书馆，可以高效地存储文档的数学表示（即向量），并根据查询快速找到最相关的结果。\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True,\n",
        "    output_key=\"answer\"\n",
        ")\n",
        "\n",
        "conversational_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    memory=memory,\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "question1 = \"什么是向量数据库？\"\n",
        "result1 = conversational_chain.invoke({\"question\": question1})\n",
        "print(f\"Q1: {question1}\")\n",
        "print(f\"A1: {result1['answer']}\\n\")\n",
        "\n",
        "question2 = \"它的主要用途是什么？\"\n",
        "result2 = conversational_chain.invoke({\"question\": question2})\n",
        "print(f\"Q2: {question2}\")\n",
        "print(f\"A2: {result2['answer']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 六、实战案例：完整的 RAG 问答机器人\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question.\n",
            "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "----------------\n",
            "LangChain是一个用于开发语言模型应用的框架。\n",
            "\n",
            "RAG是检索增强生成，结合检索和生成的技术。\n",
            "\n",
            "Chroma是一个轻量级的向量数据库。\n",
            "Human: 什么是LangChain？\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Q: 什么是LangChain？\n",
            "A: 根据提供的上下文，LangChain是一个用于开发语言模型应用的框架。\n",
            "\n",
            "来源: ['LangChain是一个用于开发语言模型应用的框架。', 'RAG是检索增强生成，结合检索和生成的技术。', 'Chroma是一个轻量级的向量数据库。']\n",
            "\n",
            "==================================================\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: 什么是LangChain？\n",
            "Assistant: 根据提供的上下文，LangChain是一个用于开发语言模型应用的框架。\n",
            "Follow Up Input: 它和向量数据库有什么关系？\n",
            "Standalone question:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question.\n",
            "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "----------------\n",
            "向量数据库用于存储和检索向量表示的文档。\n",
            "\n",
            "LangChain是一个用于开发语言模型应用的框架。\n",
            "\n",
            "Chroma是一个轻量级的向量数据库。\n",
            "Human: LangChain和向量数据库有什么关系？\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Q: 它和向量数据库有什么关系？\n",
            "A: 根据提供的上下文，LangChain和向量数据库的关系可以概括为：\n",
            "\n",
            "**LangChain是一个应用开发框架，而向量数据库是它可以集成和利用的一种数据存储与检索组件。**\n",
            "\n",
            "具体来说：\n",
            "\n",
            "1.  **功能定位不同**：\n",
            "    *   **LangChain**：是一个用于构建由语言模型驱动的应用程序的**框架**。它提供了各种工具和组件，用于连接语言模型、数据源、记忆系统等，并编排应用程序的工作流程。\n",
            "    *   **向量数据库**：是一种专门用于**存储和检索**高维向量数据（例如文档的向量表示）的数据库。\n",
            "\n",
            "2.  **协同工作关系**：\n",
            "    *   在LangChain构建的应用程序中，一个非常常见的需求是让语言模型能够访问和处理外部数据（如您自己的文档）。\n",
            "    *   为了实现这一目标，LangChain需要将文档转换为向量（这个过程称为“嵌入”），然后存储起来以便快速检索。\n",
            "    *   **向量数据库（如您提到的Chroma）正是扮演了这个“存储和检索”的角色**。LangChain内置了对多种向量数据库（包括Chroma）的支持，可以方便地将它们集成到应用链中。\n",
            "\n",
            "**一个典型的工作流程是**：\n",
            "*   使用LangChain的文本分割器将文档切块。\n",
            "*   使用LangChain的嵌入模型将文本块转换为向量。\n",
            "*   使用LangChain的向量存储接口，将这些向量及其对应的原始文本存储到**向量数据库（如Chroma）** 中。\n",
            "*   当用户提问时，LangChain将问题也转换为向量，并查询**向量数据库**，找到最相关的文档片段。\n",
            "*   最后，LangChain将这些相关片段和问题一起组合成提示，发送给语言模型来生成最终答案。\n",
            "\n",
            "**总结**：您可以将向量数据库视为LangChain生态系统中的一个关键**基础设施**或**插件**，专门负责高效处理向量化后的数据，从而赋能LangChain应用程序实现强大的检索增强生成（RAG）等功能。\n"
          ]
        }
      ],
      "source": [
        "class RAGChatBot:\n",
        "    def __init__(self, llm, vectorstore):\n",
        "        self.llm = llm\n",
        "        self.vectorstore = vectorstore\n",
        "        self.memory = ConversationBufferMemory(\n",
        "            memory_key=\"chat_history\",\n",
        "            return_messages=True,\n",
        "            output_key=\"answer\"\n",
        "        )\n",
        "        \n",
        "        self.retriever = self.vectorstore.as_retriever(\n",
        "            search_kwargs={\"k\": 3}\n",
        "        )\n",
        "        \n",
        "        self.chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm=self.llm,\n",
        "            retriever=self.retriever,\n",
        "            memory=self.memory,\n",
        "            return_source_documents=True,\n",
        "            verbose=True\n",
        "        )\n",
        "    \n",
        "    def chat(self, question: str) -> Dict[str, Any]:\n",
        "        result = self.chain.invoke({\"question\": question})\n",
        "        return {\n",
        "            \"answer\": result[\"answer\"],\n",
        "            \"sources\": [doc.page_content for doc in result[\"source_documents\"]]\n",
        "        }\n",
        "    \n",
        "    def get_history(self):\n",
        "        return self.memory.load_memory_variables({})\n",
        "\n",
        "chatbot = RAGChatBot(llm, vectorstore)\n",
        "\n",
        "response1 = chatbot.chat(\"什么是LangChain？\")\n",
        "print(\"Q: 什么是LangChain？\")\n",
        "print(f\"A: {response1['answer']}\")\n",
        "print(f\"\\n来源: {response1['sources']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "response2 = chatbot.chat(\"它和向量数据库有什么关系？\")\n",
        "print(\"Q: 它和向量数据库有什么关系？\")\n",
        "print(f\"A: {response2['answer']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 七、LangServe - API 服务化\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ LangServe 服务代码已保存到 scripts/langserve_app.py\n",
            "   (已配置 DeepSeek 模型)\n",
            "\n",
            "运行方法:\n",
            "cd scripts && python langserve_app.py\n",
            "\n",
            "访问文档: http://localhost:8000/docs\n"
          ]
        }
      ],
      "source": [
        "server_code = '''\n",
        "import os\n",
        "from fastapi import FastAPI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langserve import add_routes\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your-deepseek-api-key-here\"\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"https://api.deepseek.com\"\n",
        "\n",
        "app = FastAPI(\n",
        "    title=\"LangChain Server\",\n",
        "    version=\"1.0\",\n",
        "    description=\"A simple API server using LangChain with DeepSeek\",\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"deepseek-chat\",\n",
        "    openai_api_base=\"https://api.deepseek.com\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"告诉我一个关于{topic}的故事\"\n",
        ")\n",
        "\n",
        "chain = prompt | llm\n",
        "\n",
        "add_routes(\n",
        "    app,\n",
        "    chain,\n",
        "    path=\"/story\",\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "    uvicorn.run(app, host=\"localhost\", port=8000)\n",
        "'''\n",
        "\n",
        "with open(\"./scripts/langserve_app.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(server_code)\n",
        "\n",
        "print(\"✅ LangServe 服务代码已保存到 scripts/langserve_app.py\")\n",
        "print(\"   (已配置 DeepSeek 模型)\")\n",
        "print(\"\\n运行方法:\")\n",
        "print(\"cd scripts && python langserve_app.py\")\n",
        "print(\"\\n访问文档: http://localhost:8000/docs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 八、Streamlit Web 界面\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Streamlit 应用已保存到 scripts/streamlit_app.py\n",
            "   (已配置 DeepSeek 模型)\n",
            "\n",
            "运行方法:\n",
            "streamlit run scripts/streamlit_app.py\n"
          ]
        }
      ],
      "source": [
        "streamlit_app = '''\n",
        "import os\n",
        "import streamlit as st\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your-deepseek-api-key-here\"\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"https://api.deepseek.com\"\n",
        "\n",
        "st.title(\"🤖 RAG 问答机器人 (DeepSeek)\")\n",
        "\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "@st.cache_resource\n",
        "def load_chain():\n",
        "    llm = ChatOpenAI(\n",
        "        temperature=0,\n",
        "        model=\"deepseek-chat\",\n",
        "        openai_api_base=\"https://api.deepseek.com\"\n",
        "    )\n",
        "    embeddings = OpenAIEmbeddings(\n",
        "        openai_api_base=\"https://api.deepseek.com\"\n",
        "    )\n",
        "    vectorstore = Chroma(\n",
        "        persist_directory=\"./chroma_db\",\n",
        "        embedding_function=embeddings\n",
        "    )\n",
        "    \n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key=\"chat_history\",\n",
        "        return_messages=True,\n",
        "        output_key=\"answer\"\n",
        "    )\n",
        "    \n",
        "    chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vectorstore.as_retriever(),\n",
        "        memory=memory,\n",
        "        return_source_documents=True\n",
        "    )\n",
        "    \n",
        "    return chain\n",
        "\n",
        "chain = load_chain()\n",
        "\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "if prompt := st.chat_input(\"请输入您的问题...\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "    \n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"思考中...\"):\n",
        "            response = chain.invoke({\"question\": prompt})\n",
        "            answer = response[\"answer\"]\n",
        "            st.markdown(answer)\n",
        "            \n",
        "            with st.expander(\"查看来源文档\"):\n",
        "                for i, doc in enumerate(response[\"source_documents\"]):\n",
        "                    st.markdown(f\"**来源 {i+1}:** {doc.page_content}\")\n",
        "    \n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer})\n",
        "\n",
        "if st.sidebar.button(\"清除对话历史\"):\n",
        "    st.session_state.messages = []\n",
        "    st.rerun()\n",
        "'''\n",
        "\n",
        "with open(\"./scripts/streamlit_app.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(streamlit_app)\n",
        "\n",
        "print(\"✅ Streamlit 应用已保存到 scripts/streamlit_app.py\")\n",
        "print(\"   (已配置 DeepSeek 模型)\")\n",
        "print(\"\\n运行方法:\")\n",
        "print(\"streamlit run scripts/streamlit_app.py\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 九、总结\n",
        "\n",
        "本 Notebook 全面展示了 LangChain 框架的核心开发技能：\n",
        "\n",
        "### ✅ 已掌握的技能\n",
        "\n",
        "1. **Prompt Engineering 技术**\n",
        "   - Zero-shot、Few-shot、COT、ReAct、Prompt Chaining\n",
        "\n",
        "2. **核心组件使用**\n",
        "   - LLM、Chat Models、PromptTemplates、Output Parsers、Chains (LCEL)\n",
        "\n",
        "3. **调试监控**\n",
        "   - Verbose 日志、Debug 模式、自定义回调处理器\n",
        "\n",
        "4. **对话历史管理**\n",
        "   - 内存管理、文件持久化、带记忆的对话链\n",
        "\n",
        "5. **Tools 和 Agents**\n",
        "   - 自定义工具、同步/异步调用、Agent 执行器\n",
        "   - 天气查询、计算器、网页搜索等实用工具\n",
        "\n",
        "6. **向量数据库和 RAG**\n",
        "   - Chroma 向量存储、相似度搜索、RAG 检索链\n",
        "   - 带记忆的 RAG 系统\n",
        "\n",
        "7. **文档处理**\n",
        "   - 文档加载器、文档分割器、文本预处理\n",
        "\n",
        "8. **服务部署**\n",
        "   - LangServe API 服务、Streamlit Web 界面\n",
        "\n",
        "### 🚀 扩展方向\n",
        "\n",
        "- 多模态支持（图像、音频）\n",
        "- 流式输出优化\n",
        "- 高级 RAG 技术（重排序、混合检索）\n",
        "- LangSmith 深度集成\n",
        "- 生产环境优化\n",
        "\n",
        "### 💡 最佳实践\n",
        "\n",
        "- 使用 LCEL 构建可组合的链\n",
        "- 合理使用 Verbose 和 Debug 进行调试\n",
        "- 为生产环境配置适当的内存管理策略\n",
        "- 使用向量数据库优化检索性能\n",
        "- 实现错误处理和重试机制\n",
        "- 监控 Token 使用和成本\n",
        "\n",
        "---\n",
        "\n",
        "**🎉 恭喜！你已经掌握了 LangChain 的核心开发技能！**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
