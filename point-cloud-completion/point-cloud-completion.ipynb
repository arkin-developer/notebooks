{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ¯ 3Dç‚¹äº‘è¡¥å…¨æ¡ˆä¾‹ - ShapeNetç½‘ç»œ + PCNæ•°æ®é›†\n",
        "\n",
        "è¿™æ˜¯ä¸€ä¸ªç®€å•çš„ç‚¹äº‘è¡¥å…¨æ¼”ç¤ºé¡¹ç›®ï¼Œå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯ä»ä¸å®Œæ•´çš„ç‚¹äº‘æ•°æ®ä¸­é‡å»ºå®Œæ•´çš„3Då½¢çŠ¶ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ä¸€ã€3Dç‚¹äº‘è¡¥å…¨ä»»åŠ¡ç®€ä»‹\n",
        "\n",
        "3Dç‚¹äº‘è¡¥å…¨ï¼ˆPoint Cloud Completionï¼‰è‡´åŠ›äºåœ¨ä¼ æ„Ÿå™¨é‡‡é›†å­˜åœ¨é®æŒ¡ã€ç¨€ç–å’Œå™ªå£°çš„ç°å®æ¡ä»¶ä¸‹ï¼Œé‡å»ºç›®æ ‡æˆ–åœºæ™¯çš„å®Œæ•´å‡ ä½•ã€‚å®ƒä½äºâ€œ3Dæ„ŸçŸ¥â€æŠ€æœ¯æ ˆä¸­ï¼Œæ‰¿ä¸Šæ¸¸é‡‡é›†/é‡å»ºï¼ˆLiDARã€RGBâ€‘Dã€SfMï¼‰ä¹‹ç¼ºï¼Œå¯ä¸‹æ¸¸ç†è§£/äº¤äº’ï¼ˆè¯†åˆ«ã€å®šä½ã€äº¤äº’ã€ä»¿çœŸï¼‰ä¹‹ç”¨ã€‚\n",
        "\n",
        "- å®šä½ï¼ˆåœ¨3Dæ„ŸçŸ¥æµæ°´çº¿ä¸­çš„è§’è‰²ï¼‰\n",
        "  - ä¸Šæ¸¸ï¼šä¼ æ„Ÿå™¨é‡‡æ ·ä¸å¯é¿å…çš„è§†è§’é®æŒ¡ã€è·ç¦»è¡°å‡ã€æè´¨åå°„å¯¼è‡´ç‚¹äº‘ç¼ºå¤±ä¸ä¸å‡åŒ€ã€‚\n",
        "  - ä¸­æ¸¸ï¼šè¡¥å…¨æ¨¡å—ä¾æ®å…ˆéªŒï¼ˆå½¢çŠ¶ã€ç»“æ„ã€æ‹“æ‰‘ï¼‰æ¨æ–­ç¼ºå¤±åŒºåŸŸï¼Œè¾“å‡ºè‡´å¯†ã€è¿è´¯çš„ç‚¹é›†æˆ–éšå¼å‡ ä½•ã€‚\n",
        "  - ä¸‹æ¸¸ï¼šä¸ºæ£€æµ‹/åˆ†å‰²/å§¿æ€ä¼°è®¡ã€é…å‡†/å»ºå›¾ã€ç‰©ç†ä»¿çœŸä¸æ¸²æŸ“æä¾›æ›´å®Œæ•´ã€æ›´é²æ£’çš„è¾“å…¥ã€‚\n",
        "\n",
        "- ä½œç”¨ï¼ˆä¸ºä»€ä¹ˆéœ€è¦è¡¥å…¨ï¼‰\n",
        "  - æŠ—é®æŒ¡ä¸æŠ—ç¨€ç–ï¼šæ¢å¤å…³é”®ç»“æ„ï¼Œæé«˜ä¸‹æ¸¸ä»»åŠ¡é²æ£’æ€§ä¸ç²¾åº¦ã€‚\n",
        "  - ç»Ÿä¸€å¯†åº¦ä¸å°ºåº¦ï¼šä¸ºåŸºäºç‚¹çš„ç½‘ç»œä¸NerF/éšå¼è¡¨é¢æ–¹æ³•æä¾›æ›´ç¨³å®šçš„å‡ ä½•è¡¨ç¤ºã€‚\n",
        "  - é™å™ªä¸å¯è§†åŒ–ï¼šè¡¥é½è½®å»“ã€å‡å°‘å­”æ´ï¼Œä¾¿äºæµ‹é‡ã€å±•ç¤ºä¸åˆ¶é€ ã€‚\n",
        "\n",
        "- å…¸å‹åº”ç”¨åœºæ™¯\n",
        "  - è‡ªåŠ¨é©¾é©¶/æœºå™¨äººï¼šå•å¸§æˆ–å¤šå¸§LiDARè¡¥å…¨ï¼Œæå‡ç›®æ ‡è½®å»“ä¸è·ç¦»ä¼°è®¡ï¼›æœºæ¢°è‡‚æŠ“å–ä¸­çš„å½¢çŠ¶æ¨æ–­ã€‚\n",
        "  - AR/VRä¸æ•°å­—å­ªç”Ÿï¼šå®¤å†…/å»ºç­‘æ‰«æè¡¥æ´ï¼Œç”Ÿæˆå®Œæ•´çš„èµ„äº§ç”¨äºæ¸²æŸ“ä¸äº¤äº’ã€‚\n",
        "  - é€†å‘å·¥ç¨‹/å·¥ä¸šæ£€æµ‹ï¼šéƒ¨ä»¶æ‰«æè¡¥å…¨ç”¨äºå°ºå¯¸æµ‹é‡ã€è¯¯å·®å¯¹æ¯”ä¸åç»­CADé‡å»ºã€‚\n",
        "  - æ–‡åŒ–é—äº§/æ–‡ç‰©ä¿®å¤ï¼šå—é™è§†è§’é‡‡é›†ä¸‹çš„ç»“æ„è¡¥å…¨ä¸è¿˜åŸã€‚\n",
        "\n",
        "- æ•°æ®ä¸å½¢å¼\n",
        "  - è¾“å…¥ï¼šä¸å®Œæ•´ç‚¹äº‘ï¼ˆpartialï¼‰ï¼Œå¯æ¥è‡ªLiDARã€RGBâ€‘Dæˆ–ä»CADç½‘æ ¼æŠ•å½±é‡‡æ ·ã€‚\n",
        "  - è¾“å‡ºï¼šå®Œæ•´ç‚¹äº‘ï¼ˆcompleteï¼‰ã€è¡¨é¢ç½‘æ ¼æˆ–éšå¼åœºï¼ˆå¦‚SDF/Occ/NerFï¼‰ã€‚\n",
        "  - ç›‘ç£ï¼šæˆå¯¹çš„ partial/completeï¼ˆåˆæˆç®¡çº¿å¸¸ç”¨ï¼‰æˆ–å¼±/è‡ªç›‘ç£ï¼ˆçœŸå®æ•°æ®ï¼‰ã€‚\n",
        "\n",
        "- è¯„æµ‹æŒ‡æ ‡ä¸ç›®æ ‡\n",
        "  - å‡ ä½•ä¸€è‡´æ€§ï¼šChamfer Distanceï¼ˆL1/L2ï¼‰ã€EMDã€Fâ€‘score@Ï„ã€‚\n",
        "  - ç»“æ„åˆç†æ€§ï¼šæ³•å‘ä¸€è‡´ã€æ›²ç‡è¿ç»­ã€ä½“ç´ /ç½‘æ ¼é‡å»ºè´¨é‡ã€‚\n",
        "  - ä¸‹æ¸¸æ”¶ç›Šï¼šåœ¨æ£€æµ‹/åˆ†å‰²/æŠ“å–ç­‰ä»»åŠ¡ä¸­çš„æ€§èƒ½å¢ç›Šã€‚\n",
        "\n",
        "- æ–¹æ³•æ¦‚è§ˆï¼ˆç®€ï¼‰\n",
        "  - åŸºäºå…ˆéªŒçš„æ˜¾å¼è¡¥æ´ï¼ˆæ¨¡æ¿/å¯¹ç§°æ€§/æ£€ç´¢ï¼‰ä¸æ·±åº¦å­¦ä¹ å¼éšå¼/æ˜¾å¼é‡å»ºã€‚\n",
        "  - å…¨å±€å…ˆéªŒï¼ˆç¼–ç å™¨â€‘è§£ç å™¨ã€æ‰©æ•£/è‡ªå›å½’ï¼‰+ å±€éƒ¨ç»†åŒ–ï¼ˆpatch/ä¸Šé‡‡æ ·/ç»†èŠ‚å¢å¼ºï¼‰ã€‚\n",
        "\n",
        "æœ¬Notebookèšç„¦æœ€å°å¯è¡ŒDemoï¼šä»¥ç®€åŒ–PCNèŒƒå¼å®Œæˆä»partialåˆ°completeçš„ç«¯åˆ°ç«¯è®­ç»ƒä¸å¯è§†åŒ–ï¼Œä¾¿äºå¿«é€Ÿç†è§£è¡¥å…¨ä»»åŠ¡çš„å®šä½ä¸ä»·å€¼ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å®‰è£…å¿…è¦çš„ä¾èµ–åŒ…\n",
        "%pip install torch numpy open3d matplotlib tqdm lmdb\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import open3d as o3d\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import lmdb\n",
        "import pickle\n",
        "\n",
        "# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## äºŒã€ç¯å¢ƒé…ç½®ä¸ä¾èµ–å®‰è£…"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_point_cloud(points, colors=None, window_name=\"Point Cloud\"):\n",
        "    \"\"\"\n",
        "    ä½¿ç”¨Open3Då¯è§†åŒ–ç‚¹äº‘æ•°æ®\n",
        "    \n",
        "    Args:\n",
        "        points: numpyæ•°ç»„ï¼Œå½¢çŠ¶ä¸º(N, 3)ï¼Œè¡¨ç¤ºç‚¹äº‘åæ ‡\n",
        "        colors: numpyæ•°ç»„ï¼Œå½¢çŠ¶ä¸º(N, 3)ï¼Œè¡¨ç¤ºç‚¹äº‘é¢œè‰²ï¼ŒèŒƒå›´[0,1]ï¼Œé»˜è®¤ä¸ºNone\n",
        "        window_name: çª—å£åç§°\n",
        "    \"\"\"\n",
        "    pcd = o3d.geometry.PointCloud()\n",
        "    pcd.points = o3d.utility.Vector3dVector(points)\n",
        "    \n",
        "    if colors is not None:\n",
        "        pcd.colors = o3d.utility.Vector3dVector(colors)\n",
        "    else:\n",
        "        # é»˜è®¤ä½¿ç”¨è“è‰²\n",
        "        pcd.paint_uniform_color([0, 0.651, 0.929])\n",
        "    \n",
        "    # åˆ›å»ºåæ ‡ç³»\n",
        "    coordinate_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(\n",
        "        size=0.5, origin=[0, 0, 0])\n",
        "    \n",
        "    # å¯è§†åŒ–ç‚¹äº‘\n",
        "    o3d.visualization.draw_geometries([pcd, coordinate_frame],\n",
        "                                    window_name=window_name,\n",
        "                                    width=800,\n",
        "                                    height=600)\n",
        "\n",
        "def visualize_partial_complete(partial, complete, window_name=\"Partial vs Complete\"):\n",
        "    \"\"\"\n",
        "    å¹¶æ’æ˜¾ç¤ºä¸å®Œæ•´å’Œå®Œæ•´çš„ç‚¹äº‘\n",
        "    \n",
        "    Args:\n",
        "        partial: numpyæ•°ç»„ï¼Œå½¢çŠ¶ä¸º(N, 3)ï¼Œè¡¨ç¤ºä¸å®Œæ•´ç‚¹äº‘\n",
        "        complete: numpyæ•°ç»„ï¼Œå½¢çŠ¶ä¸º(M, 3)ï¼Œè¡¨ç¤ºå®Œæ•´ç‚¹äº‘\n",
        "        window_name: çª—å£åç§°\n",
        "    \"\"\"\n",
        "    # åˆ›å»ºä¸å®Œæ•´ç‚¹äº‘å¯¹è±¡\n",
        "    pcd_partial = o3d.geometry.PointCloud()\n",
        "    pcd_partial.points = o3d.utility.Vector3dVector(partial)\n",
        "    pcd_partial.paint_uniform_color([1, 0, 0])  # çº¢è‰²è¡¨ç¤ºä¸å®Œæ•´ç‚¹äº‘\n",
        "    \n",
        "    # åˆ›å»ºå®Œæ•´ç‚¹äº‘å¯¹è±¡\n",
        "    pcd_complete = o3d.geometry.PointCloud()\n",
        "    pcd_complete.points = o3d.utility.Vector3dVector(complete)\n",
        "    pcd_complete.paint_uniform_color([0, 1, 0])  # ç»¿è‰²è¡¨ç¤ºå®Œæ•´ç‚¹äº‘\n",
        "    \n",
        "    # å°†å®Œæ•´ç‚¹äº‘å‘å³å¹³ç§»ï¼Œä»¥ä¾¿å¹¶æ’æ˜¾ç¤º\n",
        "    center_complete = pcd_complete.get_center()\n",
        "    center_partial = pcd_partial.get_center()\n",
        "    translation = np.array([2.0, 0, 0])  # å‘xè½´æ­£æ–¹å‘å¹³ç§»2ä¸ªå•ä½\n",
        "    pcd_complete.translate(translation)\n",
        "    \n",
        "    # åˆ›å»ºåæ ‡ç³»\n",
        "    coordinate_frame1 = o3d.geometry.TriangleMesh.create_coordinate_frame(\n",
        "        size=0.5, origin=[0, 0, 0])\n",
        "    coordinate_frame2 = o3d.geometry.TriangleMesh.create_coordinate_frame(\n",
        "        size=0.5, origin=translation)\n",
        "    \n",
        "    # å¯è§†åŒ–ç‚¹äº‘\n",
        "    o3d.visualization.draw_geometries([pcd_partial, pcd_complete, \n",
        "                                     coordinate_frame1, coordinate_frame2],\n",
        "                                    window_name=window_name,\n",
        "                                    width=1600,\n",
        "                                    height=600)\n",
        "\n",
        "# æµ‹è¯•å¯è§†åŒ–å‡½æ•°\n",
        "if __name__ == \"__main__\":\n",
        "    # ç”Ÿæˆç¤ºä¾‹ç‚¹äº‘æ•°æ®\n",
        "    num_points = 1000\n",
        "    \n",
        "    # ç”Ÿæˆä¸€ä¸ªçƒä½“çš„ç‚¹äº‘\n",
        "    theta = np.random.uniform(0, 2*np.pi, num_points)\n",
        "    phi = np.random.uniform(0, np.pi, num_points)\n",
        "    r = np.ones(num_points)\n",
        "    \n",
        "    x = r * np.sin(phi) * np.cos(theta)\n",
        "    y = r * np.sin(phi) * np.sin(theta)\n",
        "    z = r * np.cos(phi)\n",
        "    \n",
        "    complete_cloud = np.stack([x, y, z], axis=1)\n",
        "    \n",
        "    # ç”Ÿæˆä¸å®Œæ•´çš„ç‚¹äº‘ï¼ˆåªä¿ç•™ä¸ŠåŠéƒ¨åˆ†ï¼‰\n",
        "    partial_cloud = complete_cloud[complete_cloud[:, 2] > 0]\n",
        "    \n",
        "    # å¯è§†åŒ–\n",
        "    print(\"æ˜¾ç¤ºå®Œæ•´ç‚¹äº‘...\")\n",
        "    visualize_point_cloud(complete_cloud)\n",
        "    \n",
        "    print(\"æ˜¾ç¤ºä¸å®Œæ•´ç‚¹äº‘...\")\n",
        "    visualize_point_cloud(partial_cloud)\n",
        "    \n",
        "    print(\"å¹¶æ’æ˜¾ç¤ºå¯¹æ¯”...\")\n",
        "    visualize_partial_complete(partial_cloud, complete_cloud)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_point_cloud(points):\n",
        "    \"\"\"\n",
        "    å¯¹ç‚¹äº‘è¿›è¡Œå½’ä¸€åŒ–å¤„ç†\n",
        "    \n",
        "    Args:\n",
        "        points: numpyæ•°ç»„ï¼Œå½¢çŠ¶ä¸º(N, 3)\n",
        "        \n",
        "    Returns:\n",
        "        normalized_points: å½’ä¸€åŒ–åçš„ç‚¹äº‘\n",
        "        centroid: è´¨å¿ƒ\n",
        "        scale: ç¼©æ”¾å› å­\n",
        "    \"\"\"\n",
        "    # è®¡ç®—è´¨å¿ƒ\n",
        "    centroid = np.mean(points, axis=0)\n",
        "    \n",
        "    # å°†ç‚¹äº‘ä¸­å¿ƒç§»åˆ°åŸç‚¹\n",
        "    points = points - centroid\n",
        "    \n",
        "    # è®¡ç®—åˆ°åŸç‚¹çš„æœ€å¤§è·ç¦»\n",
        "    distances = np.sqrt(np.sum(points ** 2, axis=1))\n",
        "    scale = np.max(distances)\n",
        "    \n",
        "    # å½’ä¸€åŒ–åˆ°å•ä½çƒå†…\n",
        "    normalized_points = points / scale\n",
        "    \n",
        "    return normalized_points, centroid, scale\n",
        "\n",
        "def random_sample_points(points, num_points):\n",
        "    \"\"\"\n",
        "    éšæœºé‡‡æ ·å›ºå®šæ•°é‡çš„ç‚¹\n",
        "    \n",
        "    Args:\n",
        "        points: numpyæ•°ç»„ï¼Œå½¢çŠ¶ä¸º(N, 3)\n",
        "        num_points: éœ€è¦é‡‡æ ·çš„ç‚¹æ•°\n",
        "        \n",
        "    Returns:\n",
        "        sampled_points: é‡‡æ ·åçš„ç‚¹äº‘\n",
        "    \"\"\"\n",
        "    if len(points) >= num_points:\n",
        "        # éšæœºé‡‡æ ·\n",
        "        indices = np.random.choice(len(points), num_points, replace=False)\n",
        "        return points[indices]\n",
        "    else:\n",
        "        # å¦‚æœç‚¹æ•°ä¸è¶³ï¼Œåˆ™éœ€è¦é‡å¤é‡‡æ ·\n",
        "        indices = np.random.choice(len(points), num_points, replace=True)\n",
        "        return points[indices]\n",
        "\n",
        "def add_noise(points, sigma=0.01, clip=0.05):\n",
        "    \"\"\"\n",
        "    æ·»åŠ é«˜æ–¯å™ªå£°\n",
        "    \n",
        "    Args:\n",
        "        points: numpyæ•°ç»„ï¼Œå½¢çŠ¶ä¸º(N, 3)\n",
        "        sigma: é«˜æ–¯å™ªå£°çš„æ ‡å‡†å·®\n",
        "        clip: å™ªå£°çš„æœ€å¤§å€¼\n",
        "        \n",
        "    Returns:\n",
        "        noisy_points: æ·»åŠ å™ªå£°åçš„ç‚¹äº‘\n",
        "    \"\"\"\n",
        "    noise = np.clip(np.random.normal(0, sigma, points.shape), -clip, clip)\n",
        "    return points + noise\n",
        "\n",
        "def create_partial_point_cloud(points, num_patches=1):\n",
        "    \"\"\"\n",
        "    é€šè¿‡éšæœºç§»é™¤éƒ¨åˆ†åŒºåŸŸæ¥åˆ›å»ºä¸å®Œæ•´ç‚¹äº‘\n",
        "    \n",
        "    Args:\n",
        "        points: numpyæ•°ç»„ï¼Œå½¢çŠ¶ä¸º(N, 3)\n",
        "        num_patches: è¦ç§»é™¤çš„åŒºåŸŸæ•°é‡\n",
        "        \n",
        "    Returns:\n",
        "        partial_points: ä¸å®Œæ•´çš„ç‚¹äº‘\n",
        "    \"\"\"\n",
        "    points = points.copy()\n",
        "    num_points = len(points)\n",
        "    \n",
        "    for _ in range(num_patches):\n",
        "        # éšæœºé€‰æ‹©ä¸€ä¸ªä¸­å¿ƒç‚¹\n",
        "        center_idx = np.random.randint(0, num_points)\n",
        "        center = points[center_idx]\n",
        "        \n",
        "        # è®¡ç®—æ‰€æœ‰ç‚¹åˆ°ä¸­å¿ƒç‚¹çš„è·ç¦»\n",
        "        distances = np.sqrt(np.sum((points - center) ** 2, axis=1))\n",
        "        \n",
        "        # éšæœºé€‰æ‹©ä¸€ä¸ªåŠå¾„ï¼ˆ0.2åˆ°0.4ä¹‹é—´ï¼‰\n",
        "        radius = np.random.uniform(0.2, 0.4)\n",
        "        \n",
        "        # ç§»é™¤è¯¥åŠå¾„å†…çš„ç‚¹\n",
        "        mask = distances > radius\n",
        "        points = points[mask]\n",
        "        num_points = len(points)\n",
        "    \n",
        "    return points\n",
        "\n",
        "# æµ‹è¯•é¢„å¤„ç†å‡½æ•°\n",
        "if __name__ == \"__main__\":\n",
        "    # ç”Ÿæˆç¤ºä¾‹ç‚¹äº‘\n",
        "    num_points = 2000\n",
        "    theta = np.random.uniform(0, 2*np.pi, num_points)\n",
        "    phi = np.random.uniform(0, np.pi, num_points)\n",
        "    r = np.ones(num_points)\n",
        "    \n",
        "    x = r * np.sin(phi) * np.cos(theta)\n",
        "    y = r * np.sin(phi) * np.sin(theta)\n",
        "    z = r * np.cos(phi)\n",
        "    \n",
        "    points = np.stack([x, y, z], axis=1)\n",
        "    \n",
        "    # æµ‹è¯•å½’ä¸€åŒ–\n",
        "    normalized_points, centroid, scale = normalize_point_cloud(points)\n",
        "    print(f\"å½’ä¸€åŒ–åç‚¹äº‘çš„èŒƒå›´: [{np.min(normalized_points):.3f}, {np.max(normalized_points):.3f}]\")\n",
        "    \n",
        "    # æµ‹è¯•é‡‡æ ·\n",
        "    sampled_points = random_sample_points(normalized_points, 1000)\n",
        "    print(f\"é‡‡æ ·åç‚¹äº‘çš„å½¢çŠ¶: {sampled_points.shape}\")\n",
        "    \n",
        "    # æµ‹è¯•æ·»åŠ å™ªå£°\n",
        "    noisy_points = add_noise(sampled_points)\n",
        "    \n",
        "    # æµ‹è¯•åˆ›å»ºä¸å®Œæ•´ç‚¹äº‘\n",
        "    partial_points = create_partial_point_cloud(sampled_points)\n",
        "    print(f\"ä¸å®Œæ•´ç‚¹äº‘çš„ç‚¹æ•°: {len(partial_points)}\")\n",
        "    \n",
        "    # å¯è§†åŒ–ç»“æœ\n",
        "    print(\"\\næ˜¾ç¤ºåŸå§‹ç‚¹äº‘...\")\n",
        "    visualize_point_cloud(sampled_points)\n",
        "    \n",
        "    print(\"\\næ˜¾ç¤ºæ·»åŠ å™ªå£°åçš„ç‚¹äº‘...\")\n",
        "    visualize_point_cloud(noisy_points)\n",
        "    \n",
        "    print(\"\\næ˜¾ç¤ºä¸å®Œæ•´ç‚¹äº‘ä¸åŸå§‹ç‚¹äº‘çš„å¯¹æ¯”...\")\n",
        "    visualize_partial_complete(partial_points, sampled_points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ä¸‰ã€æ•°æ®é›†ä¸‹è½½"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PointNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PointNetFeatureExtractor, self).__init__()\n",
        "        \n",
        "        # ç‰¹å¾æå–å±‚\n",
        "        self.conv1 = nn.Conv1d(3, 64, 1)\n",
        "        self.conv2 = nn.Conv1d(64, 128, 1)\n",
        "        self.conv3 = nn.Conv1d(128, 256, 1)\n",
        "        \n",
        "        # æ‰¹å½’ä¸€åŒ–å±‚\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(256)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # è¾“å…¥xçš„å½¢çŠ¶: (batch_size, num_points, 3)\n",
        "        # è½¬æ¢ä¸º(batch_size, 3, num_points)ç”¨äº1Då·ç§¯\n",
        "        x = x.transpose(2, 1)\n",
        "        \n",
        "        # åº”ç”¨ç‰¹å¾æå–å±‚\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        \n",
        "        # å…¨å±€ç‰¹å¾\n",
        "        x_global = torch.max(x, 2, keepdim=True)[0]\n",
        "        \n",
        "        return x, x_global\n",
        "\n",
        "class PointCompletionNet(nn.Module):\n",
        "    def __init__(self, num_points=2048):\n",
        "        super(PointCompletionNet, self).__init__()\n",
        "        \n",
        "        self.num_points = num_points\n",
        "        \n",
        "        # ç‰¹å¾æå–å™¨\n",
        "        self.feature_extractor = PointNetFeatureExtractor()\n",
        "        \n",
        "        # è§£ç å™¨ - å…¨è¿æ¥å±‚\n",
        "        self.fc1 = nn.Linear(256, 512)\n",
        "        self.fc2 = nn.Linear(512, 1024)\n",
        "        self.fc3 = nn.Linear(1024, num_points * 3)\n",
        "        \n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.bn2 = nn.BatchNorm1d(1024)\n",
        "        \n",
        "        # Dropoutå±‚\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # è¾“å…¥xçš„å½¢çŠ¶: (batch_size, num_points, 3)\n",
        "        batch_size = x.size(0)\n",
        "        \n",
        "        # æå–ç‰¹å¾\n",
        "        point_features, global_features = self.feature_extractor(x)\n",
        "        \n",
        "        # å°†å…¨å±€ç‰¹å¾å±•å¹³\n",
        "        x = global_features.view(batch_size, -1)\n",
        "        \n",
        "        # è§£ç å™¨\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        # é‡å¡‘ä¸ºç‚¹äº‘å½¢çŠ¶\n",
        "        x = x.view(batch_size, self.num_points, 3)\n",
        "        \n",
        "        return x\n",
        "\n",
        "def chamfer_distance(pred, gt, reduce_mean=True):\n",
        "    \"\"\"\n",
        "    è®¡ç®—Chamferè·ç¦»\n",
        "    \n",
        "    Args:\n",
        "        pred: é¢„æµ‹çš„ç‚¹äº‘ï¼Œå½¢çŠ¶ä¸º(B, N, 3)\n",
        "        gt: çœŸå®çš„ç‚¹äº‘ï¼Œå½¢çŠ¶ä¸º(B, M, 3)\n",
        "        reduce_mean: æ˜¯å¦è¿”å›å¹³å‡å€¼\n",
        "    \n",
        "    Returns:\n",
        "        chamfer_dist: Chamferè·ç¦»\n",
        "    \"\"\"\n",
        "    # å°†ç‚¹äº‘è½¬æ¢ä¸º(B, N, 1, 3)å’Œ(B, 1, M, 3)\n",
        "    pred = pred.unsqueeze(2)\n",
        "    gt = gt.unsqueeze(1)\n",
        "    \n",
        "    # è®¡ç®—æ¯ä¸ªç‚¹åˆ°å¦ä¸€ä¸ªç‚¹äº‘ä¸­æ‰€æœ‰ç‚¹çš„è·ç¦»\n",
        "    dist = torch.sum((pred - gt) ** 2, dim=3)  # (B, N, M)\n",
        "    \n",
        "    # æ‰¾åˆ°æœ€è¿‘ç‚¹çš„è·ç¦»\n",
        "    dist1, _ = torch.min(dist, dim=2)  # (B, N)\n",
        "    dist2, _ = torch.min(dist, dim=1)  # (B, M)\n",
        "    \n",
        "    # è®¡ç®—Chamferè·ç¦»\n",
        "    chamfer_dist = torch.mean(dist1, dim=1) + torch.mean(dist2, dim=1)  # (B,)\n",
        "    \n",
        "    if reduce_mean:\n",
        "        chamfer_dist = torch.mean(chamfer_dist)\n",
        "    \n",
        "    return chamfer_dist\n",
        "\n",
        "# æµ‹è¯•æ¨¡å‹\n",
        "if __name__ == \"__main__\":\n",
        "    # åˆ›å»ºæ¨¡å‹å®ä¾‹\n",
        "    model = PointCompletionNet(num_points=2048)\n",
        "    model = model.to(device)\n",
        "    print(f\"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters())}\")\n",
        "    \n",
        "    # ç”Ÿæˆæµ‹è¯•æ•°æ®\n",
        "    batch_size = 2\n",
        "    input_points = torch.randn(batch_size, 1024, 3).to(device)\n",
        "    \n",
        "    # å‰å‘ä¼ æ’­\n",
        "    output_points = model(input_points)\n",
        "    print(f\"è¾“å‡ºç‚¹äº‘å½¢çŠ¶: {output_points.shape}\")\n",
        "    \n",
        "    # æµ‹è¯•Chamferè·ç¦»\n",
        "    target_points = torch.randn(batch_size, 2048, 3).to(device)\n",
        "    loss = chamfer_distance(output_points, target_points)\n",
        "    print(f\"Chamferè·ç¦»: {loss.item():.6f}\")\n",
        "    \n",
        "    # å¯è§†åŒ–ç»“æœ\n",
        "    if batch_size > 0:\n",
        "        input_np = input_points[0].cpu().numpy()\n",
        "        output_np = output_points[0].detach().cpu().numpy()\n",
        "        target_np = target_points[0].cpu().numpy()\n",
        "        \n",
        "        print(\"\\næ˜¾ç¤ºè¾“å…¥ç‚¹äº‘...\")\n",
        "        visualize_point_cloud(input_np)\n",
        "        \n",
        "        print(\"\\næ˜¾ç¤ºè¾“å‡ºç‚¹äº‘...\")\n",
        "        visualize_point_cloud(output_np)\n",
        "        \n",
        "        print(\"\\næ˜¾ç¤ºç›®æ ‡ç‚¹äº‘...\")\n",
        "        visualize_point_cloud(target_np)\n",
        "        \n",
        "        print(\"\\næ˜¾ç¤ºè¾“å…¥ä¸è¾“å‡ºç‚¹äº‘çš„å¯¹æ¯”...\")\n",
        "        visualize_partial_complete(input_np, output_np)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ•°æ®é›†ä¸‹è½½/è·¯å¾„æ£€æŸ¥ï¼ˆæœ¬åœ°å·²ä¸‹è½½åˆ™ç›´æ¥ä½¿ç”¨ï¼‰\n",
        "DATA_DIR = \"/Users/arkin/Desktop/Dev/notebooks/point-cloud-completion/shapenet_car\"\n",
        "print('DATA_DIR =', DATA_DIR)\n",
        "\n",
        "required_files = ['train-001.lmdb', 'valid.lmdb']\n",
        "missing = []\n",
        "if os.path.isdir(DATA_DIR):\n",
        "    for f in required_files:\n",
        "        if not os.path.exists(os.path.join(DATA_DIR, f)):\n",
        "            missing.append(f)\n",
        "else:\n",
        "    print('ç›®å½•ä¸å­˜åœ¨ï¼')\n",
        "    missing = required_files\n",
        "\n",
        "if missing:\n",
        "    print('ç¼ºå¤±æ–‡ä»¶:', missing)\n",
        "    print('å°†ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œæ¼”ç¤ºã€‚')\n",
        "else:\n",
        "    print('æ•°æ®è·¯å¾„å®Œæ•´ï¼Œå¯ç”¨äºè®­ç»ƒã€‚')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PointCloudDataset(Dataset):\n",
        "    def __init__(self, lmdb_path, num_points=2048, partial_points=1024, \n",
        "                 mode='train', transform=None):\n",
        "        \"\"\"\n",
        "        ç‚¹äº‘æ•°æ®é›†ç±»\n",
        "        \n",
        "        Args:\n",
        "            lmdb_path: LMDBæ•°æ®åº“è·¯å¾„\n",
        "            num_points: å®Œæ•´ç‚¹äº‘çš„ç‚¹æ•°\n",
        "            partial_points: ä¸å®Œæ•´ç‚¹äº‘çš„ç‚¹æ•°\n",
        "            mode: 'train' æˆ– 'valid'\n",
        "            transform: æ•°æ®å¢å¼ºå‡½æ•°\n",
        "        \"\"\"\n",
        "        super(PointCloudDataset, self).__init__()\n",
        "        \n",
        "        self.lmdb_path = lmdb_path\n",
        "        self.num_points = num_points\n",
        "        self.partial_points = partial_points\n",
        "        self.mode = mode\n",
        "        self.transform = transform\n",
        "        \n",
        "        # æ‰“å¼€LMDBç¯å¢ƒ\n",
        "        self.env = lmdb.open(lmdb_path, readonly=True, lock=False)\n",
        "        with self.env.begin() as txn:\n",
        "            self.length = int(txn.get('length'.encode()).decode())\n",
        "            \n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        with self.env.begin() as txn:\n",
        "            # è·å–å®Œæ•´ç‚¹äº‘æ•°æ®\n",
        "            key = f'complete_{index}'.encode()\n",
        "            complete = pickle.loads(txn.get(key))\n",
        "            \n",
        "            # è·å–ä¸å®Œæ•´ç‚¹äº‘æ•°æ®\n",
        "            key = f'partial_{index}'.encode()\n",
        "            partial = pickle.loads(txn.get(key))\n",
        "            \n",
        "            # è½¬æ¢ä¸ºnumpyæ•°ç»„\n",
        "            complete = np.array(complete, dtype=np.float32)\n",
        "            partial = np.array(partial, dtype=np.float32)\n",
        "            \n",
        "            # æ•°æ®é¢„å¤„ç†\n",
        "            # 1. å½’ä¸€åŒ–\n",
        "            complete, centroid, scale = normalize_point_cloud(complete)\n",
        "            partial = (partial - centroid) / scale\n",
        "            \n",
        "            # 2. éšæœºé‡‡æ ·\n",
        "            if complete.shape[0] > self.num_points:\n",
        "                complete = random_sample_points(complete, self.num_points)\n",
        "            if partial.shape[0] > self.partial_points:\n",
        "                partial = random_sample_points(partial, self.partial_points)\n",
        "            \n",
        "            # 3. æ•°æ®å¢å¼º\n",
        "            if self.transform and self.mode == 'train':\n",
        "                complete = self.transform(complete)\n",
        "                partial = self.transform(partial)\n",
        "            \n",
        "            # è½¬æ¢ä¸ºå¼ é‡\n",
        "            complete = torch.from_numpy(complete)\n",
        "            partial = torch.from_numpy(partial)\n",
        "            \n",
        "            return {'partial': partial, 'complete': complete}\n",
        "\n",
        "class PointCloudTransform:\n",
        "    def __init__(self, noise_sigma=0.01, noise_clip=0.05, \n",
        "                 rotation=True, translation=True, scale=True):\n",
        "        \"\"\"\n",
        "        ç‚¹äº‘æ•°æ®å¢å¼ºç±»\n",
        "        \n",
        "        Args:\n",
        "            noise_sigma: é«˜æ–¯å™ªå£°çš„æ ‡å‡†å·®\n",
        "            noise_clip: å™ªå£°çš„æœ€å¤§å€¼\n",
        "            rotation: æ˜¯å¦è¿›è¡Œæ—‹è½¬\n",
        "            translation: æ˜¯å¦è¿›è¡Œå¹³ç§»\n",
        "            scale: æ˜¯å¦è¿›è¡Œç¼©æ”¾\n",
        "        \"\"\"\n",
        "        self.noise_sigma = noise_sigma\n",
        "        self.noise_clip = noise_clip\n",
        "        self.rotation = rotation\n",
        "        self.translation = translation\n",
        "        self.scale = scale\n",
        "    \n",
        "    def __call__(self, points):\n",
        "        \"\"\"\n",
        "        å¯¹ç‚¹äº‘è¿›è¡Œæ•°æ®å¢å¼º\n",
        "        \n",
        "        Args:\n",
        "            points: numpyæ•°ç»„ï¼Œå½¢çŠ¶ä¸º(N, 3)\n",
        "            \n",
        "        Returns:\n",
        "            transformed_points: å¢å¼ºåçš„ç‚¹äº‘\n",
        "        \"\"\"\n",
        "        points = points.copy()\n",
        "        \n",
        "        # 1. æ·»åŠ é«˜æ–¯å™ªå£°\n",
        "        points = add_noise(points, self.noise_sigma, self.noise_clip)\n",
        "        \n",
        "        # 2. éšæœºæ—‹è½¬\n",
        "        if self.rotation:\n",
        "            # ç”Ÿæˆéšæœºæ—‹è½¬è§’åº¦\n",
        "            theta = np.random.uniform(0, 2*np.pi)\n",
        "            # ç»•zè½´æ—‹è½¬çš„æ—‹è½¬çŸ©é˜µ\n",
        "            rotation_matrix = np.array([\n",
        "                [np.cos(theta), -np.sin(theta), 0],\n",
        "                [np.sin(theta), np.cos(theta), 0],\n",
        "                [0, 0, 1]\n",
        "            ])\n",
        "            points = points @ rotation_matrix\n",
        "        \n",
        "        # 3. éšæœºå¹³ç§»\n",
        "        if self.translation:\n",
        "            translation = np.random.uniform(-0.1, 0.1, size=3)\n",
        "            points += translation\n",
        "        \n",
        "        # 4. éšæœºç¼©æ”¾\n",
        "        if self.scale:\n",
        "            scale = np.random.uniform(0.8, 1.2)\n",
        "            points *= scale\n",
        "        \n",
        "        return points.astype(np.float32)\n",
        "\n",
        "# åˆ›å»ºæ•°æ®åŠ è½½å™¨\n",
        "def create_dataloader(lmdb_path, batch_size=32, num_workers=4, \n",
        "                     num_points=2048, partial_points=1024, mode='train'):\n",
        "    \"\"\"\n",
        "    åˆ›å»ºæ•°æ®åŠ è½½å™¨\n",
        "    \n",
        "    Args:\n",
        "        lmdb_path: LMDBæ•°æ®åº“è·¯å¾„\n",
        "        batch_size: æ‰¹æ¬¡å¤§å°\n",
        "        num_workers: æ•°æ®åŠ è½½çš„è¿›ç¨‹æ•°\n",
        "        num_points: å®Œæ•´ç‚¹äº‘çš„ç‚¹æ•°\n",
        "        partial_points: ä¸å®Œæ•´ç‚¹äº‘çš„ç‚¹æ•°\n",
        "        mode: 'train' æˆ– 'valid'\n",
        "        \n",
        "    Returns:\n",
        "        dataloader: æ•°æ®åŠ è½½å™¨\n",
        "    \"\"\"\n",
        "    # åˆ›å»ºæ•°æ®å¢å¼º\n",
        "    transform = None\n",
        "    if mode == 'train':\n",
        "        transform = PointCloudTransform(\n",
        "            noise_sigma=0.01,\n",
        "            noise_clip=0.05,\n",
        "            rotation=True,\n",
        "            translation=True,\n",
        "            scale=True\n",
        "        )\n",
        "    \n",
        "    # åˆ›å»ºæ•°æ®é›†\n",
        "    dataset = PointCloudDataset(\n",
        "        lmdb_path=lmdb_path,\n",
        "        num_points=num_points,\n",
        "        partial_points=partial_points,\n",
        "        mode=mode,\n",
        "        transform=transform\n",
        "    )\n",
        "    \n",
        "    # åˆ›å»ºæ•°æ®åŠ è½½å™¨\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=(mode == 'train'),\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "    \n",
        "    return dataloader\n",
        "\n",
        "# æµ‹è¯•æ•°æ®åŠ è½½å™¨\n",
        "if __name__ == \"__main__\":\n",
        "    # æ•°æ®è·¯å¾„\n",
        "    train_path = \"/Users/arkin/Desktop/Dev/notebooks/point-cloud-completion/shapenet_car/train-001.lmdb\"\n",
        "    valid_path = \"/Users/arkin/Desktop/Dev/notebooks/point-cloud-completion/shapenet_car/valid.lmdb\"\n",
        "    \n",
        "    # åˆ›å»ºæ•°æ®åŠ è½½å™¨\n",
        "    train_loader = create_dataloader(train_path, mode='train')\n",
        "    valid_loader = create_dataloader(valid_path, mode='valid')\n",
        "    \n",
        "    print(f\"è®­ç»ƒæ•°æ®åŠ è½½å™¨å¤§å°: {len(train_loader)}\")\n",
        "    print(f\"éªŒè¯æ•°æ®åŠ è½½å™¨å¤§å°: {len(valid_loader)}\")\n",
        "    \n",
        "    # è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®\n",
        "    batch = next(iter(train_loader))\n",
        "    partial = batch['partial']\n",
        "    complete = batch['complete']\n",
        "    \n",
        "    print(f\"ä¸å®Œæ•´ç‚¹äº‘å½¢çŠ¶: {partial.shape}\")\n",
        "    print(f\"å®Œæ•´ç‚¹äº‘å½¢çŠ¶: {complete.shape}\")\n",
        "    \n",
        "    # å¯è§†åŒ–ç¬¬ä¸€ä¸ªæ ·æœ¬\n",
        "    partial_np = partial[0].numpy()\n",
        "    complete_np = complete[0].numpy()\n",
        "    \n",
        "    print(\"\\næ˜¾ç¤ºä¸å®Œæ•´ç‚¹äº‘ä¸å®Œæ•´ç‚¹äº‘çš„å¯¹æ¯”...\")\n",
        "    visualize_partial_complete(partial_np, complete_np)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## å››ã€æ•°æ®å¯è§†åŒ–å±•ç¤º"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, valid_loader, num_epochs=100, \n",
        "              learning_rate=0.001, device=device):\n",
        "    \"\"\"\n",
        "    è®­ç»ƒæ¨¡å‹\n",
        "    \n",
        "    Args:\n",
        "        model: æ¨¡å‹å®ä¾‹\n",
        "        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨\n",
        "        valid_loader: éªŒè¯æ•°æ®åŠ è½½å™¨\n",
        "        num_epochs: è®­ç»ƒè½®æ•°\n",
        "        learning_rate: å­¦ä¹ ç‡\n",
        "        device: è®­ç»ƒè®¾å¤‡\n",
        "    \"\"\"\n",
        "    # ä¼˜åŒ–å™¨\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    # å­¦ä¹ ç‡è°ƒåº¦å™¨\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "    \n",
        "    # è®°å½•æœ€ä½³æ¨¡å‹\n",
        "    best_valid_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    \n",
        "    # è®­ç»ƒå†å²\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'valid_loss': [],\n",
        "        'lr': []\n",
        "    }\n",
        "    \n",
        "    # è®­ç»ƒå¾ªç¯\n",
        "    for epoch in range(num_epochs):\n",
        "        # è®­ç»ƒé˜¶æ®µ\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        train_progress = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
        "        \n",
        "        for batch in train_progress:\n",
        "            # å‡†å¤‡æ•°æ®\n",
        "            partial = batch['partial'].to(device)\n",
        "            complete = batch['complete'].to(device)\n",
        "            \n",
        "            # å‰å‘ä¼ æ’­\n",
        "            output = model(partial)\n",
        "            loss = chamfer_distance(output, complete)\n",
        "            \n",
        "            # åå‘ä¼ æ’­\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # è®°å½•æŸå¤±\n",
        "            train_losses.append(loss.item())\n",
        "            train_progress.set_postfix({'loss': f'{loss.item():.6f}'})\n",
        "        \n",
        "        # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±\n",
        "        avg_train_loss = np.mean(train_losses)\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        \n",
        "        # éªŒè¯é˜¶æ®µ\n",
        "        model.eval()\n",
        "        valid_losses = []\n",
        "        valid_progress = tqdm(valid_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Valid]')\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch in valid_progress:\n",
        "                # å‡†å¤‡æ•°æ®\n",
        "                partial = batch['partial'].to(device)\n",
        "                complete = batch['complete'].to(device)\n",
        "                \n",
        "                # å‰å‘ä¼ æ’­\n",
        "                output = model(partial)\n",
        "                loss = chamfer_distance(output, complete)\n",
        "                \n",
        "                # è®°å½•æŸå¤±\n",
        "                valid_losses.append(loss.item())\n",
        "                valid_progress.set_postfix({'loss': f'{loss.item():.6f}'})\n",
        "        \n",
        "        # è®¡ç®—å¹³å‡éªŒè¯æŸå¤±\n",
        "        avg_valid_loss = np.mean(valid_losses)\n",
        "        history['valid_loss'].append(avg_valid_loss)\n",
        "        \n",
        "        # æ›´æ–°å­¦ä¹ ç‡\n",
        "        scheduler.step()\n",
        "        current_lr = scheduler.get_last_lr()[0]\n",
        "        history['lr'].append(current_lr)\n",
        "        \n",
        "        # æ‰“å°è®­ç»ƒä¿¡æ¯\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'Train Loss: {avg_train_loss:.6f}')\n",
        "        print(f'Valid Loss: {avg_valid_loss:.6f}')\n",
        "        print(f'Learning Rate: {current_lr:.6f}\\n')\n",
        "        \n",
        "        # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
        "        if avg_valid_loss < best_valid_loss:\n",
        "            best_valid_loss = avg_valid_loss\n",
        "            best_model_state = model.state_dict()\n",
        "            print(f'Found new best model with validation loss: {best_valid_loss:.6f}')\n",
        "            \n",
        "            # ä¿å­˜æ¨¡å‹\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': best_model_state,\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'train_loss': avg_train_loss,\n",
        "                'valid_loss': best_valid_loss,\n",
        "                'history': history\n",
        "            }, 'best_model.pth')\n",
        "    \n",
        "    return history, best_model_state\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"\n",
        "    ç»˜åˆ¶è®­ç»ƒå†å²\n",
        "    \n",
        "    Args:\n",
        "        history: åŒ…å«è®­ç»ƒå†å²çš„å­—å…¸\n",
        "    \"\"\"\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    \n",
        "    # åˆ›å»ºå›¾å½¢\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    \n",
        "    # ç»˜åˆ¶æŸå¤±æ›²çº¿\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, history['train_loss'], 'b-', label='Train Loss')\n",
        "    plt.plot(epochs, history['valid_loss'], 'r-', label='Valid Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    \n",
        "    # ç»˜åˆ¶å­¦ä¹ ç‡æ›²çº¿\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, history['lr'], 'g-')\n",
        "    plt.title('Learning Rate')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# è®­ç»ƒæ¨¡å‹\n",
        "if __name__ == \"__main__\":\n",
        "    # åˆ›å»ºæ•°æ®åŠ è½½å™¨\n",
        "    train_path = \"/Users/arkin/Desktop/Dev/notebooks/point-cloud-completion/shapenet_car/train-001.lmdb\"\n",
        "    valid_path = \"/Users/arkin/Desktop/Dev/notebooks/point-cloud-completion/shapenet_car/valid.lmdb\"\n",
        "    \n",
        "    train_loader = create_dataloader(train_path, batch_size=32, mode='train')\n",
        "    valid_loader = create_dataloader(valid_path, batch_size=32, mode='valid')\n",
        "    \n",
        "    # åˆ›å»ºæ¨¡å‹\n",
        "    model = PointCompletionNet(num_points=2048)\n",
        "    model = model.to(device)\n",
        "    \n",
        "    # è®­ç»ƒæ¨¡å‹\n",
        "    history, best_model_state = train_model(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        valid_loader=valid_loader,\n",
        "        num_epochs=100,\n",
        "        learning_rate=0.001,\n",
        "        device=device\n",
        "    )\n",
        "    \n",
        "    # ç»˜åˆ¶è®­ç»ƒå†å²\n",
        "    plot_training_history(history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, device=device, num_visualize=5):\n",
        "    \"\"\"\n",
        "    è¯„ä¼°æ¨¡å‹å¹¶å¯è§†åŒ–ç»“æœ\n",
        "    \n",
        "    Args:\n",
        "        model: è®­ç»ƒå¥½çš„æ¨¡å‹\n",
        "        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨\n",
        "        device: è¿è¡Œè®¾å¤‡\n",
        "        num_visualize: è¦å¯è§†åŒ–çš„æ ·æœ¬æ•°é‡\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    test_losses = []\n",
        "    visualize_samples = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(tqdm(test_loader, desc='Evaluating')):\n",
        "            # å‡†å¤‡æ•°æ®\n",
        "            partial = batch['partial'].to(device)\n",
        "            complete = batch['complete'].to(device)\n",
        "            \n",
        "            # å‰å‘ä¼ æ’­\n",
        "            output = model(partial)\n",
        "            loss = chamfer_distance(output, complete)\n",
        "            \n",
        "            # è®°å½•æŸå¤±\n",
        "            test_losses.append(loss.item())\n",
        "            \n",
        "            # æ”¶é›†å¯è§†åŒ–æ ·æœ¬\n",
        "            if i < num_visualize:\n",
        "                visualize_samples.append({\n",
        "                    'partial': partial[0].cpu().numpy(),\n",
        "                    'complete': complete[0].cpu().numpy(),\n",
        "                    'output': output[0].cpu().numpy()\n",
        "                })\n",
        "    \n",
        "    # è®¡ç®—å¹³å‡æµ‹è¯•æŸå¤±\n",
        "    avg_test_loss = np.mean(test_losses)\n",
        "    print(f'\\nAverage Test Loss: {avg_test_loss:.6f}')\n",
        "    \n",
        "    # å¯è§†åŒ–ç»“æœ\n",
        "    for i, sample in enumerate(visualize_samples):\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        \n",
        "        # åˆ›å»ºä¸‰ä¸ªå­å›¾\n",
        "        ax1 = plt.subplot(131, projection='3d')\n",
        "        ax2 = plt.subplot(132, projection='3d')\n",
        "        ax3 = plt.subplot(133, projection='3d')\n",
        "        \n",
        "        # ç»˜åˆ¶ä¸å®Œæ•´ç‚¹äº‘\n",
        "        partial = sample['partial']\n",
        "        ax1.scatter(partial[:, 0], partial[:, 1], partial[:, 2], c='r', marker='.')\n",
        "        ax1.set_title('Partial Point Cloud')\n",
        "        \n",
        "        # ç»˜åˆ¶å®Œæ•´ç‚¹äº‘\n",
        "        complete = sample['complete']\n",
        "        ax2.scatter(complete[:, 0], complete[:, 1], complete[:, 2], c='g', marker='.')\n",
        "        ax2.set_title('Ground Truth')\n",
        "        \n",
        "        # ç»˜åˆ¶é¢„æµ‹ç‚¹äº‘\n",
        "        output = sample['output']\n",
        "        ax3.scatter(output[:, 0], output[:, 1], output[:, 2], c='b', marker='.')\n",
        "        ax3.set_title('Model Output')\n",
        "        \n",
        "        # è®¾ç½®è§†è§’\n",
        "        for ax in [ax1, ax2, ax3]:\n",
        "            ax.view_init(elev=30, azim=45)\n",
        "            ax.set_xlabel('X')\n",
        "            ax.set_ylabel('Y')\n",
        "            ax.set_zlabel('Z')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # ä½¿ç”¨Open3Dè¿›è¡Œäº¤äº’å¼å¯è§†åŒ–\n",
        "        print(f'\\nSample {i+1} - Interactive Visualization:')\n",
        "        print('æ˜¾ç¤ºä¸å®Œæ•´ç‚¹äº‘ä¸é¢„æµ‹ç‚¹äº‘çš„å¯¹æ¯”...')\n",
        "        visualize_partial_complete(partial, output)\n",
        "        \n",
        "        print('æ˜¾ç¤ºé¢„æµ‹ç‚¹äº‘ä¸çœŸå®ç‚¹äº‘çš„å¯¹æ¯”...')\n",
        "        visualize_partial_complete(output, complete)\n",
        "\n",
        "def compute_metrics(model, test_loader, device=device):\n",
        "    \"\"\"\n",
        "    è®¡ç®—è¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡\n",
        "    \n",
        "    Args:\n",
        "        model: è®­ç»ƒå¥½çš„æ¨¡å‹\n",
        "        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨\n",
        "        device: è¿è¡Œè®¾å¤‡\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    chamfer_distances = []\n",
        "    point_nums = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc='Computing Metrics'):\n",
        "            partial = batch['partial'].to(device)\n",
        "            complete = batch['complete'].to(device)\n",
        "            output = model(partial)\n",
        "            \n",
        "            # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„Chamferè·ç¦»\n",
        "            cd = chamfer_distance(output, complete, reduce_mean=False)\n",
        "            chamfer_distances.extend(cd.cpu().numpy())\n",
        "            \n",
        "            # è®°å½•ç‚¹çš„æ•°é‡\n",
        "            point_nums.extend([\n",
        "                partial.shape[1],  # è¾“å…¥ç‚¹æ•°\n",
        "                complete.shape[1],  # ç›®æ ‡ç‚¹æ•°\n",
        "                output.shape[1]    # è¾“å‡ºç‚¹æ•°\n",
        "            ])\n",
        "    \n",
        "    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯\n",
        "    cd_mean = np.mean(chamfer_distances)\n",
        "    cd_std = np.std(chamfer_distances)\n",
        "    cd_median = np.median(chamfer_distances)\n",
        "    cd_min = np.min(chamfer_distances)\n",
        "    cd_max = np.max(chamfer_distances)\n",
        "    \n",
        "    # æ‰“å°è¯„ä¼°ç»“æœ\n",
        "    print('\\nEvaluation Metrics:')\n",
        "    print(f'Chamfer Distance:')\n",
        "    print(f'  Mean   : {cd_mean:.6f}')\n",
        "    print(f'  Std    : {cd_std:.6f}')\n",
        "    print(f'  Median : {cd_median:.6f}')\n",
        "    print(f'  Min    : {cd_min:.6f}')\n",
        "    print(f'  Max    : {cd_max:.6f}')\n",
        "    \n",
        "    # ç»˜åˆ¶Chamferè·ç¦»åˆ†å¸ƒ\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.hist(chamfer_distances, bins=50, density=True)\n",
        "    plt.axvline(cd_mean, color='r', linestyle='--', label=f'Mean: {cd_mean:.6f}')\n",
        "    plt.axvline(cd_median, color='g', linestyle='--', label=f'Median: {cd_median:.6f}')\n",
        "    plt.title('Distribution of Chamfer Distances')\n",
        "    plt.xlabel('Chamfer Distance')\n",
        "    plt.ylabel('Density')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# è¯„ä¼°æ¨¡å‹\n",
        "if __name__ == \"__main__\":\n",
        "    # åŠ è½½æœ€ä½³æ¨¡å‹\n",
        "    checkpoint = torch.load('best_model.pth')\n",
        "    model = PointCompletionNet(num_points=2048)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model = model.to(device)\n",
        "    \n",
        "    # åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨\n",
        "    test_path = \"/Users/arkin/Desktop/Dev/notebooks/point-cloud-completion/shapenet_car/valid.lmdb\"\n",
        "    test_loader = create_dataloader(test_path, batch_size=32, mode='valid')\n",
        "    \n",
        "    # è¯„ä¼°æ¨¡å‹\n",
        "    print(\"æ­£åœ¨è¯„ä¼°æ¨¡å‹...\")\n",
        "    evaluate_model(model, test_loader, num_visualize=5)\n",
        "    \n",
        "    # è®¡ç®—è¯¦ç»†æŒ‡æ ‡\n",
        "    print(\"\\nè®¡ç®—è¯¦ç»†è¯„ä¼°æŒ‡æ ‡...\")\n",
        "    compute_metrics(model, test_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## äº”ã€æ•°æ®é¢„å¤„ç†"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## å…­ã€ShapeNetæ¨¡å‹æ¶æ„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ä¸ƒã€æ•°æ®é›†ç±»ä¸æ•°æ®åŠ è½½å™¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## å…«ã€æ¨¡å‹è®­ç»ƒ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ä¹ã€æ¨¡å‹è¯„ä¼°ä¸å¯è§†åŒ–"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
