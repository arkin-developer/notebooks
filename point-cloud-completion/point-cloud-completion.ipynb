{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🎯 3D点云补全案例 - ShapeNet网络 + PCN数据集\n",
        "\n",
        "这是一个简单的点云补全演示项目，展示了如何使用深度学习技术从不完整的点云数据中重建完整的3D形状。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 一、3D点云补全任务简介\n",
        "\n",
        "3D点云补全（Point Cloud Completion）致力于在传感器采集存在遮挡、稀疏和噪声的现实条件下，重建目标或场景的完整几何。它位于“3D感知”技术栈中，承上游采集/重建（LiDAR、RGB‑D、SfM）之缺，启下游理解/交互（识别、定位、交互、仿真）之用。\n",
        "\n",
        "- 定位（在3D感知流水线中的角色）\n",
        "  - 上游：传感器采样不可避免的视角遮挡、距离衰减、材质反射导致点云缺失与不均匀。\n",
        "  - 中游：补全模块依据先验（形状、结构、拓扑）推断缺失区域，输出致密、连贯的点集或隐式几何。\n",
        "  - 下游：为检测/分割/姿态估计、配准/建图、物理仿真与渲染提供更完整、更鲁棒的输入。\n",
        "\n",
        "- 作用（为什么需要补全）\n",
        "  - 抗遮挡与抗稀疏：恢复关键结构，提高下游任务鲁棒性与精度。\n",
        "  - 统一密度与尺度：为基于点的网络与NerF/隐式表面方法提供更稳定的几何表示。\n",
        "  - 降噪与可视化：补齐轮廓、减少孔洞，便于测量、展示与制造。\n",
        "\n",
        "- 典型应用场景\n",
        "  - 自动驾驶/机器人：单帧或多帧LiDAR补全，提升目标轮廓与距离估计；机械臂抓取中的形状推断。\n",
        "  - AR/VR与数字孪生：室内/建筑扫描补洞，生成完整的资产用于渲染与交互。\n",
        "  - 逆向工程/工业检测：部件扫描补全用于尺寸测量、误差对比与后续CAD重建。\n",
        "  - 文化遗产/文物修复：受限视角采集下的结构补全与还原。\n",
        "\n",
        "- 数据与形式\n",
        "  - 输入：不完整点云（partial），可来自LiDAR、RGB‑D或从CAD网格投影采样。\n",
        "  - 输出：完整点云（complete）、表面网格或隐式场（如SDF/Occ/NerF）。\n",
        "  - 监督：成对的 partial/complete（合成管线常用）或弱/自监督（真实数据）。\n",
        "\n",
        "- 评测指标与目标\n",
        "  - 几何一致性：Chamfer Distance（L1/L2）、EMD、F‑score@τ。\n",
        "  - 结构合理性：法向一致、曲率连续、体素/网格重建质量。\n",
        "  - 下游收益：在检测/分割/抓取等任务中的性能增益。\n",
        "\n",
        "- 方法概览（简）\n",
        "  - 基于先验的显式补洞（模板/对称性/检索）与深度学习式隐式/显式重建。\n",
        "  - 全局先验（编码器‑解码器、扩散/自回归）+ 局部细化（patch/上采样/细节增强）。\n",
        "\n",
        "本Notebook聚焦最小可行Demo：以简化PCN范式完成从partial到complete的端到端训练与可视化，便于快速理解补全任务的定位与价值。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 安装必要的依赖包\n",
        "%pip install torch numpy open3d matplotlib tqdm lmdb\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import open3d as o3d\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import lmdb\n",
        "import pickle\n",
        "\n",
        "# 设置随机种子以确保可重复性\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# 检查CUDA是否可用\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 二、环境配置与依赖安装"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_point_cloud(points, colors=None, window_name=\"Point Cloud\"):\n",
        "    \"\"\"\n",
        "    使用Open3D可视化点云数据\n",
        "    \n",
        "    Args:\n",
        "        points: numpy数组，形状为(N, 3)，表示点云坐标\n",
        "        colors: numpy数组，形状为(N, 3)，表示点云颜色，范围[0,1]，默认为None\n",
        "        window_name: 窗口名称\n",
        "    \"\"\"\n",
        "    pcd = o3d.geometry.PointCloud()\n",
        "    pcd.points = o3d.utility.Vector3dVector(points)\n",
        "    \n",
        "    if colors is not None:\n",
        "        pcd.colors = o3d.utility.Vector3dVector(colors)\n",
        "    else:\n",
        "        # 默认使用蓝色\n",
        "        pcd.paint_uniform_color([0, 0.651, 0.929])\n",
        "    \n",
        "    # 创建坐标系\n",
        "    coordinate_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(\n",
        "        size=0.5, origin=[0, 0, 0])\n",
        "    \n",
        "    # 可视化点云\n",
        "    o3d.visualization.draw_geometries([pcd, coordinate_frame],\n",
        "                                    window_name=window_name,\n",
        "                                    width=800,\n",
        "                                    height=600)\n",
        "\n",
        "def visualize_partial_complete(partial, complete, window_name=\"Partial vs Complete\"):\n",
        "    \"\"\"\n",
        "    并排显示不完整和完整的点云\n",
        "    \n",
        "    Args:\n",
        "        partial: numpy数组，形状为(N, 3)，表示不完整点云\n",
        "        complete: numpy数组，形状为(M, 3)，表示完整点云\n",
        "        window_name: 窗口名称\n",
        "    \"\"\"\n",
        "    # 创建不完整点云对象\n",
        "    pcd_partial = o3d.geometry.PointCloud()\n",
        "    pcd_partial.points = o3d.utility.Vector3dVector(partial)\n",
        "    pcd_partial.paint_uniform_color([1, 0, 0])  # 红色表示不完整点云\n",
        "    \n",
        "    # 创建完整点云对象\n",
        "    pcd_complete = o3d.geometry.PointCloud()\n",
        "    pcd_complete.points = o3d.utility.Vector3dVector(complete)\n",
        "    pcd_complete.paint_uniform_color([0, 1, 0])  # 绿色表示完整点云\n",
        "    \n",
        "    # 将完整点云向右平移，以便并排显示\n",
        "    center_complete = pcd_complete.get_center()\n",
        "    center_partial = pcd_partial.get_center()\n",
        "    translation = np.array([2.0, 0, 0])  # 向x轴正方向平移2个单位\n",
        "    pcd_complete.translate(translation)\n",
        "    \n",
        "    # 创建坐标系\n",
        "    coordinate_frame1 = o3d.geometry.TriangleMesh.create_coordinate_frame(\n",
        "        size=0.5, origin=[0, 0, 0])\n",
        "    coordinate_frame2 = o3d.geometry.TriangleMesh.create_coordinate_frame(\n",
        "        size=0.5, origin=translation)\n",
        "    \n",
        "    # 可视化点云\n",
        "    o3d.visualization.draw_geometries([pcd_partial, pcd_complete, \n",
        "                                     coordinate_frame1, coordinate_frame2],\n",
        "                                    window_name=window_name,\n",
        "                                    width=1600,\n",
        "                                    height=600)\n",
        "\n",
        "# 测试可视化函数\n",
        "if __name__ == \"__main__\":\n",
        "    # 生成示例点云数据\n",
        "    num_points = 1000\n",
        "    \n",
        "    # 生成一个球体的点云\n",
        "    theta = np.random.uniform(0, 2*np.pi, num_points)\n",
        "    phi = np.random.uniform(0, np.pi, num_points)\n",
        "    r = np.ones(num_points)\n",
        "    \n",
        "    x = r * np.sin(phi) * np.cos(theta)\n",
        "    y = r * np.sin(phi) * np.sin(theta)\n",
        "    z = r * np.cos(phi)\n",
        "    \n",
        "    complete_cloud = np.stack([x, y, z], axis=1)\n",
        "    \n",
        "    # 生成不完整的点云（只保留上半部分）\n",
        "    partial_cloud = complete_cloud[complete_cloud[:, 2] > 0]\n",
        "    \n",
        "    # 可视化\n",
        "    print(\"显示完整点云...\")\n",
        "    visualize_point_cloud(complete_cloud)\n",
        "    \n",
        "    print(\"显示不完整点云...\")\n",
        "    visualize_point_cloud(partial_cloud)\n",
        "    \n",
        "    print(\"并排显示对比...\")\n",
        "    visualize_partial_complete(partial_cloud, complete_cloud)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_point_cloud(points):\n",
        "    \"\"\"\n",
        "    对点云进行归一化处理\n",
        "    \n",
        "    Args:\n",
        "        points: numpy数组，形状为(N, 3)\n",
        "        \n",
        "    Returns:\n",
        "        normalized_points: 归一化后的点云\n",
        "        centroid: 质心\n",
        "        scale: 缩放因子\n",
        "    \"\"\"\n",
        "    # 计算质心\n",
        "    centroid = np.mean(points, axis=0)\n",
        "    \n",
        "    # 将点云中心移到原点\n",
        "    points = points - centroid\n",
        "    \n",
        "    # 计算到原点的最大距离\n",
        "    distances = np.sqrt(np.sum(points ** 2, axis=1))\n",
        "    scale = np.max(distances)\n",
        "    \n",
        "    # 归一化到单位球内\n",
        "    normalized_points = points / scale\n",
        "    \n",
        "    return normalized_points, centroid, scale\n",
        "\n",
        "def random_sample_points(points, num_points):\n",
        "    \"\"\"\n",
        "    随机采样固定数量的点\n",
        "    \n",
        "    Args:\n",
        "        points: numpy数组，形状为(N, 3)\n",
        "        num_points: 需要采样的点数\n",
        "        \n",
        "    Returns:\n",
        "        sampled_points: 采样后的点云\n",
        "    \"\"\"\n",
        "    if len(points) >= num_points:\n",
        "        # 随机采样\n",
        "        indices = np.random.choice(len(points), num_points, replace=False)\n",
        "        return points[indices]\n",
        "    else:\n",
        "        # 如果点数不足，则需要重复采样\n",
        "        indices = np.random.choice(len(points), num_points, replace=True)\n",
        "        return points[indices]\n",
        "\n",
        "def add_noise(points, sigma=0.01, clip=0.05):\n",
        "    \"\"\"\n",
        "    添加高斯噪声\n",
        "    \n",
        "    Args:\n",
        "        points: numpy数组，形状为(N, 3)\n",
        "        sigma: 高斯噪声的标准差\n",
        "        clip: 噪声的最大值\n",
        "        \n",
        "    Returns:\n",
        "        noisy_points: 添加噪声后的点云\n",
        "    \"\"\"\n",
        "    noise = np.clip(np.random.normal(0, sigma, points.shape), -clip, clip)\n",
        "    return points + noise\n",
        "\n",
        "def create_partial_point_cloud(points, num_patches=1):\n",
        "    \"\"\"\n",
        "    通过随机移除部分区域来创建不完整点云\n",
        "    \n",
        "    Args:\n",
        "        points: numpy数组，形状为(N, 3)\n",
        "        num_patches: 要移除的区域数量\n",
        "        \n",
        "    Returns:\n",
        "        partial_points: 不完整的点云\n",
        "    \"\"\"\n",
        "    points = points.copy()\n",
        "    num_points = len(points)\n",
        "    \n",
        "    for _ in range(num_patches):\n",
        "        # 随机选择一个中心点\n",
        "        center_idx = np.random.randint(0, num_points)\n",
        "        center = points[center_idx]\n",
        "        \n",
        "        # 计算所有点到中心点的距离\n",
        "        distances = np.sqrt(np.sum((points - center) ** 2, axis=1))\n",
        "        \n",
        "        # 随机选择一个半径（0.2到0.4之间）\n",
        "        radius = np.random.uniform(0.2, 0.4)\n",
        "        \n",
        "        # 移除该半径内的点\n",
        "        mask = distances > radius\n",
        "        points = points[mask]\n",
        "        num_points = len(points)\n",
        "    \n",
        "    return points\n",
        "\n",
        "# 测试预处理函数\n",
        "if __name__ == \"__main__\":\n",
        "    # 生成示例点云\n",
        "    num_points = 2000\n",
        "    theta = np.random.uniform(0, 2*np.pi, num_points)\n",
        "    phi = np.random.uniform(0, np.pi, num_points)\n",
        "    r = np.ones(num_points)\n",
        "    \n",
        "    x = r * np.sin(phi) * np.cos(theta)\n",
        "    y = r * np.sin(phi) * np.sin(theta)\n",
        "    z = r * np.cos(phi)\n",
        "    \n",
        "    points = np.stack([x, y, z], axis=1)\n",
        "    \n",
        "    # 测试归一化\n",
        "    normalized_points, centroid, scale = normalize_point_cloud(points)\n",
        "    print(f\"归一化后点云的范围: [{np.min(normalized_points):.3f}, {np.max(normalized_points):.3f}]\")\n",
        "    \n",
        "    # 测试采样\n",
        "    sampled_points = random_sample_points(normalized_points, 1000)\n",
        "    print(f\"采样后点云的形状: {sampled_points.shape}\")\n",
        "    \n",
        "    # 测试添加噪声\n",
        "    noisy_points = add_noise(sampled_points)\n",
        "    \n",
        "    # 测试创建不完整点云\n",
        "    partial_points = create_partial_point_cloud(sampled_points)\n",
        "    print(f\"不完整点云的点数: {len(partial_points)}\")\n",
        "    \n",
        "    # 可视化结果\n",
        "    print(\"\\n显示原始点云...\")\n",
        "    visualize_point_cloud(sampled_points)\n",
        "    \n",
        "    print(\"\\n显示添加噪声后的点云...\")\n",
        "    visualize_point_cloud(noisy_points)\n",
        "    \n",
        "    print(\"\\n显示不完整点云与原始点云的对比...\")\n",
        "    visualize_partial_complete(partial_points, sampled_points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 三、数据集下载"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PointNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PointNetFeatureExtractor, self).__init__()\n",
        "        \n",
        "        # 特征提取层\n",
        "        self.conv1 = nn.Conv1d(3, 64, 1)\n",
        "        self.conv2 = nn.Conv1d(64, 128, 1)\n",
        "        self.conv3 = nn.Conv1d(128, 256, 1)\n",
        "        \n",
        "        # 批归一化层\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(256)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # 输入x的形状: (batch_size, num_points, 3)\n",
        "        # 转换为(batch_size, 3, num_points)用于1D卷积\n",
        "        x = x.transpose(2, 1)\n",
        "        \n",
        "        # 应用特征提取层\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        \n",
        "        # 全局特征\n",
        "        x_global = torch.max(x, 2, keepdim=True)[0]\n",
        "        \n",
        "        return x, x_global\n",
        "\n",
        "class PointCompletionNet(nn.Module):\n",
        "    def __init__(self, num_points=2048):\n",
        "        super(PointCompletionNet, self).__init__()\n",
        "        \n",
        "        self.num_points = num_points\n",
        "        \n",
        "        # 特征提取器\n",
        "        self.feature_extractor = PointNetFeatureExtractor()\n",
        "        \n",
        "        # 解码器 - 全连接层\n",
        "        self.fc1 = nn.Linear(256, 512)\n",
        "        self.fc2 = nn.Linear(512, 1024)\n",
        "        self.fc3 = nn.Linear(1024, num_points * 3)\n",
        "        \n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.bn2 = nn.BatchNorm1d(1024)\n",
        "        \n",
        "        # Dropout层\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # 输入x的形状: (batch_size, num_points, 3)\n",
        "        batch_size = x.size(0)\n",
        "        \n",
        "        # 提取特征\n",
        "        point_features, global_features = self.feature_extractor(x)\n",
        "        \n",
        "        # 将全局特征展平\n",
        "        x = global_features.view(batch_size, -1)\n",
        "        \n",
        "        # 解码器\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        # 重塑为点云形状\n",
        "        x = x.view(batch_size, self.num_points, 3)\n",
        "        \n",
        "        return x\n",
        "\n",
        "def chamfer_distance(pred, gt, reduce_mean=True):\n",
        "    \"\"\"\n",
        "    计算Chamfer距离\n",
        "    \n",
        "    Args:\n",
        "        pred: 预测的点云，形状为(B, N, 3)\n",
        "        gt: 真实的点云，形状为(B, M, 3)\n",
        "        reduce_mean: 是否返回平均值\n",
        "    \n",
        "    Returns:\n",
        "        chamfer_dist: Chamfer距离\n",
        "    \"\"\"\n",
        "    # 将点云转换为(B, N, 1, 3)和(B, 1, M, 3)\n",
        "    pred = pred.unsqueeze(2)\n",
        "    gt = gt.unsqueeze(1)\n",
        "    \n",
        "    # 计算每个点到另一个点云中所有点的距离\n",
        "    dist = torch.sum((pred - gt) ** 2, dim=3)  # (B, N, M)\n",
        "    \n",
        "    # 找到最近点的距离\n",
        "    dist1, _ = torch.min(dist, dim=2)  # (B, N)\n",
        "    dist2, _ = torch.min(dist, dim=1)  # (B, M)\n",
        "    \n",
        "    # 计算Chamfer距离\n",
        "    chamfer_dist = torch.mean(dist1, dim=1) + torch.mean(dist2, dim=1)  # (B,)\n",
        "    \n",
        "    if reduce_mean:\n",
        "        chamfer_dist = torch.mean(chamfer_dist)\n",
        "    \n",
        "    return chamfer_dist\n",
        "\n",
        "# 测试模型\n",
        "if __name__ == \"__main__\":\n",
        "    # 创建模型实例\n",
        "    model = PointCompletionNet(num_points=2048)\n",
        "    model = model.to(device)\n",
        "    print(f\"模型参数数量: {sum(p.numel() for p in model.parameters())}\")\n",
        "    \n",
        "    # 生成测试数据\n",
        "    batch_size = 2\n",
        "    input_points = torch.randn(batch_size, 1024, 3).to(device)\n",
        "    \n",
        "    # 前向传播\n",
        "    output_points = model(input_points)\n",
        "    print(f\"输出点云形状: {output_points.shape}\")\n",
        "    \n",
        "    # 测试Chamfer距离\n",
        "    target_points = torch.randn(batch_size, 2048, 3).to(device)\n",
        "    loss = chamfer_distance(output_points, target_points)\n",
        "    print(f\"Chamfer距离: {loss.item():.6f}\")\n",
        "    \n",
        "    # 可视化结果\n",
        "    if batch_size > 0:\n",
        "        input_np = input_points[0].cpu().numpy()\n",
        "        output_np = output_points[0].detach().cpu().numpy()\n",
        "        target_np = target_points[0].cpu().numpy()\n",
        "        \n",
        "        print(\"\\n显示输入点云...\")\n",
        "        visualize_point_cloud(input_np)\n",
        "        \n",
        "        print(\"\\n显示输出点云...\")\n",
        "        visualize_point_cloud(output_np)\n",
        "        \n",
        "        print(\"\\n显示目标点云...\")\n",
        "        visualize_point_cloud(target_np)\n",
        "        \n",
        "        print(\"\\n显示输入与输出点云的对比...\")\n",
        "        visualize_partial_complete(input_np, output_np)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 数据集下载/路径检查（本地已下载则直接使用）\n",
        "DATA_DIR = \"/Users/arkin/Desktop/Dev/notebooks/point-cloud-completion/shapenet_car\"\n",
        "print('DATA_DIR =', DATA_DIR)\n",
        "\n",
        "required_files = ['train-001.lmdb', 'valid.lmdb']\n",
        "missing = []\n",
        "if os.path.isdir(DATA_DIR):\n",
        "    for f in required_files:\n",
        "        if not os.path.exists(os.path.join(DATA_DIR, f)):\n",
        "            missing.append(f)\n",
        "else:\n",
        "    print('目录不存在！')\n",
        "    missing = required_files\n",
        "\n",
        "if missing:\n",
        "    print('缺失文件:', missing)\n",
        "    print('将使用合成数据进行演示。')\n",
        "else:\n",
        "    print('数据路径完整，可用于训练。')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PointCloudDataset(Dataset):\n",
        "    def __init__(self, lmdb_path, num_points=2048, partial_points=1024, \n",
        "                 mode='train', transform=None):\n",
        "        \"\"\"\n",
        "        点云数据集类\n",
        "        \n",
        "        Args:\n",
        "            lmdb_path: LMDB数据库路径\n",
        "            num_points: 完整点云的点数\n",
        "            partial_points: 不完整点云的点数\n",
        "            mode: 'train' 或 'valid'\n",
        "            transform: 数据增强函数\n",
        "        \"\"\"\n",
        "        super(PointCloudDataset, self).__init__()\n",
        "        \n",
        "        self.lmdb_path = lmdb_path\n",
        "        self.num_points = num_points\n",
        "        self.partial_points = partial_points\n",
        "        self.mode = mode\n",
        "        self.transform = transform\n",
        "        \n",
        "        # 打开LMDB环境\n",
        "        self.env = lmdb.open(lmdb_path, readonly=True, lock=False)\n",
        "        with self.env.begin() as txn:\n",
        "            self.length = int(txn.get('length'.encode()).decode())\n",
        "            \n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        with self.env.begin() as txn:\n",
        "            # 获取完整点云数据\n",
        "            key = f'complete_{index}'.encode()\n",
        "            complete = pickle.loads(txn.get(key))\n",
        "            \n",
        "            # 获取不完整点云数据\n",
        "            key = f'partial_{index}'.encode()\n",
        "            partial = pickle.loads(txn.get(key))\n",
        "            \n",
        "            # 转换为numpy数组\n",
        "            complete = np.array(complete, dtype=np.float32)\n",
        "            partial = np.array(partial, dtype=np.float32)\n",
        "            \n",
        "            # 数据预处理\n",
        "            # 1. 归一化\n",
        "            complete, centroid, scale = normalize_point_cloud(complete)\n",
        "            partial = (partial - centroid) / scale\n",
        "            \n",
        "            # 2. 随机采样\n",
        "            if complete.shape[0] > self.num_points:\n",
        "                complete = random_sample_points(complete, self.num_points)\n",
        "            if partial.shape[0] > self.partial_points:\n",
        "                partial = random_sample_points(partial, self.partial_points)\n",
        "            \n",
        "            # 3. 数据增强\n",
        "            if self.transform and self.mode == 'train':\n",
        "                complete = self.transform(complete)\n",
        "                partial = self.transform(partial)\n",
        "            \n",
        "            # 转换为张量\n",
        "            complete = torch.from_numpy(complete)\n",
        "            partial = torch.from_numpy(partial)\n",
        "            \n",
        "            return {'partial': partial, 'complete': complete}\n",
        "\n",
        "class PointCloudTransform:\n",
        "    def __init__(self, noise_sigma=0.01, noise_clip=0.05, \n",
        "                 rotation=True, translation=True, scale=True):\n",
        "        \"\"\"\n",
        "        点云数据增强类\n",
        "        \n",
        "        Args:\n",
        "            noise_sigma: 高斯噪声的标准差\n",
        "            noise_clip: 噪声的最大值\n",
        "            rotation: 是否进行旋转\n",
        "            translation: 是否进行平移\n",
        "            scale: 是否进行缩放\n",
        "        \"\"\"\n",
        "        self.noise_sigma = noise_sigma\n",
        "        self.noise_clip = noise_clip\n",
        "        self.rotation = rotation\n",
        "        self.translation = translation\n",
        "        self.scale = scale\n",
        "    \n",
        "    def __call__(self, points):\n",
        "        \"\"\"\n",
        "        对点云进行数据增强\n",
        "        \n",
        "        Args:\n",
        "            points: numpy数组，形状为(N, 3)\n",
        "            \n",
        "        Returns:\n",
        "            transformed_points: 增强后的点云\n",
        "        \"\"\"\n",
        "        points = points.copy()\n",
        "        \n",
        "        # 1. 添加高斯噪声\n",
        "        points = add_noise(points, self.noise_sigma, self.noise_clip)\n",
        "        \n",
        "        # 2. 随机旋转\n",
        "        if self.rotation:\n",
        "            # 生成随机旋转角度\n",
        "            theta = np.random.uniform(0, 2*np.pi)\n",
        "            # 绕z轴旋转的旋转矩阵\n",
        "            rotation_matrix = np.array([\n",
        "                [np.cos(theta), -np.sin(theta), 0],\n",
        "                [np.sin(theta), np.cos(theta), 0],\n",
        "                [0, 0, 1]\n",
        "            ])\n",
        "            points = points @ rotation_matrix\n",
        "        \n",
        "        # 3. 随机平移\n",
        "        if self.translation:\n",
        "            translation = np.random.uniform(-0.1, 0.1, size=3)\n",
        "            points += translation\n",
        "        \n",
        "        # 4. 随机缩放\n",
        "        if self.scale:\n",
        "            scale = np.random.uniform(0.8, 1.2)\n",
        "            points *= scale\n",
        "        \n",
        "        return points.astype(np.float32)\n",
        "\n",
        "# 创建数据加载器\n",
        "def create_dataloader(lmdb_path, batch_size=32, num_workers=4, \n",
        "                     num_points=2048, partial_points=1024, mode='train'):\n",
        "    \"\"\"\n",
        "    创建数据加载器\n",
        "    \n",
        "    Args:\n",
        "        lmdb_path: LMDB数据库路径\n",
        "        batch_size: 批次大小\n",
        "        num_workers: 数据加载的进程数\n",
        "        num_points: 完整点云的点数\n",
        "        partial_points: 不完整点云的点数\n",
        "        mode: 'train' 或 'valid'\n",
        "        \n",
        "    Returns:\n",
        "        dataloader: 数据加载器\n",
        "    \"\"\"\n",
        "    # 创建数据增强\n",
        "    transform = None\n",
        "    if mode == 'train':\n",
        "        transform = PointCloudTransform(\n",
        "            noise_sigma=0.01,\n",
        "            noise_clip=0.05,\n",
        "            rotation=True,\n",
        "            translation=True,\n",
        "            scale=True\n",
        "        )\n",
        "    \n",
        "    # 创建数据集\n",
        "    dataset = PointCloudDataset(\n",
        "        lmdb_path=lmdb_path,\n",
        "        num_points=num_points,\n",
        "        partial_points=partial_points,\n",
        "        mode=mode,\n",
        "        transform=transform\n",
        "    )\n",
        "    \n",
        "    # 创建数据加载器\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=(mode == 'train'),\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "    \n",
        "    return dataloader\n",
        "\n",
        "# 测试数据加载器\n",
        "if __name__ == \"__main__\":\n",
        "    # 数据路径\n",
        "    train_path = \"/Users/arkin/Desktop/Dev/notebooks/point-cloud-completion/shapenet_car/train-001.lmdb\"\n",
        "    valid_path = \"/Users/arkin/Desktop/Dev/notebooks/point-cloud-completion/shapenet_car/valid.lmdb\"\n",
        "    \n",
        "    # 创建数据加载器\n",
        "    train_loader = create_dataloader(train_path, mode='train')\n",
        "    valid_loader = create_dataloader(valid_path, mode='valid')\n",
        "    \n",
        "    print(f\"训练数据加载器大小: {len(train_loader)}\")\n",
        "    print(f\"验证数据加载器大小: {len(valid_loader)}\")\n",
        "    \n",
        "    # 获取一个批次的数据\n",
        "    batch = next(iter(train_loader))\n",
        "    partial = batch['partial']\n",
        "    complete = batch['complete']\n",
        "    \n",
        "    print(f\"不完整点云形状: {partial.shape}\")\n",
        "    print(f\"完整点云形状: {complete.shape}\")\n",
        "    \n",
        "    # 可视化第一个样本\n",
        "    partial_np = partial[0].numpy()\n",
        "    complete_np = complete[0].numpy()\n",
        "    \n",
        "    print(\"\\n显示不完整点云与完整点云的对比...\")\n",
        "    visualize_partial_complete(partial_np, complete_np)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 四、数据可视化展示"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, valid_loader, num_epochs=100, \n",
        "              learning_rate=0.001, device=device):\n",
        "    \"\"\"\n",
        "    训练模型\n",
        "    \n",
        "    Args:\n",
        "        model: 模型实例\n",
        "        train_loader: 训练数据加载器\n",
        "        valid_loader: 验证数据加载器\n",
        "        num_epochs: 训练轮数\n",
        "        learning_rate: 学习率\n",
        "        device: 训练设备\n",
        "    \"\"\"\n",
        "    # 优化器\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    # 学习率调度器\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "    \n",
        "    # 记录最佳模型\n",
        "    best_valid_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    \n",
        "    # 训练历史\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'valid_loss': [],\n",
        "        'lr': []\n",
        "    }\n",
        "    \n",
        "    # 训练循环\n",
        "    for epoch in range(num_epochs):\n",
        "        # 训练阶段\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        train_progress = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
        "        \n",
        "        for batch in train_progress:\n",
        "            # 准备数据\n",
        "            partial = batch['partial'].to(device)\n",
        "            complete = batch['complete'].to(device)\n",
        "            \n",
        "            # 前向传播\n",
        "            output = model(partial)\n",
        "            loss = chamfer_distance(output, complete)\n",
        "            \n",
        "            # 反向传播\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # 记录损失\n",
        "            train_losses.append(loss.item())\n",
        "            train_progress.set_postfix({'loss': f'{loss.item():.6f}'})\n",
        "        \n",
        "        # 计算平均训练损失\n",
        "        avg_train_loss = np.mean(train_losses)\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        \n",
        "        # 验证阶段\n",
        "        model.eval()\n",
        "        valid_losses = []\n",
        "        valid_progress = tqdm(valid_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Valid]')\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch in valid_progress:\n",
        "                # 准备数据\n",
        "                partial = batch['partial'].to(device)\n",
        "                complete = batch['complete'].to(device)\n",
        "                \n",
        "                # 前向传播\n",
        "                output = model(partial)\n",
        "                loss = chamfer_distance(output, complete)\n",
        "                \n",
        "                # 记录损失\n",
        "                valid_losses.append(loss.item())\n",
        "                valid_progress.set_postfix({'loss': f'{loss.item():.6f}'})\n",
        "        \n",
        "        # 计算平均验证损失\n",
        "        avg_valid_loss = np.mean(valid_losses)\n",
        "        history['valid_loss'].append(avg_valid_loss)\n",
        "        \n",
        "        # 更新学习率\n",
        "        scheduler.step()\n",
        "        current_lr = scheduler.get_last_lr()[0]\n",
        "        history['lr'].append(current_lr)\n",
        "        \n",
        "        # 打印训练信息\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'Train Loss: {avg_train_loss:.6f}')\n",
        "        print(f'Valid Loss: {avg_valid_loss:.6f}')\n",
        "        print(f'Learning Rate: {current_lr:.6f}\\n')\n",
        "        \n",
        "        # 保存最佳模型\n",
        "        if avg_valid_loss < best_valid_loss:\n",
        "            best_valid_loss = avg_valid_loss\n",
        "            best_model_state = model.state_dict()\n",
        "            print(f'Found new best model with validation loss: {best_valid_loss:.6f}')\n",
        "            \n",
        "            # 保存模型\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': best_model_state,\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'train_loss': avg_train_loss,\n",
        "                'valid_loss': best_valid_loss,\n",
        "                'history': history\n",
        "            }, 'best_model.pth')\n",
        "    \n",
        "    return history, best_model_state\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"\n",
        "    绘制训练历史\n",
        "    \n",
        "    Args:\n",
        "        history: 包含训练历史的字典\n",
        "    \"\"\"\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    \n",
        "    # 创建图形\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    \n",
        "    # 绘制损失曲线\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, history['train_loss'], 'b-', label='Train Loss')\n",
        "    plt.plot(epochs, history['valid_loss'], 'r-', label='Valid Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    \n",
        "    # 绘制学习率曲线\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, history['lr'], 'g-')\n",
        "    plt.title('Learning Rate')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 训练模型\n",
        "if __name__ == \"__main__\":\n",
        "    # 创建数据加载器\n",
        "    train_path = \"/Users/arkin/Desktop/Dev/notebooks/point-cloud-completion/shapenet_car/train-001.lmdb\"\n",
        "    valid_path = \"/Users/arkin/Desktop/Dev/notebooks/point-cloud-completion/shapenet_car/valid.lmdb\"\n",
        "    \n",
        "    train_loader = create_dataloader(train_path, batch_size=32, mode='train')\n",
        "    valid_loader = create_dataloader(valid_path, batch_size=32, mode='valid')\n",
        "    \n",
        "    # 创建模型\n",
        "    model = PointCompletionNet(num_points=2048)\n",
        "    model = model.to(device)\n",
        "    \n",
        "    # 训练模型\n",
        "    history, best_model_state = train_model(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        valid_loader=valid_loader,\n",
        "        num_epochs=100,\n",
        "        learning_rate=0.001,\n",
        "        device=device\n",
        "    )\n",
        "    \n",
        "    # 绘制训练历史\n",
        "    plot_training_history(history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, device=device, num_visualize=5):\n",
        "    \"\"\"\n",
        "    评估模型并可视化结果\n",
        "    \n",
        "    Args:\n",
        "        model: 训练好的模型\n",
        "        test_loader: 测试数据加载器\n",
        "        device: 运行设备\n",
        "        num_visualize: 要可视化的样本数量\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    test_losses = []\n",
        "    visualize_samples = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(tqdm(test_loader, desc='Evaluating')):\n",
        "            # 准备数据\n",
        "            partial = batch['partial'].to(device)\n",
        "            complete = batch['complete'].to(device)\n",
        "            \n",
        "            # 前向传播\n",
        "            output = model(partial)\n",
        "            loss = chamfer_distance(output, complete)\n",
        "            \n",
        "            # 记录损失\n",
        "            test_losses.append(loss.item())\n",
        "            \n",
        "            # 收集可视化样本\n",
        "            if i < num_visualize:\n",
        "                visualize_samples.append({\n",
        "                    'partial': partial[0].cpu().numpy(),\n",
        "                    'complete': complete[0].cpu().numpy(),\n",
        "                    'output': output[0].cpu().numpy()\n",
        "                })\n",
        "    \n",
        "    # 计算平均测试损失\n",
        "    avg_test_loss = np.mean(test_losses)\n",
        "    print(f'\\nAverage Test Loss: {avg_test_loss:.6f}')\n",
        "    \n",
        "    # 可视化结果\n",
        "    for i, sample in enumerate(visualize_samples):\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        \n",
        "        # 创建三个子图\n",
        "        ax1 = plt.subplot(131, projection='3d')\n",
        "        ax2 = plt.subplot(132, projection='3d')\n",
        "        ax3 = plt.subplot(133, projection='3d')\n",
        "        \n",
        "        # 绘制不完整点云\n",
        "        partial = sample['partial']\n",
        "        ax1.scatter(partial[:, 0], partial[:, 1], partial[:, 2], c='r', marker='.')\n",
        "        ax1.set_title('Partial Point Cloud')\n",
        "        \n",
        "        # 绘制完整点云\n",
        "        complete = sample['complete']\n",
        "        ax2.scatter(complete[:, 0], complete[:, 1], complete[:, 2], c='g', marker='.')\n",
        "        ax2.set_title('Ground Truth')\n",
        "        \n",
        "        # 绘制预测点云\n",
        "        output = sample['output']\n",
        "        ax3.scatter(output[:, 0], output[:, 1], output[:, 2], c='b', marker='.')\n",
        "        ax3.set_title('Model Output')\n",
        "        \n",
        "        # 设置视角\n",
        "        for ax in [ax1, ax2, ax3]:\n",
        "            ax.view_init(elev=30, azim=45)\n",
        "            ax.set_xlabel('X')\n",
        "            ax.set_ylabel('Y')\n",
        "            ax.set_zlabel('Z')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # 使用Open3D进行交互式可视化\n",
        "        print(f'\\nSample {i+1} - Interactive Visualization:')\n",
        "        print('显示不完整点云与预测点云的对比...')\n",
        "        visualize_partial_complete(partial, output)\n",
        "        \n",
        "        print('显示预测点云与真实点云的对比...')\n",
        "        visualize_partial_complete(output, complete)\n",
        "\n",
        "def compute_metrics(model, test_loader, device=device):\n",
        "    \"\"\"\n",
        "    计算详细的评估指标\n",
        "    \n",
        "    Args:\n",
        "        model: 训练好的模型\n",
        "        test_loader: 测试数据加载器\n",
        "        device: 运行设备\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    chamfer_distances = []\n",
        "    point_nums = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc='Computing Metrics'):\n",
        "            partial = batch['partial'].to(device)\n",
        "            complete = batch['complete'].to(device)\n",
        "            output = model(partial)\n",
        "            \n",
        "            # 计算每个样本的Chamfer距离\n",
        "            cd = chamfer_distance(output, complete, reduce_mean=False)\n",
        "            chamfer_distances.extend(cd.cpu().numpy())\n",
        "            \n",
        "            # 记录点的数量\n",
        "            point_nums.extend([\n",
        "                partial.shape[1],  # 输入点数\n",
        "                complete.shape[1],  # 目标点数\n",
        "                output.shape[1]    # 输出点数\n",
        "            ])\n",
        "    \n",
        "    # 计算统计信息\n",
        "    cd_mean = np.mean(chamfer_distances)\n",
        "    cd_std = np.std(chamfer_distances)\n",
        "    cd_median = np.median(chamfer_distances)\n",
        "    cd_min = np.min(chamfer_distances)\n",
        "    cd_max = np.max(chamfer_distances)\n",
        "    \n",
        "    # 打印评估结果\n",
        "    print('\\nEvaluation Metrics:')\n",
        "    print(f'Chamfer Distance:')\n",
        "    print(f'  Mean   : {cd_mean:.6f}')\n",
        "    print(f'  Std    : {cd_std:.6f}')\n",
        "    print(f'  Median : {cd_median:.6f}')\n",
        "    print(f'  Min    : {cd_min:.6f}')\n",
        "    print(f'  Max    : {cd_max:.6f}')\n",
        "    \n",
        "    # 绘制Chamfer距离分布\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.hist(chamfer_distances, bins=50, density=True)\n",
        "    plt.axvline(cd_mean, color='r', linestyle='--', label=f'Mean: {cd_mean:.6f}')\n",
        "    plt.axvline(cd_median, color='g', linestyle='--', label=f'Median: {cd_median:.6f}')\n",
        "    plt.title('Distribution of Chamfer Distances')\n",
        "    plt.xlabel('Chamfer Distance')\n",
        "    plt.ylabel('Density')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# 评估模型\n",
        "if __name__ == \"__main__\":\n",
        "    # 加载最佳模型\n",
        "    checkpoint = torch.load('best_model.pth')\n",
        "    model = PointCompletionNet(num_points=2048)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model = model.to(device)\n",
        "    \n",
        "    # 创建测试数据加载器\n",
        "    test_path = \"/Users/arkin/Desktop/Dev/notebooks/point-cloud-completion/shapenet_car/valid.lmdb\"\n",
        "    test_loader = create_dataloader(test_path, batch_size=32, mode='valid')\n",
        "    \n",
        "    # 评估模型\n",
        "    print(\"正在评估模型...\")\n",
        "    evaluate_model(model, test_loader, num_visualize=5)\n",
        "    \n",
        "    # 计算详细指标\n",
        "    print(\"\\n计算详细评估指标...\")\n",
        "    compute_metrics(model, test_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 五、数据预处理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 六、ShapeNet模型架构"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 七、数据集类与数据加载器"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 八、模型训练"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 九、模型评估与可视化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
