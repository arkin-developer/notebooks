{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5483e876",
   "metadata": {},
   "source": [
    "### 什么是 RLHF（Reinforcement Learning from Human Feedback）\n",
    "\n",
    "RLHF 是用“人类偏好”来对大语言模型进行对齐的一套训练范式：先让模型会做事，再让模型知道“什么更好”，最后用强化学习把“更好”的偏好真正优化进生成策略里。\n",
    "\n",
    "- **目标**：让模型更符合人类意图、更安全、更有用\n",
    "- **核心思想**：\n",
    "  - 用监督微调（SFT）教会模型基本的指令跟随\n",
    "  - 用偏好数据训练奖励模型（RM），学会打分“更好/更差”的回答\n",
    "  - 用强化学习（PPO）在奖励信号下优化策略，权衡质量、稳定性与多样性\n",
    "- **关键组件**：指令数据、偏好数据（A/B 对比）、奖励模型、强化学习算法、KL 约束/参考策略\n",
    "- **典型产物**：\n",
    "  - SFT 模型（会做事）\n",
    "  - RM 奖励模型（会打分）\n",
    "  - PPO 后的对齐模型（做得更好）\n",
    "  - DPO （取缔RM+PPO）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ff10f6",
   "metadata": {},
   "source": [
    "### 三、RLHF 的三阶段流程（工程化视角）\n",
    "\n",
    "| 阶段 | 名称 | 作用 | 技术 |\n",
    "|---|---|---|---|\n",
    "| 1️⃣ | SFT（监督微调） | 教模型执行指令 | CrossEntropyLoss |\n",
    "| 2️⃣ | Reward Model 训练 | 学会“什么样的回答更好” | Pairwise ranking (A > B) |\n",
    "| 3️⃣ | PPO 强化优化 | 用奖励信号优化生成策略 | PPO 算法（Policy Gradient） |\n",
    "\n",
    "#### 1️⃣ SFT（监督微调）\n",
    "- **输入**：指令-回答对（高质量、人类书写/筛选）\n",
    "- **目标**：让模型基本学会“按指令作答”\n",
    "- **训练**：最小化交叉熵损失（参考常用指令数据集）\n",
    "- **输出**：SFT 模型（作为后续 RM/PPO 的参考策略）\n",
    "\n",
    "#### 2️⃣ 奖励模型（RM）训练\n",
    "- **输入**：同一指令下成对回答（A、B），以及偏好标签（A > B）\n",
    "- **目标**：学习“偏好评分函数” r(x, y)\n",
    "- **训练**：Pairwise ranking（如 Bradley–Terry/Logistic loss）\n",
    "- **输出**：能对任意回答打分的奖励模型\n",
    "\n",
    "#### 3️⃣ PPO 强化优化\n",
    "- **输入**：SFT 模型作为初始策略 π_θ，奖励模型 r 作为奖励信号\n",
    "- **目标**：在 KL 约束下最大化期望奖励，提升对齐度与有用性\n",
    "- **训练**：PPO（剪切策略梯度），引入 KL 惩罚以保持与参考策略接近\n",
    "- **输出**：PPO 后的对齐模型（更符合人类偏好）\n",
    "\n",
    "> 实践要点：高质量偏好数据与稳定的 KL 控制是成功关键；监控长度偏置、模式坍缩与过拟合。\n",
    "\n",
    "#### DPO（Direct Preference Optimization）\n",
    "- **定位**：作为第 3 阶段（PPO）的常见替代方案，用偏好对直接优化策略。\n",
    "- **核心**：基于 `(x, y_pos, y_neg)` 提高 `y_pos` 概率、降低 `y_neg`，并以参考策略 `π_ref` 的对数概率差作隐式 KL 约束。\n",
    "- **直观目标**：最小化 `-log σ(β[(log πθ(y_pos|x) - log πθ(y_neg|x)) - (log πref(y_pos|x) - log πref(y_neg|x))])`\n",
    "- **优点**：流程简单、无奖励模型与 RL 回路、稳定易复现、吞吐高。\n",
    "- **局限**：依赖高质量偏好数据；极端分布迁移下可控性较弱。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777789fd",
   "metadata": {},
   "source": [
    "### 实验设置：模型与数据集选择\n",
    "\n",
    "- 模型：`Qwen2.5-1.5B-Instruct`（中文指令能力强，小参数、易于 LoRA/QLoRA）\n",
    "- SFT 数据：`BelleGroup/train_0.5M_CN`（中文指令-回答对，体量适中，可采样）\n",
    "- 偏好数据（用于 DPO/RM）：`argilla/ultrafeedback-binarized-preferences`（成对偏好，易直接用于 DPO）\n",
    "\n",
    "下面先安装依赖并加载模型、抽样加载 SFT 数据（少量样本用于快速跑通）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8be2dae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: 4.44.0 not found\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 安装依赖（仅需首次）\n",
    "%pip -q install transformers>=4.44.0 accelerate datasets peft bitsandbytes trl>=0.9.6 sentencepiece\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a054388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] CUDA 不可用，跳过 bitsandbytes 量化，改用 MPS/CPU.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39701511a03045a9894de1a2d23a975b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819e26a2ee33465cb0172f7968969ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e59697db756a4b37bc7b25282bb87f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89992a0d96104244a1aa1d9e54d3a999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8974f32f9d1c4528a9c22ba13d1e69f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f58ab1594d429b9b3e2e120af1f90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_mps = torch.backends.mps.is_available()\n",
    "\n",
    "quant_config = None\n",
    "try:\n",
    "    if use_cuda:\n",
    "        from transformers import BitsAndBytesConfig  # 仅在 CUDA 下尝试 4bit\n",
    "        import importlib.metadata as im\n",
    "        im.version(\"bitsandbytes\")  # 检查安装\n",
    "        quant_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "        print(\"[Info] Using bitsandbytes 4-bit on CUDA.\")\n",
    "    else:\n",
    "        print(\"[Info] CUDA 不可用，跳过 bitsandbytes 量化，改用 MPS/CPU.\")\n",
    "except Exception as e:\n",
    "    print(f\"[Warn] bitsandbytes 不可用或未安装：{e}. 将使用非量化加载。\")\n",
    "\n",
    "# 设备映射\n",
    "if use_cuda:\n",
    "    device_map = \"auto\"\n",
    "    dtype = torch.bfloat16\n",
    "elif use_mps:\n",
    "    device_map = {\"\": \"mps\"}\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    device_map = {\"\": \"cpu\"}\n",
    "    dtype = torch.float32\n",
    "\n",
    "# 加载 tokenizer / model（按可用性量化）\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, trust_remote_code=True)\n",
    "\n",
    "load_kwargs = dict(\n",
    "    device_map=device_map,\n",
    "    torch_dtype=dtype,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "if quant_config is not None:\n",
    "    load_kwargs[\"quantization_config\"] = quant_config\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, **load_kwargs)\n",
    "\n",
    "print(f\"[Device] cuda={use_cuda}, mps={use_mps}, dtype={dtype}\")\n",
    "\n",
    "# 快速自检\n",
    "inputs = tokenizer(\"你好，简要介绍一下你自己。\", return_tensors=\"pt\")\n",
    "if use_mps:\n",
    "    inputs = {k: v.to(\"mps\") for k, v in inputs.items()}\n",
    "else:\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "with torch.inference_mode():\n",
    "    out = model.generate(**inputs, max_new_tokens=64, do_sample=False)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7715e506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def _to_sft(example):\n",
    "    instr = example.get(\"instruction\", \"\")\n",
    "    inp = example.get(\"input\", \"\")\n",
    "    output = example.get(\"output\", None)\n",
    "    prompt = (instr + (\"\\n\" + inp if inp else \"\")).strip()\n",
    "    return {\"prompt\": prompt, \"response\": output}\n",
    "\n",
    "# SFT：抽样加载 BELLE 中文指令数据\n",
    "sft_ds = load_dataset(\"BelleGroup/train_0.5M_CN\", split=\"train[:2000]\")\n",
    "sft_ds = sft_ds.map(_to_sft, remove_columns=sft_ds.column_names)\n",
    "print(\"SFT 样本示例:\", sft_ds[0])\n",
    "\n",
    "# 偏好数据：UltraFeedback（用于 DPO/RM）\n",
    "pref = load_dataset(\"argilla/ultrafeedback-binarized-preferences\", split=\"train[:5000]\")\n",
    "\n",
    "def _to_pref(ex):\n",
    "    prompt = ex.get(\"prompt\") or ex.get(\"question\") or ex.get(\"instruction\")\n",
    "    y_pos = ex.get(\"chosen\") or ex.get(\"better_response\")\n",
    "    y_neg = ex.get(\"rejected\") or ex.get(\"worse_response\")\n",
    "    return {\"prompt\": prompt, \"y_pos\": y_pos, \"y_neg\": y_neg}\n",
    "\n",
    "pref = pref.map(_to_pref)\n",
    "pref = pref.filter(lambda e: e[\"prompt\"] and e[\"y_pos\"] and e[\"y_neg\"])  # 保留完整样本\n",
    "print(\"偏好样本示例:\", {k: pref[0][k][:60] + \"...\" for k in [\"prompt\", \"y_pos\", \"y_neg\"]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f63aa6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
