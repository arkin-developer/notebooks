{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5483e876",
   "metadata": {},
   "source": [
    "### 什么是 RLHF（Reinforcement Learning from Human Feedback）\n",
    "\n",
    "RLHF 是用“人类偏好”来对大语言模型进行对齐的一套训练范式：先让模型会做事，再让模型知道“什么更好”，最后用强化学习把“更好”的偏好真正优化进生成策略里。\n",
    "\n",
    "- **目标**：让模型更符合人类意图、更安全、更有用\n",
    "- **核心思想**：\n",
    "  - 用监督微调（SFT）教会模型基本的指令跟随\n",
    "  - 用偏好数据训练奖励模型（RM），学会打分“更好/更差”的回答\n",
    "  - 用强化学习（PPO）在奖励信号下优化策略，权衡质量、稳定性与多样性\n",
    "- **关键组件**：指令数据、偏好数据（A/B 对比）、奖励模型、强化学习算法、KL 约束/参考策略\n",
    "- **典型产物**：\n",
    "  - SFT 模型（会做事）\n",
    "  - RM 奖励模型（会打分）\n",
    "  - PPO 后的对齐模型（做得更好）\n",
    "  - DPO （取缔RM+PPO）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ff10f6",
   "metadata": {},
   "source": [
    "### 三、RLHF 的三阶段流程（工程化视角）\n",
    "\n",
    "| 阶段 | 名称 | 作用 | 技术 |\n",
    "|---|---|---|---|\n",
    "| 1️⃣ | SFT（监督微调） | 教模型执行指令 | CrossEntropyLoss |\n",
    "| 2️⃣ | Reward Model 训练 | 学会“什么样的回答更好” | Pairwise ranking (A > B) |\n",
    "| 3️⃣ | PPO 强化优化 | 用奖励信号优化生成策略 | PPO 算法（Policy Gradient） |\n",
    "\n",
    "#### 1️⃣ SFT（监督微调）\n",
    "- **输入**：指令-回答对（高质量、人类书写/筛选）\n",
    "- **目标**：让模型基本学会“按指令作答”\n",
    "- **训练**：最小化交叉熵损失（参考常用指令数据集）\n",
    "- **输出**：SFT 模型（作为后续 RM/PPO 的参考策略）\n",
    "\n",
    "#### 2️⃣ 奖励模型（RM）训练\n",
    "- **输入**：同一指令下成对回答（A、B），以及偏好标签（A > B）\n",
    "- **目标**：学习“偏好评分函数” r(x, y)\n",
    "- **训练**：Pairwise ranking（如 Bradley–Terry/Logistic loss）\n",
    "- **输出**：能对任意回答打分的奖励模型\n",
    "\n",
    "#### 3️⃣ PPO 强化优化\n",
    "- **输入**：SFT 模型作为初始策略 π_θ，奖励模型 r 作为奖励信号\n",
    "- **目标**：在 KL 约束下最大化期望奖励，提升对齐度与有用性\n",
    "- **训练**：PPO（剪切策略梯度），引入 KL 惩罚以保持与参考策略接近\n",
    "- **输出**：PPO 后的对齐模型（更符合人类偏好）\n",
    "\n",
    "> 实践要点：高质量偏好数据与稳定的 KL 控制是成功关键；监控长度偏置、模式坍缩与过拟合。\n",
    "\n",
    "#### DPO（Direct Preference Optimization）\n",
    "- **定位**：作为第 3 阶段（PPO）的常见替代方案，用偏好对直接优化策略。\n",
    "- **核心**：基于 `(x, y_pos, y_neg)` 提高 `y_pos` 概率、降低 `y_neg`，并以参考策略 `π_ref` 的对数概率差作隐式 KL 约束。\n",
    "- **直观目标**：最小化 `-log σ(β[(log πθ(y_pos|x) - log πθ(y_neg|x)) - (log πref(y_pos|x) - log πref(y_neg|x))])`\n",
    "- **优点**：流程简单、无奖励模型与 RL 回路、稳定易复现、吞吐高。\n",
    "- **局限**：依赖高质量偏好数据；极端分布迁移下可控性较弱。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777789fd",
   "metadata": {},
   "source": [
    "### 实验设置：模型与数据集选择\n",
    "\n",
    "- 模型：`Qwen2.5-1.5B-Instruct`（中文指令能力强，小参数、易于 LoRA/QLoRA）\n",
    "- SFT 数据：`BelleGroup/train_0.5M_CN`（中文指令-回答对，体量适中，可采样）\n",
    "- 偏好数据（用于 DPO/RM）：`argilla/ultrafeedback-binarized-preferences`（成对偏好，易直接用于 DPO）\n",
    "\n",
    "下面先安装依赖并加载模型、抽样加载 SFT 数据（少量样本用于快速跑通）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8be2dae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: 4.44.0 not found\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 安装依赖（仅需首次）\n",
    "%pip -q install transformers>=4.44.0 accelerate datasets peft bitsandbytes trl>=0.9.6 sentencepiece\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a054388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] CUDA 不可用，跳过 bitsandbytes 量化，改用 MPS/CPU.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39701511a03045a9894de1a2d23a975b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819e26a2ee33465cb0172f7968969ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e59697db756a4b37bc7b25282bb87f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89992a0d96104244a1aa1d9e54d3a999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8974f32f9d1c4528a9c22ba13d1e69f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f58ab1594d429b9b3e2e120af1f90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acda0442b28c47b4a5c9c5ac945da0fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Device] cuda=False, mps=True, dtype=torch.float16\n",
      "你好，简要介绍一下你自己。 作为一个AI助手，我叫通义千问。我可以回答各种问题、提供信息和帮助您完成任务。有什么我能帮您的吗？\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_mps = torch.backends.mps.is_available()\n",
    "\n",
    "quant_config = None\n",
    "try:\n",
    "    if use_cuda:\n",
    "        from transformers import BitsAndBytesConfig  # 仅在 CUDA 下尝试 4bit\n",
    "        import importlib.metadata as im\n",
    "        im.version(\"bitsandbytes\")  # 检查安装\n",
    "        quant_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "        print(\"[Info] Using bitsandbytes 4-bit on CUDA.\")\n",
    "    else:\n",
    "        print(\"[Info] CUDA 不可用，跳过 bitsandbytes 量化，改用 MPS/CPU.\")\n",
    "except Exception as e:\n",
    "    print(f\"[Warn] bitsandbytes 不可用或未安装：{e}. 将使用非量化加载。\")\n",
    "\n",
    "# 设备映射\n",
    "if use_cuda:\n",
    "    device_map = \"auto\"\n",
    "    dtype = torch.bfloat16\n",
    "elif use_mps:\n",
    "    device_map = {\"\": \"mps\"}\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    device_map = {\"\": \"cpu\"}\n",
    "    dtype = torch.float32\n",
    "\n",
    "# 加载 tokenizer / model（按可用性量化）\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, trust_remote_code=True)\n",
    "\n",
    "load_kwargs = dict(\n",
    "    device_map=device_map,\n",
    "    torch_dtype=dtype,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "if quant_config is not None:\n",
    "    load_kwargs[\"quantization_config\"] = quant_config\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, **load_kwargs)\n",
    "\n",
    "print(f\"[Device] cuda={use_cuda}, mps={use_mps}, dtype={dtype}\")\n",
    "\n",
    "# 快速自检\n",
    "inputs = tokenizer(\"你好，简要介绍一下你自己。\", return_tensors=\"pt\")\n",
    "if use_mps:\n",
    "    inputs = {k: v.to(\"mps\") for k, v in inputs.items()}\n",
    "else:\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "with torch.inference_mode():\n",
    "    out = model.generate(**inputs, max_new_tokens=64, do_sample=False)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7715e506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT 条数: 2000\n",
      "SFT 样本示例: {'prompt': '给定一个英文句子，翻译成中文。\\nI love to learn new things every day.', 'response': '我每天喜欢学习新事物。'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004bdf07eb9447ac83501e4e1abe8fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train_prefs-00000-of-00001.parquet:   0%|          | 0.00/226M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cancellation requested; stopping current tasks.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:629\u001b[0m, in \u001b[0;36mxet_get\u001b[0;34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001b[0m\n\u001b[1;32m    627\u001b[0m     progress\u001b[38;5;241m.\u001b[39mupdate(progress_bytes)\n\u001b[0;32m--> 629\u001b[0m download_files(\n\u001b[1;32m    630\u001b[0m     xet_download_info,\n\u001b[1;32m    631\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39mconnection_info\u001b[38;5;241m.\u001b[39mendpoint,\n\u001b[1;32m    632\u001b[0m     token_info\u001b[38;5;241m=\u001b[39m(connection_info\u001b[38;5;241m.\u001b[39maccess_token, connection_info\u001b[38;5;241m.\u001b[39mexpiration_unix_epoch),\n\u001b[1;32m    633\u001b[0m     token_refresher\u001b[38;5;241m=\u001b[39mtoken_refresher,\n\u001b[1;32m    634\u001b[0m     progress_updater\u001b[38;5;241m=\u001b[39m[progress_updater],\n\u001b[1;32m    635\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Data processing error: Internal error : Join error",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m         ds \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margilla/ultrafeedback-binarized-preferences\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain[:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_rows\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n\u001b[0;32m---> 25\u001b[0m pref \u001b[38;5;241m=\u001b[39m load_ultrafeedback()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 统一映射到 (prompt, y_pos, y_neg)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_pref\u001b[39m(ex):\n",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m, in \u001b[0;36mload_ultrafeedback\u001b[0;34m(max_rows)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_ultrafeedback\u001b[39m(max_rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m20000\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m         ds \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuggingFaceH4/ultrafeedback_binarized\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_prefs[:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_rows\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m         ds \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margilla/ultrafeedback-binarized-preferences\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain[:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_rows\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/load.py:1412\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[1;32m   1411\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 1412\u001b[0m builder_instance\u001b[38;5;241m.\u001b[39mdownload_and_prepare(\n\u001b[1;32m   1413\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[1;32m   1414\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[1;32m   1415\u001b[0m     verification_mode\u001b[38;5;241m=\u001b[39mverification_mode,\n\u001b[1;32m   1416\u001b[0m     num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[1;32m   1417\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m   1418\u001b[0m )\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   1421\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1422\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   1423\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/builder.py:894\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    893\u001b[0m     prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m--> 894\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_and_prepare(\n\u001b[1;32m    895\u001b[0m     dl_manager\u001b[38;5;241m=\u001b[39mdl_manager,\n\u001b[1;32m    896\u001b[0m     verification_mode\u001b[38;5;241m=\u001b[39mverification_mode,\n\u001b[1;32m    897\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_split_kwargs,\n\u001b[1;32m    898\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdownload_and_prepare_kwargs,\n\u001b[1;32m    899\u001b[0m )\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/builder.py:948\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    946\u001b[0m split_dict \u001b[38;5;241m=\u001b[39m SplitDict(dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name)\n\u001b[1;32m    947\u001b[0m split_generators_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_split_generators_kwargs(prepare_split_kwargs)\n\u001b[0;32m--> 948\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_generators(dl_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msplit_generators_kwargs)\n\u001b[1;32m    950\u001b[0m \u001b[38;5;66;03m# Checksums verification\u001b[39;00m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS \u001b[38;5;129;01mand\u001b[39;00m dl_manager\u001b[38;5;241m.\u001b[39mrecord_checksums:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/packaged_modules/parquet/parquet.py:49\u001b[0m, in \u001b[0;36mParquet._split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one data file must be specified, but got data_files=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdata_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m dl_manager\u001b[38;5;241m.\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mextract_on_the_fly \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m data_files \u001b[38;5;241m=\u001b[39m dl_manager\u001b[38;5;241m.\u001b[39mdownload_and_extract(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdata_files)\n\u001b[1;32m     50\u001b[0m splits \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split_name, files \u001b[38;5;129;01min\u001b[39;00m data_files\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/download/download_manager.py:326\u001b[0m, in \u001b[0;36mDownloadManager.download_and_extract\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_and_extract\u001b[39m(\u001b[38;5;28mself\u001b[39m, url_or_urls):\n\u001b[1;32m    311\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Download and extract given `url_or_urls`.\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m    Is roughly equivalent to:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;124;03m        extracted_path(s): `str`, extracted paths of given URL(s).\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload(url_or_urls))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/download/download_manager.py:159\u001b[0m, in \u001b[0;36mDownloadManager.download\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    157\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m stack_multiprocessing_download_progress_bars():\n\u001b[0;32m--> 159\u001b[0m     downloaded_path_or_paths \u001b[38;5;241m=\u001b[39m map_nested(\n\u001b[1;32m    160\u001b[0m         download_func,\n\u001b[1;32m    161\u001b[0m         url_or_urls,\n\u001b[1;32m    162\u001b[0m         map_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    163\u001b[0m         num_proc\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mnum_proc,\n\u001b[1;32m    164\u001b[0m         desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading data files\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    165\u001b[0m         batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    166\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    167\u001b[0m     )\n\u001b[1;32m    168\u001b[0m duration \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    169\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduration\u001b[38;5;241m.\u001b[39mtotal_seconds()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m min\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/utils/py_utils.py:520\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    518\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_proc \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m%\u001b[39m num_proc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    519\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(iter_batched(iterable, batch_size))\n\u001b[0;32m--> 520\u001b[0m mapped \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    521\u001b[0m     _single_map_nested((function, obj, batched, batch_size, types, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m hf_tqdm(iterable, disable\u001b[38;5;241m=\u001b[39mdisable_tqdm, desc\u001b[38;5;241m=\u001b[39mdesc)\n\u001b[1;32m    523\u001b[0m ]\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    525\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m mapped_batch \u001b[38;5;129;01min\u001b[39;00m mapped \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m mapped_batch]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/utils/py_utils.py:521\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    518\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_proc \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m%\u001b[39m num_proc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    519\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(iter_batched(iterable, batch_size))\n\u001b[1;32m    520\u001b[0m mapped \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 521\u001b[0m     _single_map_nested((function, obj, batched, batch_size, types, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m hf_tqdm(iterable, disable\u001b[38;5;241m=\u001b[39mdisable_tqdm, desc\u001b[38;5;241m=\u001b[39mdesc)\n\u001b[1;32m    523\u001b[0m ]\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    525\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m mapped_batch \u001b[38;5;129;01min\u001b[39;00m mapped \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m mapped_batch]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/utils/py_utils.py:408\u001b[0m, in \u001b[0;36m_single_map_nested\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    405\u001b[0m         k: _single_map_nested((function, v, batched, batch_size, types, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m pbar\n\u001b[1;32m    406\u001b[0m     }\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 408\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m [_single_map_nested((function, v, batched, batch_size, types, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m pbar]\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mapped\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/utils/py_utils.py:408\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    405\u001b[0m         k: _single_map_nested((function, v, batched, batch_size, types, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m pbar\n\u001b[1;32m    406\u001b[0m     }\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 408\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m [_single_map_nested((function, v, batched, batch_size, types, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m pbar]\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mapped\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/utils/py_utils.py:389\u001b[0m, in \u001b[0;36m_single_map_nested\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m function(data_struct)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    384\u001b[0m     batched\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, types)\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mdict\u001b[39m, types)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data_struct)\n\u001b[1;32m    388\u001b[0m ):\n\u001b[0;32m--> 389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iter_batched(data_struct, batch_size) \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m function(batch)]\n\u001b[1;32m    391\u001b[0m \u001b[38;5;66;03m# Reduce logging to keep things readable in multiprocessing with tqdm\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mget_verbosity() \u001b[38;5;241m<\u001b[39m logging\u001b[38;5;241m.\u001b[39mWARNING:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/utils/py_utils.py:389\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m function(data_struct)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    384\u001b[0m     batched\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, types)\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mdict\u001b[39m, types)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data_struct)\n\u001b[1;32m    388\u001b[0m ):\n\u001b[0;32m--> 389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iter_batched(data_struct, batch_size) \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m function(batch)]\n\u001b[1;32m    391\u001b[0m \u001b[38;5;66;03m# Reduce logging to keep things readable in multiprocessing with tqdm\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mget_verbosity() \u001b[38;5;241m<\u001b[39m logging\u001b[38;5;241m.\u001b[39mWARNING:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/download/download_manager.py:219\u001b[0m, in \u001b[0;36mDownloadManager._download_batched\u001b[0;34m(self, url_or_filenames, download_config)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m thread_map(\n\u001b[1;32m    207\u001b[0m         download_func,\n\u001b[1;32m    208\u001b[0m         url_or_filenames,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m         tqdm_class\u001b[38;5;241m=\u001b[39mtqdm,\n\u001b[1;32m    217\u001b[0m     )\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_single(url_or_filename, download_config\u001b[38;5;241m=\u001b[39mdownload_config)\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m url_or_filename \u001b[38;5;129;01min\u001b[39;00m url_or_filenames\n\u001b[1;32m    222\u001b[0m     ]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/download/download_manager.py:220\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m thread_map(\n\u001b[1;32m    207\u001b[0m         download_func,\n\u001b[1;32m    208\u001b[0m         url_or_filenames,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m         tqdm_class\u001b[38;5;241m=\u001b[39mtqdm,\n\u001b[1;32m    217\u001b[0m     )\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 220\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_single(url_or_filename, download_config\u001b[38;5;241m=\u001b[39mdownload_config)\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m url_or_filename \u001b[38;5;129;01min\u001b[39;00m url_or_filenames\n\u001b[1;32m    222\u001b[0m     ]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/download/download_manager.py:229\u001b[0m, in \u001b[0;36mDownloadManager._download_single\u001b[0;34m(self, url_or_filename, download_config)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_relative_path(url_or_filename):\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# append the relative path to the base_path\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     url_or_filename \u001b[38;5;241m=\u001b[39m url_or_path_join(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_path, url_or_filename)\n\u001b[0;32m--> 229\u001b[0m out \u001b[38;5;241m=\u001b[39m cached_path(url_or_filename, download_config\u001b[38;5;241m=\u001b[39mdownload_config)\n\u001b[1;32m    230\u001b[0m out \u001b[38;5;241m=\u001b[39m tracked_str(out)\n\u001b[1;32m    231\u001b[0m out\u001b[38;5;241m.\u001b[39mset_origin(url_or_filename)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/datasets/utils/file_utils.py:177\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m resolved_path \u001b[38;5;241m=\u001b[39m huggingface_hub\u001b[38;5;241m.\u001b[39mHfFileSystem(\n\u001b[1;32m    168\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mHF_ENDPOINT, token\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mtoken\n\u001b[1;32m    169\u001b[0m )\u001b[38;5;241m.\u001b[39mresolve_path(url_or_filename)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m huggingface_hub\u001b[38;5;241m.\u001b[39mHfApi(\n\u001b[1;32m    172\u001b[0m         endpoint\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mHF_ENDPOINT,\n\u001b[1;32m    173\u001b[0m         token\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mtoken,\n\u001b[1;32m    174\u001b[0m         library_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    175\u001b[0m         library_version\u001b[38;5;241m=\u001b[39m__version__,\n\u001b[1;32m    176\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39mget_datasets_user_agent(download_config\u001b[38;5;241m.\u001b[39muser_agent),\n\u001b[0;32m--> 177\u001b[0m     )\u001b[38;5;241m.\u001b[39mhf_hub_download(\n\u001b[1;32m    178\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mresolved_path\u001b[38;5;241m.\u001b[39mrepo_id,\n\u001b[1;32m    179\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mresolved_path\u001b[38;5;241m.\u001b[39mrepo_type,\n\u001b[1;32m    180\u001b[0m         revision\u001b[38;5;241m=\u001b[39mresolved_path\u001b[38;5;241m.\u001b[39mrevision,\n\u001b[1;32m    181\u001b[0m         filename\u001b[38;5;241m=\u001b[39mresolved_path\u001b[38;5;241m.\u001b[39mpath_in_repo,\n\u001b[1;32m    182\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mforce_download,\n\u001b[1;32m    183\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mproxies,\n\u001b[1;32m    184\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    186\u001b[0m     huggingface_hub\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mRepositoryNotFoundError,\n\u001b[1;32m    187\u001b[0m     huggingface_hub\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mEntryNotFoundError,\n\u001b[1;32m    188\u001b[0m     huggingface_hub\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mRevisionNotFoundError,\n\u001b[1;32m    189\u001b[0m     huggingface_hub\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mGatedRepoError,\n\u001b[1;32m    190\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/hf_api.py:5528\u001b[0m, in \u001b[0;36mHfApi.hf_hub_download\u001b[0;34m(self, repo_id, filename, subfolder, repo_type, revision, cache_dir, local_dir, force_download, proxies, etag_timeout, token, local_files_only, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   5524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5525\u001b[0m     \u001b[38;5;66;03m# Cannot do `token = token or self.token` as token can be `False`.\u001b[39;00m\n\u001b[1;32m   5526\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken\n\u001b[0;32m-> 5528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hf_hub_download(\n\u001b[1;32m   5529\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[1;32m   5530\u001b[0m     filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m   5531\u001b[0m     subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[1;32m   5532\u001b[0m     repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m   5533\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   5534\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint,\n\u001b[1;32m   5535\u001b[0m     library_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlibrary_name,\n\u001b[1;32m   5536\u001b[0m     library_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlibrary_version,\n\u001b[1;32m   5537\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   5538\u001b[0m     local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[1;32m   5539\u001b[0m     local_dir_use_symlinks\u001b[38;5;241m=\u001b[39mlocal_dir_use_symlinks,\n\u001b[1;32m   5540\u001b[0m     user_agent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_agent,\n\u001b[1;32m   5541\u001b[0m     force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m   5542\u001b[0m     force_filename\u001b[38;5;241m=\u001b[39mforce_filename,\n\u001b[1;32m   5543\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   5544\u001b[0m     etag_timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[1;32m   5545\u001b[0m     resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m   5546\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   5547\u001b[0m     headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   5548\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   5549\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1010\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    992\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1007\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1008\u001b[0m     )\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1010\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   1013\u001b[0m         \u001b[38;5;66;03m# File info\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[1;32m   1015\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m   1016\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m   1017\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1018\u001b[0m         \u001b[38;5;66;03m# HTTP info\u001b[39;00m\n\u001b[1;32m   1019\u001b[0m         endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[1;32m   1020\u001b[0m         etag_timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[1;32m   1021\u001b[0m         headers\u001b[38;5;241m=\u001b[39mhf_headers,\n\u001b[1;32m   1022\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1023\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   1024\u001b[0m         \u001b[38;5;66;03m# Additional options\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1026\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m   1027\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1171\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1171\u001b[0m     _download_to_tmp_and_move(\n\u001b[1;32m   1172\u001b[0m         incomplete_path\u001b[38;5;241m=\u001b[39mPath(blob_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.incomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1173\u001b[0m         destination_path\u001b[38;5;241m=\u001b[39mPath(blob_path),\n\u001b[1;32m   1174\u001b[0m         url_to_download\u001b[38;5;241m=\u001b[39murl_to_download,\n\u001b[1;32m   1175\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1176\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1177\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[1;32m   1178\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m   1179\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m   1180\u001b[0m         etag\u001b[38;5;241m=\u001b[39metag,\n\u001b[1;32m   1181\u001b[0m         xet_file_data\u001b[38;5;241m=\u001b[39mxet_file_data,\n\u001b[1;32m   1182\u001b[0m     )\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[1;32m   1184\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1723\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_xet_available():\n\u001b[1;32m   1722\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXet Storage is enabled for this repo. Downloading file from Xet Storage..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1723\u001b[0m     xet_get(\n\u001b[1;32m   1724\u001b[0m         incomplete_path\u001b[38;5;241m=\u001b[39mincomplete_path,\n\u001b[1;32m   1725\u001b[0m         xet_file_data\u001b[38;5;241m=\u001b[39mxet_file_data,\n\u001b[1;32m   1726\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1727\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[1;32m   1728\u001b[0m         displayed_filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m   1729\u001b[0m     )\n\u001b[1;32m   1730\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1731\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m constants\u001b[38;5;241m.\u001b[39mHF_HUB_DISABLE_XET:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:624\u001b[0m, in \u001b[0;36mxet_get\u001b[0;34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001b[0m\n\u001b[1;32m    613\u001b[0m     displayed_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplayed_filename[:\u001b[38;5;241m40\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(…)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    615\u001b[0m progress_cm \u001b[38;5;241m=\u001b[39m _get_progress_bar_context(\n\u001b[1;32m    616\u001b[0m     desc\u001b[38;5;241m=\u001b[39mdisplayed_filename,\n\u001b[1;32m    617\u001b[0m     log_level\u001b[38;5;241m=\u001b[39mlogger\u001b[38;5;241m.\u001b[39mgetEffectiveLevel(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    621\u001b[0m     _tqdm_bar\u001b[38;5;241m=\u001b[39m_tqdm_bar,\n\u001b[1;32m    622\u001b[0m )\n\u001b[0;32m--> 624\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m progress_cm \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprogress_updater\u001b[39m(progress_bytes: \u001b[38;5;28mfloat\u001b[39m):\n\u001b[1;32m    627\u001b[0m         progress\u001b[38;5;241m.\u001b[39mupdate(progress_bytes)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tqdm/std.py:1138\u001b[0m, in \u001b[0;36mtqdm.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m-> 1138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_value, traceback):\n\u001b[1;32m   1139\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1140\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def _to_sft(example):\n",
    "    instr = example.get(\"instruction\", \"\")\n",
    "    inp = example.get(\"input\", \"\")\n",
    "    output = example.get(\"output\", None)\n",
    "    prompt = (instr + (\"\\n\" + inp if inp else \"\")).strip()\n",
    "    return {\"prompt\": prompt, \"response\": output}\n",
    "\n",
    "# SFT：抽样加载 BELLE 中文指令数据\n",
    "sft_ds = load_dataset(\"BelleGroup/train_0.5M_CN\", split=\"train[:2000]\")\n",
    "sft_ds = sft_ds.map(_to_sft, remove_columns=sft_ds.column_names)\n",
    "print(\"SFT 条数:\", len(sft_ds))\n",
    "print(\"SFT 样本示例:\", sft_ds[0])\n",
    "\n",
    "# 偏好数据：UltraFeedback（用于 DPO/RM）优先使用官方版本\n",
    "\n",
    "def load_ultrafeedback(max_rows=\"20000\"):\n",
    "    try:\n",
    "        ds = load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\", split=f\"train_prefs[:{max_rows}]\")\n",
    "    except Exception:\n",
    "        ds = load_dataset(\"argilla/ultrafeedback-binarized-preferences\", split=f\"train[:{max_rows}]\")\n",
    "    return ds\n",
    "\n",
    "pref = load_ultrafeedback()\n",
    "\n",
    "# 统一映射到 (prompt, y_pos, y_neg)\n",
    "\n",
    "def _to_pref(ex):\n",
    "    prompt = ex.get(\"prompt\") or ex.get(\"question\") or ex.get(\"instruction\") or ex.get(\"input\")\n",
    "    y_pos = ex.get(\"chosen\") or ex.get(\"better_response\") or ex.get(\"pos\") or ex.get(\"preferred\")\n",
    "    y_neg = ex.get(\"rejected\") or ex.get(\"worse_response\") or ex.get(\"neg\") or ex.get(\"other\")\n",
    "    return {\"prompt\": prompt, \"y_pos\": y_pos, \"y_neg\": y_neg}\n",
    "\n",
    "pref = pref.map(_to_pref)\n",
    "pref = pref.filter(lambda e: isinstance(e[\"prompt\"], str) and isinstance(e[\"y_pos\"], str) and isinstance(e[\"y_neg\"], str))\n",
    "\n",
    "print(\"偏好条数:\", len(pref))\n",
    "if len(pref) > 0:\n",
    "    sample = pref[0]\n",
    "    preview = {k: (sample[k][:120] + \"...\") for k in [\"prompt\", \"y_pos\", \"y_neg\"]}\n",
    "    print(\"偏好样本示例:\", preview)\n",
    "else:\n",
    "    print(\"[Warn] 偏好数据为空，请增大抽样量或检查数据集网络可用性。可尝试将 max_rows 提升到 '50000'。\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f63aa6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
