





# 检测硬件环境
import platform
import torch
import sys

print("🖥️ 硬件环境检测")
print("="*50)
print(f"操作系统: {platform.system()}")
print(f"Python版本: {sys.version.split()[0]}")
print(f"PyTorch版本: {torch.__version__}")

# 设备检测 (苹果芯片优化)
if torch.backends.mps.is_available():
    device = torch.device("mps")
    print("🍎 使用Apple Silicon MPS加速")
elif torch.cuda.is_available():
    device = torch.device("cuda")
    print(f"🔥 使用CUDA加速: {torch.cuda.get_device_name()}")
    print(f"   显存: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
else:
    device = torch.device("cpu")
    print("💻 使用CPU")

print(f"🚀 选择设备: {device}")



# 安装必要的依赖包
import subprocess
import sys

def install_package(package):
    subprocess.check_call([sys.executable, "-m", "pip", "install", package])

# 核心依赖
packages = [
    "torch", "torchvision", "torchaudio",
    "numpy", "pandas", "matplotlib", "seaborn", 
    "scikit-learn", "tqdm", "requests"
]

print("📦 安装依赖包...")
for package in packages:
    try:
        __import__(package)
        print(f"✅ {package} 已安装")
    except ImportError:
        print(f"📥 安装 {package}...")
        install_package(package)

print("🎉 环境配置完成！")






# 导入必要的库
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import requests
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# 设置matplotlib中文字体
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False

print("📚 库导入完成！")






class NYCTaxiDataProcessor:
    """NYC出租车数据处理器"""
    
    def __init__(self, data_dir="./data"):
        self.data_dir = data_dir
        os.makedirs(data_dir, exist_ok=True)
    
    def download_nyc_taxi_data(self, year=2016, month=1):
        """下载NYC出租车数据"""
        print(f"📥 下载NYC出租车数据: {year}年{month}月...")
        
        # 检查CSV文件是否已存在
        csv_file = f"yellow_tripdata_{year}-{month:02d}.csv"
        csv_path = os.path.join(self.data_dir, csv_file)
        
        if os.path.exists(csv_path):
            print(f"📁 CSV文件已存在: {csv_path}")
            return csv_path
        
        # 检查Parquet文件是否已存在
        parquet_file = f"yellow_tripdata_{year}-{month:02d}.parquet"
        parquet_path = os.path.join(self.data_dir, parquet_file)
        
        if os.path.exists(parquet_path):
            print(f"📁 Parquet文件已存在: {parquet_path}")
            # 转换为CSV
            return self.convert_parquet_to_csv(parquet_path)
        
        # 下载Parquet文件
        url = f"https://d37ci6vzurychx.cloudfront.net/trip-data/{parquet_file}"
        
        try:
            print(f"🌐 从 {url} 下载数据...")
            
            response = requests.get(url, stream=True, timeout=60)
            response.raise_for_status()
            
            # 显示下载进度
            total_size = int(response.headers.get('content-length', 0))
            if total_size > 0:
                print(f"📊 文件大小: {total_size / 1024 / 1024:.1f} MB")
            
            with open(parquet_path, 'wb') as f:
                for chunk in tqdm(response.iter_content(chunk_size=8192), 
                                total=total_size//8192 if total_size > 0 else None, 
                                desc="下载进度", 
                                unit="KB"):
                    f.write(chunk)
            
            print(f"✅ 下载完成: {parquet_path}")
            print(f"📊 实际文件大小: {os.path.getsize(parquet_path) / 1024 / 1024:.1f} MB")
            
            # 转换为CSV
            return self.convert_parquet_to_csv(parquet_path)
            
        except Exception as e:
            print(f"❌ 下载失败: {e}")
            return None
    
    def convert_parquet_to_csv(self, parquet_path):
        """将Parquet文件转换为CSV"""
        try:
            import pandas as pd
            
            csv_path = parquet_path.replace('.parquet', '.csv')
            
            print(f"🔄 转换Parquet文件为CSV: {parquet_path}")
            
            # 读取Parquet文件
            df = pd.read_parquet(parquet_path)
            print(f"📊 数据形状: {df.shape}")
            print(f"📋 列名: {list(df.columns)}")
            
            # 保存为CSV
            df.to_csv(csv_path, index=False)
            
            print(f"✅ 转换完成: {csv_path}")
            print(f"📊 CSV文件大小: {os.path.getsize(csv_path) / 1024 / 1024:.1f} MB")
            
            return csv_path
            
        except ImportError:
            print("❌ 需要安装pandas和pyarrow: pip install pandas pyarrow")
            return None
        except Exception as e:
            print(f"❌ 转换失败: {e}")
            return None
    

    
    def _load_taxi_zone_centroids(self):
        """下载/读取Taxi Zones GeoJSON，返回LocationID→(lon, lat)的质心映射"""
        try:
            zones_path = os.path.join(self.data_dir, 'taxi_zones.geojson')
            if not os.path.exists(zones_path):
                url = "https://d37ci6vzurychx.cloudfront.net/misc/taxi_zones.geojson"
                print(f"📥 下载Taxi Zones: {url}")
                r = requests.get(url, timeout=60)
                r.raise_for_status()
                with open(zones_path, 'wb') as f:
                    f.write(r.content)
            import json
            from shapely.geometry import shape
            with open(zones_path, 'r') as f:
                gj = json.load(f)
            id2centroid = {}
            for feat in gj['features']:
                props = feat['properties']
                loc_id = int(props.get('location_id', props.get('LocationID')))
                geom = shape(feat['geometry'])
                x, y = geom.representative_point().coords[0]  # lon, lat
                id2centroid[loc_id] = (x, y)
            return id2centroid
        except Exception as e:
            print(f"❌ Taxi Zones处理失败: {e}")
            return None

    def load_and_clean_data(self, file_path):
        """加载并清洗NYC出租车数据（只依赖PULocationID/DOLocationID，不再尝试经纬度）"""
        print(f"📖 加载并清洗数据: {file_path}")
        try:
            print("📊 读取CSV文件(分区Schema)...")
            usecols = [
                'tpep_pickup_datetime','tpep_dropoff_datetime',
                'PULocationID','DOLocationID',
                'passenger_count','trip_distance'
            ]
            df = pd.read_csv(file_path, usecols=usecols)

            # 统一时间列名
            df = df.rename(columns={
                'tpep_pickup_datetime':'pickup_datetime',
                'tpep_dropoff_datetime':'dropoff_datetime'
            })

            print(f"   原始数据量: {len(df):,} 条记录")
            print("🧹 数据清洗...")
            df = df.dropna()
            df = df[df['passenger_count'] > 0]
            df = df[df['trip_distance'] > 0]
            df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])
            df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'])

            # 不再基于经纬度裁剪区域，直接按区聚合
            df = df.sort_values('pickup_datetime').reset_index(drop=True)
            print("✅ 数据清洗完成 (按区聚合)")
            return df
        except Exception as e:
            print(f"❌ 数据加载失败: {e}")
            return None
    
    def process_nyc_taxi_to_zones(self, csv_path, time_interval='30T'):
        """将NYC出租车CSV数据转换为'分区'空间格式（无需经纬度）
        输出形状: [T, C=2, H=1, W=num_zones]
        """
        print(f"🔄 将NYC出租车数据转换为分区空间格式...")
        # 读取必要列（新Schema）
        usecols = [
            'tpep_pickup_datetime','tpep_dropoff_datetime',
            'PULocationID','DOLocationID'
        ]
        df = pd.read_csv(csv_path, usecols=usecols)
        df = df.rename(columns={
            'tpep_pickup_datetime':'pickup_datetime',
            'tpep_dropoff_datetime':'dropoff_datetime'
        })
        df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])
        # 生成时间槽
        df['time_slot'] = df['pickup_datetime'].dt.floor(time_interval)
        time_slots = sorted(df['time_slot'].unique())

        # 构建分区集合
        zones = sorted(set(df['PULocationID'].unique()).union(set(df['DOLocationID'].unique())))
        zone_to_idx = {z:i for i,z in enumerate(zones)}
        S = len(zones)
        print(f"📍 分区数量: {S}")

        # 聚合到 [T, 2, 1, S]
        grid_data = []
        for ts in tqdm(time_slots, desc="处理时间槽"):
            slot = df[df['time_slot']==ts]
            inflow = np.zeros((S,), dtype=np.float32)
            outflow = np.zeros((S,), dtype=np.float32)
            pc = slot['PULocationID'].value_counts()
            dc = slot['DOLocationID'].value_counts()
            for z, c in pc.items():
                inflow[zone_to_idx[int(z)]] = c
            for z, c in dc.items():
                outflow[zone_to_idx[int(z)]] = c
            frame = np.stack([inflow, outflow], axis=0)  # [2, S]
            frame = frame.reshape(2, 1, S)               # [2, 1, S]
            grid_data.append(frame)

        grid_data = np.stack(grid_data, axis=0)  # [T, 2, 1, S]
        height, width = 1, S
        save_path = os.path.join(self.data_dir, f'nyc_taxi_zones_{width}.npz')
        np.savez_compressed(save_path,
                            data=grid_data,
                            time_slots=time_slots,
                            grid_info={'height': height, 'width': width, 'zones': zones})
        print(f"✅ 分区数据已保存: {save_path}")
        print(f"   数据形状: {grid_data.shape}")
        return grid_data, save_path

# 执行NYC出租车数据下载和处理
print("🚕 开始下载和处理NYC出租车数据...")
processor = NYCTaxiDataProcessor()

# 下载NYC出租车数据 (2016年1月，约144MB Parquet文件)
print("\n📥 步骤1: 下载真实NYC出租车数据")
csv_path = processor.download_nyc_taxi_data(year=2016, month=1)

zones_geojson_path = os.path.join("./spatiotemporal-forecasting/data", "NYC Taxi Zones_20250922.geojson")
if not os.path.exists(zones_geojson_path):
    # 兼容放在./data/的情况
    alt_path = os.path.join("./data", "taxi_zones.geojson")
    if os.path.exists(alt_path):
        zones_geojson_path = alt_path

if csv_path:
    # 加载并清洗数据（按分区Schema）
    print("\n🧹 步骤2: 加载并清洗数据 (按分区)")
    df = processor.load_and_clean_data(csv_path)
    
    if df is not None:
        # 分区聚合
        print("\n🗺️ 步骤3: 转换为分区空间格式（按PULocationID/DOLocationID）")
        zones_data, zones_npz_path = processor.process_nyc_taxi_to_zones(csv_path)
        
        # 若提供了GeoJSON，则将分区投影到32x32规则栅格，便于热力图展示
        if os.path.exists(zones_geojson_path):
            print("\n🗺️ 步骤4: 分区→32×32栅格投影（用于可视化）")
            import json
            import numpy as np
            from shapely.geometry import shape, Polygon, MultiPolygon, Point

            # 加载分区文件
            with open(zones_geojson_path, 'r') as f:
                gj = json.load(f)
            # 读取分区ID列表
            loaded = np.load(zones_npz_path, allow_pickle=True)
            zones = loaded['grid_info'].item().get('zones')
            zones_set = set(int(z) for z in zones)

            # 建立 LocationID -> shapely geometry
            id2geom = {}
            for feat in gj['features']:
                props = feat['properties']
                loc_id = props.get('locationid') or props.get('location_id') or props.get('LocationID')
                if loc_id is None:
                    continue
                try:
                    loc_id = int(loc_id)
                except Exception:
                    continue
                if loc_id in zones_set:
                    geom = shape(feat['geometry'])
                    id2geom[loc_id] = geom

            # 计算全局边界（仅使用存在于本数据集的分区）
            all_bounds = [g.bounds for g in id2geom.values()]
            if len(all_bounds) == 0:
                print("❌ GeoJSON中找不到与本CSV对应的分区ID，跳过栅格化")
                grid_path = zones_npz_path
            else:
                minx = min(b[0] for b in all_bounds); miny = min(b[1] for b in all_bounds)
                maxx = max(b[2] for b in all_bounds); maxy = max(b[3] for b in all_bounds)

                H, W = 32, 32
                xs = np.linspace(minx, maxx, W+1)
                ys = np.linspace(miny, maxy, H+1)
                # 预计算每个网格像元的中心点
                cx = (xs[:-1] + xs[1:]) / 2
                cy = (ys[:-1] + ys[1:]) / 2
                # shapely点测试
                def zone_mask(geom):
                    mask = np.zeros((H, W), dtype=bool)
                    for iy, y in enumerate(cy):
                        for ix, x in enumerate(cx):
                            if geom.contains(Point(x, y)) or geom.touches(Point(x, y)):
                                mask[iy, ix] = True
                    return mask

                # 为每个分区生成掩膜
                zone_to_mask = {}
                for zid, geom in id2geom.items():
                    zone_to_mask[zid] = zone_mask(geom)

                data_zones = loaded['data']  # [T, 2, 1, S]
                T = data_zones.shape[0]; S = data_zones.shape[3]
                zones_list = list(zones)
                # 生成 [T, 2, H, W]
                grid = np.zeros((T, 2, H, W), dtype=np.float32)
                for s, zid in enumerate(zones_list):
                    zid = int(zid)
                    mask = zone_to_mask.get(zid)
                    if mask is None:
                        continue
                    # 将该分区的数值复制到其掩膜覆盖的网格像元
                    # 上客量
                    grid[:, 0][..., mask] += data_zones[:, 0, 0, s][:, None]
                    # 下客量
                    grid[:, 1][..., mask] += data_zones[:, 1, 0, s][:, None]

                # 保存新的规则栅格数据
                grid_path = os.path.join(processor.data_dir, f'nyc_taxi_grid_32x32.npz')
                np.savez_compressed(grid_path,
                                    data=grid,
                                    time_slots=loaded['time_slots'],
                                    grid_info={'height': H, 'width': W, 'bbox':[minx, miny, maxx, maxy]})
                print(f"✅ 规则栅格数据已保存: {grid_path}")
        else:
            grid_path = zones_npz_path

        print(f"\n🎉 NYC出租车数据处理完成！")
        print(f"📊 最终数据形状文件: {grid_path}")
        print("✅ 使用真实的NYC出租车数据（分区聚合，若提供GeoJSON则栅格化为32×32）")
    else:
        print("❌ 数据加载失败，请检查文件格式")
else:
    print("❌ 数据下载失败")
    print("💡 请检查网络连接或手动下载数据")






# 分区→32×32 栅格化（使用提供的 GeoJSON），并保存为 ./data/nyc_taxi_grid_32x32.npz
import os, json, numpy as np

zones_geojson_path = os.path.join("./data", "Zones_20250922.geojson")
if not os.path.exists(zones_geojson_path):
    alt_path = os.path.join("./data", "taxi_zones.geojson")
    if os.path.exists(alt_path):
        zones_geojson_path = alt_path

zones_npz_path = os.path.join("./data", "nyc_taxi_zones_262.npz")
print("\n🧭 栅格化准备：")
print(f"   • 分区NPZ: {zones_npz_path} ({'存在' if os.path.exists(zones_npz_path) else '不存在'})")
print(f"   • GeoJSON: {zones_geojson_path} ({'存在' if os.path.exists(zones_geojson_path) else '不存在'})")

if not os.path.exists(zones_npz_path):
    raise FileNotFoundError("未找到分区聚合的NPZ，请先运行数据下载与分区聚合步骤")
if not os.path.exists(zones_geojson_path):
    raise FileNotFoundError("未找到GeoJSON，请将完整 taxi zones GeoJSON 放置到上述路径")

# 依赖：shapely
try:
    from shapely.geometry import shape, Point
except Exception:
    import sys, subprocess
    print("📥 安装 shapely...")
    subprocess.check_call([sys.executable, "-m", "pip", "install", "shapely"])
    from shapely.geometry import shape, Point

# 加载GeoJSON
with open(zones_geojson_path, 'r') as f:
    gj = json.load(f)
features = gj.get('features', [])
print(f"   • GeoJSON要素数: {len(features)}")

# 加载分区数据
loaded = np.load(zones_npz_path, allow_pickle=True)
data_zones = loaded['data']             # [T, 2, 1, S]
time_slots = loaded['time_slots']
info = loaded['grid_info'].item() if hasattr(loaded['grid_info'], 'item') else dict(loaded['grid_info'])
zones_list = info.get('zones')
if zones_list is None:
    raise KeyError("grid_info 中缺少 'zones' 列表，无法建立 LocationID 映射")

zones_set = set(int(z) for z in zones_list)

# 建立 LocationID -> geometry 映射
id2geom = {}
for feat in features:
    props = feat.get('properties', {})
    loc_id = props.get('locationid') or props.get('location_id') or props.get('LocationID')
    if loc_id is None:
        continue
    try:
        loc_id = int(loc_id)
    except Exception:
        continue
    if loc_id in zones_set:
        id2geom[loc_id] = shape(feat['geometry'])

print(f"   • 匹配到的分区几何: {len(id2geom)} / {len(zones_set)}")
if len(id2geom) == 0:
    raise RuntimeError("GeoJSON 中未匹配到任何与 NPZ 对应的分区ID，请确认GeoJSON是否完整")

# 计算整体边界
all_bounds = [g.bounds for g in id2geom.values()]
minx = min(b[0] for b in all_bounds); miny = min(b[1] for b in all_bounds)
maxx = max(b[2] for b in all_bounds); maxy = max(b[3] for b in all_bounds)

H, W = 32, 32
xs = np.linspace(minx, maxx, W+1); ys = np.linspace(miny, maxy, H+1)
cx = (xs[:-1] + xs[1:]) / 2
cy = (ys[:-1] + ys[1:]) / 2

# 为每个分区生成掩膜（像元中心点落入或接触多边形）
from functools import lru_cache

@lru_cache(maxsize=None)
def pt(x, y):
    return Point(float(x), float(y))

zone_to_mask = {}
for zid, geom in id2geom.items():
    mask = np.zeros((H, W), dtype=bool)
    for iy, y in enumerate(cy):
        for ix, x in enumerate(cx):
            p = pt(x, y)
            if geom.contains(p) or geom.touches(p):
                mask[iy, ix] = True
    zone_to_mask[zid] = mask

# 将分区时间序列铺到栅格
T = data_zones.shape[0]
grid = np.zeros((T, 2, H, W), dtype=np.float32)
for s, zid in enumerate(zones_list):
    zid = int(zid)
    mask = zone_to_mask.get(zid)
    if mask is None:
        continue
    # 将分区量复制到其掩膜覆盖的像元
    grid[:, 0][..., mask] += data_zones[:, 0, 0, s][:, None]
    grid[:, 1][..., mask] += data_zones[:, 1, 0, s][:, None]

# 保存结果
grid_path = os.path.join("./data", "nyc_taxi_grid_32x32.npz")
np.savez_compressed(
    grid_path,
    data=grid,
    time_slots=time_slots,
    grid_info={'height': H, 'width': W, 'bbox': [float(minx), float(miny), float(maxx), float(maxy)]}
)
print(f"✅ 规则栅格数据已保存: {grid_path}")



# 如果存在32x32栅格数据，展示平均输入输出值的空间分布
grid_32x32_path = os.path.join("./data", "nyc_taxi_grid_32x32.npz")
if os.path.exists(grid_32x32_path):
    print("🗺️ 展示32×32栅格的平均输入输出值分布...")
    
    # 加载32x32栅格数据
    grid_data = np.load(grid_32x32_path, allow_pickle=True)
    grid_array = grid_data['data']  # [T, 2, 32, 32]
    grid_info_32x32 = grid_data['grid_info'].item()
    
    # 计算平均输入输出值
    avg_pickup = np.mean(grid_array[:, 0, :, :], axis=0)  # 平均上客量 [32, 32]
    avg_dropoff = np.mean(grid_array[:, 1, :, :], axis=0)  # 平均下客量 [32, 32]
    
    # 创建可视化
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    
    # 上客量热力图
    im1 = axes[0].imshow(avg_pickup, cmap='Reds', aspect='equal')
    axes[0].set_title('平均上客量分布 (32×32栅格)', fontsize=14, fontweight='bold')
    axes[0].set_xlabel('经度方向 (网格)')
    axes[0].set_ylabel('纬度方向 (网格)')
    plt.colorbar(im1, ax=axes[0], label='平均上客量')
    
    # 下客量热力图
    im2 = axes[1].imshow(avg_dropoff, cmap='Blues', aspect='equal')
    axes[1].set_title('平均下客量分布 (32×32栅格)', fontsize=14, fontweight='bold')
    axes[1].set_xlabel('经度方向 (网格)')
    axes[1].set_ylabel('纬度方向 (网格)')
    plt.colorbar(im2, ax=axes[1], label='平均下客量')
    
    # 总流量热力图 (上客+下客)
    total_flow = avg_pickup + avg_dropoff
    im3 = axes[2].imshow(total_flow, cmap='viridis', aspect='equal')
    axes[2].set_title('平均总流量分布 (上客+下客)', fontsize=14, fontweight='bold')
    axes[2].set_xlabel('经度方向 (网格)')
    axes[2].set_ylabel('纬度方向 (网格)')
    plt.colorbar(im3, ax=axes[2], label='平均总流量')
    
    plt.tight_layout()
    plt.show()
    
    # 统计信息
    print(f"\n📈 32×32栅格统计信息:")
    print(f"   • 上客量: 平均={avg_pickup.mean():.2f}, 最大={avg_pickup.max():.2f}, 最小={avg_pickup.min():.2f}")
    print(f"   • 下客量: 平均={avg_dropoff.mean():.2f}, 最大={avg_dropoff.max():.2f}, 最小={avg_dropoff.min():.2f}")
    print(f"   • 总流量: 平均={total_flow.mean():.2f}, 最大={total_flow.max():.2f}, 最小={total_flow.min():.2f}")
    
    # 找出热点区域 (前5个最高流量的网格)
    flat_total = total_flow.flatten()
    top5_indices = np.argsort(flat_total)[-5:][::-1]
    top5_coords = [(idx // 32, idx % 32) for idx in top5_indices]
    
    print(f"\n🔥 流量热点区域 (前5名):")
    for i, (y, x) in enumerate(top5_coords):
        pickup_val = avg_pickup[y, x]
        dropoff_val = avg_dropoff[y, x]
        total_val = total_flow[y, x]
        print(f"   {i+1}. 网格({y:2d},{x:2d}): 上客={pickup_val:.1f}, 下客={dropoff_val:.1f}, 总计={total_val:.1f}")
else:
    print("❌ 未找到32×32栅格数据文件")
    print("💡 请先运行数据下载和处理步骤，确保生成了 nyc_taxi_grid_32x32.npz 文件")



# 加载并展示NYC出租车网格数据
def visualize_taxi_data(data_path):
    """可视化NYC出租车时空数据"""
    
    # 加载数据
    loaded = np.load(data_path, allow_pickle=True)
    data = loaded['data']  # [T, C, H, W]
    time_slots = loaded['time_slots']
    grid_info = loaded['grid_info'].item()
    
    print(f"📊 NYC出租车网格数据分析")
    print(f"   数据形状: {data.shape}")
    print(f"   时间步数: {data.shape[0]} (每30分钟一个时间步)")
    print(f"   通道数: {data.shape[1]} (上客量 + 下客量)")
    print(f"   网格大小: {data.shape[2]} × {data.shape[3]}")
    print(f"   时间范围: {time_slots[0]} 到 {time_slots[-1]}")
    
    # 数据统计
    print(f"\n📈 数据统计:")
    print(f"   最大流量: {data.max():.0f}")
    print(f"   平均流量: {data.mean():.2f}")
    print(f"   标准差: {data.std():.2f}")
    print(f"   非零比例: {(data > 0).mean()*100:.1f}%")
    
    return data, time_slots, grid_info



def visualize_data_overview(data, time_slots, grid_info):
    """数据概览可视化"""
    print("📊 数据概览可视化...")
    
    # 1. 静态时间点对比
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    
    # 选择几个时间点进行可视化
    time_indices = [0, len(data)//4, len(data)//2, 3*len(data)//4, len(data)-1]
    
    for i, t_idx in enumerate(time_indices[:3]):
        # 上客量
        im1 = axes[0, i].imshow(data[t_idx, 0], cmap='Reds', interpolation='nearest')
        axes[0, i].set_title(f'上客量 - {time_slots[t_idx].strftime("%m-%d %H:%M")}')
        axes[0, i].axis('off')
        plt.colorbar(im1, ax=axes[0, i], fraction=0.046, pad=0.04)
        
        # 下客量
        im2 = axes[1, i].imshow(data[t_idx, 1], cmap='Blues', interpolation='nearest')
        axes[1, i].set_title(f'下客量 - {time_slots[t_idx].strftime("%m-%d %H:%M")}')
        axes[1, i].axis('off')
        plt.colorbar(im2, ax=axes[1, i], fraction=0.046, pad=0.04)
    
    plt.suptitle('NYC出租车流量时空分布对比', fontsize=16)
    plt.tight_layout()
    plt.show()
    
    # 2. 时间序列分析
    plt.figure(figsize=(15, 8))
    
    # 分区一维格式：选取若干分区索引进行时间序列可视化
    S = data.shape[3]
    zone_indices = [0, max(1, S//4), max(2, S//2), max(3, (3*S)//4), S-1]
    
    plt.subplot(2, 2, 1)
    for idx in zone_indices:
        pickup_ts = data[:, 0, 0, idx]
        plt.plot(pickup_ts, label=f'区{idx}', alpha=0.8, linewidth=1)
    plt.title('不同分区的上客量时间序列')
    plt.xlabel('时间步 (每30分钟)')
    plt.ylabel('上客量')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(2, 2, 2)
    for idx in zone_indices:
        dropoff_ts = data[:, 1, 0, idx]
        plt.plot(dropoff_ts, label=f'区{idx}', alpha=0.8, linewidth=1)
    plt.title('不同分区的下客量时间序列')
    plt.xlabel('时间步 (每30分钟)')
    plt.ylabel('下客量')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 全局平均流量
    plt.subplot(2, 2, 3)
    global_pickup = data[:, 0].mean(axis=(1, 2))
    global_dropoff = data[:, 1].mean(axis=(1, 2))
    
    plt.plot(global_pickup, label='平均上客量', color='red', alpha=0.7, linewidth=2)
    plt.plot(global_dropoff, label='平均下客量', color='blue', alpha=0.7, linewidth=2)
    plt.title('全局平均客流量变化')
    plt.xlabel('时间步 (每30分钟)')
    plt.ylabel('平均客流量')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 客流量分布直方图
    plt.subplot(2, 2, 4)
    all_pickup = data[:, 0].flatten()
    all_dropoff = data[:, 1].flatten()
    
    plt.hist(all_pickup[all_pickup > 0], bins=50, alpha=0.7, label='上客量', color='red', density=True)
    plt.hist(all_dropoff[all_dropoff > 0], bins=50, alpha=0.7, label='下客量', color='blue', density=True)
    plt.title('客流量分布直方图')
    plt.xlabel('客流量值')
    plt.ylabel('密度')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.yscale('log')
    
    plt.tight_layout()
    plt.show()
    
    # 3. 空间统计
    plt.figure(figsize=(12, 5))
    
    # 分区一维的空间统计：按分区索引绘制均值曲线
    plt.subplot(1, 2, 1)
    mean_pickup = data[:, 0, 0, :].mean(axis=0)
    plt.plot(mean_pickup, color='red')
    plt.title('平均上客量（按分区）')
    plt.xlabel('分区索引'); plt.ylabel('上客量')
    
    plt.subplot(1, 2, 2)
    mean_dropoff = data[:, 1, 0, :].mean(axis=0)
    plt.plot(mean_dropoff, color='blue')
    plt.title('平均下客量（按分区）')
    plt.xlabel('分区索引'); plt.ylabel('下客量')
    
    plt.suptitle('空间平均客流量分布', fontsize=16)
    plt.tight_layout()
    plt.show()

# 检查grid_path是否存在，如果不存在则使用默认路径
try:
    if 'grid_path' not in locals():
        # 尝试从data目录加载
        data_dir = "./data"
        grid_path = os.path.join(data_dir, 'nyc_taxi_real_32x32.npz')
        
        if not os.path.exists(grid_path):
            print("❌ 网格数据文件不存在，请先运行数据下载和处理步骤")
            print(f"   期望文件路径: {grid_path}")
        else:
            print(f"📁 找到网格数据文件: {grid_path}")
    
    # 加载和可视化数据
    data, time_slots, grid_info = visualize_taxi_data(grid_path)
    
    # 数据概览
    print("\n📊 数据概览...")
    visualize_data_overview(data, time_slots, grid_info)
    
except NameError as e:
    print(f"❌ 错误: {e}")
    print("💡 请先运行数据下载和处理步骤 (Cell 7)")
except FileNotFoundError as e:
    print(f"❌ 文件未找到: {e}")
    print("💡 请先运行数据下载和处理步骤 (Cell 7)")






class SpatialPatchTST(nn.Module):
    """
    向量化的空间PatchTST：
    - 将空间维(H*W)合并进批次，一次性进行时间patch + Transformer编码
    - 显著减少Python循环与kernel launch开销
    """
    def __init__(self, 
                 seq_len=12,
                 pred_len=6,
                 channels=2,
                 height=32,
                 width=32,
                 patch_size=4,
                 d_model=128,
                 n_heads=8,
                 n_layers=3,
                 dropout=0.1):
        super().__init__()
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.channels = channels
        self.height = height
        self.width = width
        self.patch_size = patch_size
        self.d_model = d_model
        self.spatial_size = height * width
        self.d_s = d_model // 4  # 空间投影后的通道维度

        # 预计算patch数（按seq_len整除patch_size的上取整）
        self.n_patches = (seq_len + patch_size - 1) // patch_size

        # 空间通道升维
        self.spatial_proj = nn.Linear(channels, self.d_s)

        # 时间patch嵌入（线性）：[patch_size*d_s] → [d_model]
        self.patch_embedding = nn.Linear(patch_size * self.d_s, d_model)

        # 位置编码
        self.pos_encoding = nn.Parameter(torch.randn(1, self.n_patches, d_model))       # 时间patch位置编码
        self.spatial_pos = nn.Parameter(torch.randn(1, self.spatial_size, self.d_s))     # 空间位置编码

        # Transformer编码器
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=n_heads,
            dim_feedforward=d_model * 2,
            dropout=dropout,
            activation='gelu',
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)

        # 预测头 + 输出投影
        self.predictor = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, pred_len * self.d_s)
        )
        self.output_proj = nn.Linear(self.d_s, channels)

        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)

    def forward(self, x):
        """
        x: [B, T, C, H, W]
        return: [B, pred_len, C, H, W]
        """
        B, T, C, H, W = x.shape
        d_s = self.d_s
        S = H * W

        # 确保输入张量在MPS设备上使用正确的内存格式
        if x.device.type == 'mps':
            x = x.contiguous()

        # 重排为 [B, T, H*W, C] 并做空间通道升维
        x = x.permute(0, 1, 3, 4, 2).reshape(B, T, S, C)             # [B, T, S, C]
        x = self.spatial_proj(x)                                      # [B, T, S, d_s]
        x = x + self.spatial_pos                                      # + [1, S, d_s] 广播到 [B, T, S, d_s]

        # 合并空间到批次：一次性处理所有网格的时间序列
        x = x.permute(0, 2, 1, 3).reshape(B * S, T, d_s)              # [B*S, T, d_s]

        # 时间维padding到patch对齐
        if T % self.patch_size != 0:
            pad_len = self.patch_size - (T % self.patch_size)
            # 在时间维末尾pad
            x = F.pad(x, (0, 0, 0, pad_len))                           # [B*S, T_pad, d_s]
            T_padded = T + pad_len
        else:
            T_padded = T

        # 生成非重叠patch: [B*S, n_patches, patch_size, d_s]
        n_patches = T_padded // self.patch_size
        x = x.view(B * S, n_patches, self.patch_size, d_s)
        # 合并patch维度做线性嵌入: [B*S, n_patches, patch_size*d_s] → [B*S, n_patches, d_model]
        patches = self.patch_embedding(x.reshape(B * S, n_patches, self.patch_size * d_s))

        # 添加时间位置编码
        patches = patches + self.pos_encoding[:, :n_patches, :]

        # 确保Transformer输入是连续的
        if patches.device.type == 'mps':
            patches = patches.contiguous()

        # Transformer编码
        encoded = self.transformer(patches)                            # [B*S, n_patches, d_model]

        # 取最后一个patch的表示并做预测
        last_feat = encoded[:, -1, :]                                  # [B*S, d_model]
        pred = self.predictor(last_feat)                               # [B*S, pred_len * d_s]
        pred = pred.view(B * S, self.pred_len, d_s)                    # [B*S, pred_len, d_s]

        # 输出投影到通道C，并恢复空间
        pred = self.output_proj(pred)                                  # [B*S, pred_len, C]
        pred = pred.view(B, S, self.pred_len, C).permute(0, 2, 3, 1)   # [B, pred_len, C, S]
        pred = pred.view(B, self.pred_len, C, H, W)                    # [B, pred_len, C, H, W]
        
        # 确保输出张量是连续的
        if pred.device.type == 'mps':
            pred = pred.contiguous()
            
        return pred



# 使用向量化模型进行快速前向与训练
print("🧠 创建快速版SpatialPatchTSTFast模型...")
model = SpatialPatchTST(
    seq_len=12,
    pred_len=6,
    channels=2,
    height=32,
    width=32,
    patch_size=4,
    d_model=64,     # 轻量配置
    n_heads=4,
    n_layers=2,
    dropout=0.1
).to(device)

# 模型信息
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
model_size_mb = total_params * 4 / 1024 / 1024

print(f"✅ 快速模型创建完成！")
print(f"   总参数量: {total_params:,}")
print(f"   可训练参数: {trainable_params:,}")
print(f"   模型大小: {model_size_mb:.2f} MB")
print(f"   设备: {device}")

# 快速前向测试
print(f"\n🧪 测试快速模型前向传播...")
x_test = torch.randn(2, 12, 2, 32, 32).to(device)
y_test = model(x_test)
print(f"   输入形状: {x_test.shape}")
print(f"   输出形状: {y_test.shape}")
print(f"   前向传播成功！✅")






class NYCTaxiDataset(Dataset):
    """NYC出租车空间数据集"""
    
    def __init__(self, data_path, seq_len=12, pred_len=6, train_ratio=0.7, val_ratio=0.2, split='train'):
        """
        Args:
            data_path: 数据文件路径
            seq_len: 输入序列长度 (默认12 = 6小时)
            pred_len: 预测序列长度 (默认6 = 3小时)
            train_ratio: 训练集比例
            val_ratio: 验证集比例
            split: 'train', 'val', 'test'
        """
        # 加载数据
        loaded = np.load(data_path, allow_pickle=True)
        data = loaded['data'].astype(np.float32)  # [T, C, H, W]
        
        self.seq_len = seq_len
        self.pred_len = pred_len
        
        # 数据标准化
        self.data_mean = data.mean()
        self.data_std = data.std()
        data = (data - self.data_mean) / (self.data_std + 1e-8)
        
        # 数据集划分
        total_samples = len(data) - seq_len - pred_len + 1
        train_size = int(total_samples * train_ratio)
        val_size = int(total_samples * val_ratio)
        
        if split == 'train':
            self.data = data[:train_size + seq_len + pred_len - 1]
            self.start_idx = 0
            self.end_idx = train_size
        elif split == 'val':
            self.data = data[train_size:train_size + val_size + seq_len + pred_len - 1]
            self.start_idx = 0
            self.end_idx = val_size
        else:  # test
            self.data = data[train_size + val_size:]
            self.start_idx = 0
            self.end_idx = len(self.data) - seq_len - pred_len + 1
        
        print(f"📊 {split.upper()}数据集:")
        print(f"   数据形状: {self.data.shape}")
        print(f"   样本数量: {self.end_idx - self.start_idx}")
        print(f"   输入长度: {seq_len} (代表{seq_len*0.5:.1f}小时)")
        print(f"   预测长度: {pred_len} (代表{pred_len*0.5:.1f}小时)")
    
    def __len__(self):
        return self.end_idx - self.start_idx
    
    def __getitem__(self, idx):
        actual_idx = self.start_idx + idx
        x = self.data[actual_idx:actual_idx + self.seq_len]
        y = self.data[actual_idx + self.seq_len:actual_idx + self.seq_len + self.pred_len]
        return torch.tensor(x), torch.tensor(y)

# 创建数据集
print("📊 创建数据集...")
train_dataset = NYCTaxiDataset(grid_path, seq_len=12, pred_len=6, split='train')
val_dataset = NYCTaxiDataset(grid_path, seq_len=12, pred_len=6, split='val')
test_dataset = NYCTaxiDataset(grid_path, seq_len=12, pred_len=6, split='test')

# 创建数据加载器
batch_size = 16
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)

print(f"\n✅ 数据加载器创建完成！")
print(f"   训练批次数: {len(train_loader)}")
print(f"   验证批次数: {len(val_loader)}")
print(f"   测试批次数: {len(test_loader)}")
print(f"   批次大小: {batch_size}")

# 检查一个批次的数据
sample_x, sample_y = next(iter(train_loader))
print(f"\n🔍 样本检查:")
print(f"   输入形状: {sample_x.shape}")  # [batch, seq_len, channels, height, width]
print(f"   输出形状: {sample_y.shape}")  # [batch, pred_len, channels, height, width]






class TrafficPredictor:
    """交通预测器"""
    
    def __init__(self, model, device=device):
        self.model = model.to(device)
        self.device = device
        self.train_losses = []
        self.val_losses = []
    
    def train_epoch(self, train_loader, optimizer, criterion):
        """训练一个epoch"""
        self.model.train()
        total_loss = 0
        
        for batch_x, batch_y in train_loader:
            # 确保张量在MPS设备上使用正确的内存格式
            batch_x = batch_x.to(self.device, non_blocking=True)
            batch_y = batch_y.to(self.device, non_blocking=True)
            
            # 确保张量是连续的（解决MPS channels_last格式问题）
            if self.device.type == 'mps':
                batch_x = batch_x.contiguous()
                batch_y = batch_y.contiguous()
            
            optimizer.zero_grad()
            
            # 前向传播
            pred = self.model(batch_x)
            loss = criterion(pred, batch_y)
            
            # 反向传播
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        return total_loss / len(train_loader)
    
    def validate(self, val_loader, criterion):
        """验证"""
        self.model.eval()
        total_loss = 0
        
        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                # 确保张量在MPS设备上使用正确的内存格式
                batch_x = batch_x.to(self.device, non_blocking=True)
                batch_y = batch_y.to(self.device, non_blocking=True)
                
                # 确保张量是连续的（解决MPS channels_last格式问题）
                if self.device.type == 'mps':
                    batch_x = batch_x.contiguous()
                    batch_y = batch_y.contiguous()
                
                pred = self.model(batch_x)
                loss = criterion(pred, batch_y)
                
                total_loss += loss.item()
        
        return total_loss / len(val_loader)
    
    def train(self, train_loader, val_loader, epochs=30, lr=1e-3, patience=8):
        """完整训练流程"""
        print(f"\n🚀 开始训练交通预测模型")
        print(f"📱 使用设备: {self.device}")
        print(f"🧠 模型参数量: {sum(p.numel() for p in self.model.parameters()):,}")
        
        # 业务目标说明
        print(f"\n🎯 业务目标:")
        print(f"   输入: 过去6小时的交通流量分布")
        print(f"   输出: 未来3小时的交通流量分布") 
        print(f"   应用: 出租车调度优化，司机导航建议")
        
        # 优化器和损失函数
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=1e-4)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
        criterion = nn.MSELoss()
        
        # 早停机制
        best_val_loss = float('inf')
        patience_counter = 0
        
        # 训练循环
        print(f"\n📈 开始训练...")
        for epoch in tqdm(range(epochs), desc="训练进度"):
            # 训练
            train_loss = self.train_epoch(train_loader, optimizer, criterion)
            
            # 验证
            val_loss = self.validate(val_loader, criterion)
            
            # 学习率调度
            scheduler.step()
            
            # 记录损失
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            
            # 早停检查
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                # 保存最佳模型
                torch.save(self.model.state_dict(), 'best_traffic_model.pth')
            else:
                patience_counter += 1
            
            # 打印进度
            if (epoch + 1) % 10 == 0:
                print(f"\\nEpoch {epoch+1}/{epochs}")
                print(f"训练损失: {train_loss:.6f}")
                print(f"验证损失: {val_loss:.6f}")
                print(f"学习率: {scheduler.get_last_lr()[0]:.6f}")
            
            # 早停
            if patience_counter >= patience:
                print(f"\\n🛑 早停触发，在第{epoch+1}轮停止训练")
                break
        
        print(f"✅ 训练完成！最佳验证损失: {best_val_loss:.6f}")
        
        # 加载最佳模型
        self.model.load_state_dict(torch.load('best_traffic_model.pth'))
        
        return self.train_losses, self.val_losses

# 创建训练器
predictor = TrafficPredictor(model, device)

# 开始训练
train_losses, val_losses = predictor.train(
    train_loader, val_loader,
    epochs=30, lr=1e-3, patience=8
)






# 绘制训练历史
print(f"📈 绘制训练历史...")
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(train_losses, label='训练损失', alpha=0.8)
plt.plot(val_losses, label='验证损失', alpha=0.8)
plt.title('训练损失曲线')
plt.xlabel('Epoch')
plt.ylabel('MSE Loss')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
epochs_to_show = min(20, len(train_losses))
plt.plot(train_losses[-epochs_to_show:], label='训练损失 (最后20轮)', alpha=0.8)
plt.plot(val_losses[-epochs_to_show:], label='验证损失 (最后20轮)', alpha=0.8)
plt.title('训练损失曲线 (最后20轮)')
plt.xlabel('Epoch')
plt.ylabel('MSE Loss')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()



# 测试预测并可视化结果
def visualize_predictions(predictor, test_loader, dataset_stats):
    """可视化预测结果"""
    print("📊 可视化预测结果...")
    
    predictor.model.eval()
    
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x = batch_x.to(predictor.device)
            batch_y = batch_y.to(predictor.device)
            
            pred = predictor.model(batch_x)
            
            # 反标准化
            data_mean, data_std = dataset_stats
            batch_x = batch_x * data_std + data_mean
            batch_y = batch_y * data_std + data_mean
            pred = pred * data_std + data_mean
            
            # 转换为numpy
            batch_x = batch_x.cpu().numpy()
            batch_y = batch_y.cpu().numpy()
            pred = pred.cpu().numpy()
            
            break
    
    # 可视化
    sample_idx = 0
    fig, axes = plt.subplots(3, 4, figsize=(16, 12))
    
    # 输入序列的最后一帧
    axes[0, 0].imshow(batch_x[sample_idx, -1, 0], cmap='Reds')
    axes[0, 0].set_title('输入最后帧 - 流入量')
    axes[0, 0].axis('off')
    
    axes[0, 1].imshow(batch_x[sample_idx, -1, 1], cmap='Blues')
    axes[0, 1].set_title('输入最后帧 - 流出量')
    axes[0, 1].axis('off')
    
    # 真实的预测目标
    axes[1, 0].imshow(batch_y[sample_idx, 0, 0], cmap='Reds')
    axes[1, 0].set_title('真实值第1帧 - 流入量')
    axes[1, 0].axis('off')
    
    axes[1, 1].imshow(batch_y[sample_idx, 0, 1], cmap='Blues')
    axes[1, 1].set_title('真实值第1帧 - 流出量')
    axes[1, 1].axis('off')
    
    # 模型预测结果
    axes[2, 0].imshow(pred[sample_idx, 0, 0], cmap='Reds')
    axes[2, 0].set_title('预测值第1帧 - 流入量')
    axes[2, 0].axis('off')
    
    axes[2, 1].imshow(pred[sample_idx, 0, 1], cmap='Blues')
    axes[2, 1].set_title('预测值第1帧 - 流出量')
    axes[2, 1].axis('off')
    
    # 误差分析
    error_in = np.abs(batch_y[sample_idx, 0, 0] - pred[sample_idx, 0, 0])
    error_out = np.abs(batch_y[sample_idx, 0, 1] - pred[sample_idx, 0, 1])
    
    axes[0, 2].imshow(error_in, cmap='Reds')
    axes[0, 2].set_title('流入量预测误差')
    axes[0, 2].axis('off')
    
    axes[1, 2].imshow(error_out, cmap='Blues')
    axes[1, 2].set_title('流出量预测误差')
    axes[1, 2].axis('off')
    
    # 时间序列对比 (选择一个活跃位置)
    pos_y, pos_x = 16, 16  # 中心位置
    
    input_ts_in = batch_x[sample_idx, :, 0, pos_y, pos_x]
    target_ts_in = batch_y[sample_idx, :, 0, pos_y, pos_x]
    pred_ts_in = pred[sample_idx, :, 0, pos_y, pos_x]
    
    full_time = np.concatenate([input_ts_in, target_ts_in])
    pred_time = np.concatenate([input_ts_in, pred_ts_in])
    
    axes[0, 3].plot(range(len(input_ts_in)), input_ts_in, 'g-', label='历史', linewidth=2)
    axes[0, 3].plot(range(len(input_ts_in), len(full_time)), target_ts_in, 'b-', label='真实', linewidth=2)
    axes[0, 3].plot(range(len(input_ts_in), len(pred_time)), pred_ts_in, 'r--', label='预测', linewidth=2)
    axes[0, 3].axvline(x=len(input_ts_in), color='black', linestyle=':', alpha=0.7)
    axes[0, 3].set_title(f'位置({pos_y},{pos_x}) 流入量时序')
    axes[0, 3].legend()
    axes[0, 3].grid(True, alpha=0.3)
    
    # 类似的流出量时序
    input_ts_out = batch_x[sample_idx, :, 1, pos_y, pos_x]
    target_ts_out = batch_y[sample_idx, :, 1, pos_y, pos_x]
    pred_ts_out = pred[sample_idx, :, 1, pos_y, pos_x]
    
    full_time_out = np.concatenate([input_ts_out, target_ts_out])
    pred_time_out = np.concatenate([input_ts_out, pred_ts_out])
    
    axes[1, 3].plot(range(len(input_ts_out)), input_ts_out, 'g-', label='历史', linewidth=2)
    axes[1, 3].plot(range(len(input_ts_out), len(full_time_out)), target_ts_out, 'b-', label='真实', linewidth=2)
    axes[1, 3].plot(range(len(input_ts_out), len(pred_time_out)), pred_ts_out, 'r--', label='预测', linewidth=2)
    axes[1, 3].axvline(x=len(input_ts_out), color='black', linestyle=':', alpha=0.7)
    axes[1, 3].set_title(f'位置({pos_y},{pos_x}) 流出量时序')
    axes[1, 3].legend()
    axes[1, 3].grid(True, alpha=0.3)
    
    # 业务解释
    axes[2, 2].text(0.1, 0.8, '🎯 业务价值:', transform=axes[2, 2].transAxes, fontsize=12, weight='bold')
    axes[2, 2].text(0.1, 0.6, '• 预测未来3小时需求', transform=axes[2, 2].transAxes, fontsize=10)
    axes[2, 2].text(0.1, 0.4, '• 优化司机调度', transform=axes[2, 2].transAxes, fontsize=10)
    axes[2, 2].text(0.1, 0.2, '• 减少空驶时间', transform=axes[2, 2].transAxes, fontsize=10)
    axes[2, 2].axis('off')
    
    axes[2, 3].text(0.1, 0.8, '📊 模型输出:', transform=axes[2, 3].transAxes, fontsize=12, weight='bold')
    axes[2, 3].text(0.1, 0.6, f'• 预测时长: {pred.shape[1]*0.5:.1f}小时', transform=axes[2, 3].transAxes, fontsize=10)
    axes[2, 3].text(0.1, 0.4, f'• 空间分辨率: {pred.shape[3]}×{pred.shape[4]}', transform=axes[2, 3].transAxes, fontsize=10)
    axes[2, 3].text(0.1, 0.2, f'• 通道数: {pred.shape[2]}', transform=axes[2, 3].transAxes, fontsize=10)
    axes[2, 3].axis('off')
    
    plt.suptitle('NYC出租车流量预测结果', fontsize=16)
    plt.tight_layout()
    plt.show()
    
    # 计算预测误差
    mse = np.mean((pred - batch_y)**2)
    mae = np.mean(np.abs(pred - batch_y))
    
    print(f"\\n📊 预测性能:")
    print(f"   MSE: {mse:.6f}")
    print(f"   MAE: {mae:.6f}")
    print(f"   相对误差: {mae/np.mean(batch_y)*100:.2f}%")
    
    return mse, mae

# 执行预测可视化
dataset_stats = (train_dataset.data_mean, train_dataset.data_std)
mse, mae = visualize_predictions(predictor, test_loader, dataset_stats)

# 业务价值分析
print(f"\\n💼 业务价值分析:")
print(f"   📈 预测准确度: MAE = {mae:.3f} (每网格每30分钟误差{mae:.1f}辆车)")
print(f"   ⏰ 预测时间范围: 未来3小时")
print(f"   🗺️ 空间覆盖: 整个曼哈顿32×32网格")
print(f"   💰 商业应用:")
print(f"      - 司机导航: 推荐高需求区域")
print(f"      - 调度优化: 提前调配车辆")
print(f"      - 动态定价: 根据预测需求调价")
print(f"      - 运力规划: 合理安排班次")




