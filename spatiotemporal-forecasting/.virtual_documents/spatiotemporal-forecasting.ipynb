





# æ£€æµ‹ç¡¬ä»¶ç¯å¢ƒ
import platform
import torch
import sys

print("ğŸ–¥ï¸ ç¡¬ä»¶ç¯å¢ƒæ£€æµ‹")
print("="*50)
print(f"æ“ä½œç³»ç»Ÿ: {platform.system()}")
print(f"Pythonç‰ˆæœ¬: {sys.version.split()[0]}")
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")

# è®¾å¤‡æ£€æµ‹ (è‹¹æœèŠ¯ç‰‡ä¼˜åŒ–)
if torch.backends.mps.is_available():
    device = torch.device("mps")
    print("ğŸ ä½¿ç”¨Apple Silicon MPSåŠ é€Ÿ")
elif torch.cuda.is_available():
    device = torch.device("cuda")
    print(f"ğŸ”¥ ä½¿ç”¨CUDAåŠ é€Ÿ: {torch.cuda.get_device_name()}")
    print(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
else:
    device = torch.device("cpu")
    print("ğŸ’» ä½¿ç”¨CPU")

print(f"ğŸš€ é€‰æ‹©è®¾å¤‡: {device}")



# å®‰è£…å¿…è¦çš„ä¾èµ–åŒ…
import subprocess
import sys

def install_package(package):
    subprocess.check_call([sys.executable, "-m", "pip", "install", package])

# æ ¸å¿ƒä¾èµ–
packages = [
    "torch", "torchvision", "torchaudio",
    "numpy", "pandas", "matplotlib", "seaborn", 
    "scikit-learn", "tqdm", "requests"
]

print("ğŸ“¦ å®‰è£…ä¾èµ–åŒ…...")
for package in packages:
    try:
        __import__(package)
        print(f"âœ… {package} å·²å®‰è£…")
    except ImportError:
        print(f"ğŸ“¥ å®‰è£… {package}...")
        install_package(package)

print("ğŸ‰ ç¯å¢ƒé…ç½®å®Œæˆï¼")






# å¯¼å…¥å¿…è¦çš„åº“
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import requests
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False

print("ğŸ“š åº“å¯¼å…¥å®Œæˆï¼")






class NYCTaxiDataProcessor:
    """NYCå‡ºç§Ÿè½¦æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, data_dir="./data"):
        self.data_dir = data_dir
        os.makedirs(data_dir, exist_ok=True)
    
    def download_nyc_taxi_data(self, year=2016, month=1):
        """ä¸‹è½½NYCå‡ºç§Ÿè½¦æ•°æ®"""
        print(f"ğŸ“¥ ä¸‹è½½NYCå‡ºç§Ÿè½¦æ•°æ®: {year}å¹´{month}æœˆ...")
        
        # æ£€æŸ¥CSVæ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        csv_file = f"yellow_tripdata_{year}-{month:02d}.csv"
        csv_path = os.path.join(self.data_dir, csv_file)
        
        if os.path.exists(csv_path):
            print(f"ğŸ“ CSVæ–‡ä»¶å·²å­˜åœ¨: {csv_path}")
            return csv_path
        
        # æ£€æŸ¥Parquetæ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        parquet_file = f"yellow_tripdata_{year}-{month:02d}.parquet"
        parquet_path = os.path.join(self.data_dir, parquet_file)
        
        if os.path.exists(parquet_path):
            print(f"ğŸ“ Parquetæ–‡ä»¶å·²å­˜åœ¨: {parquet_path}")
            # è½¬æ¢ä¸ºCSV
            return self.convert_parquet_to_csv(parquet_path)
        
        # ä¸‹è½½Parquetæ–‡ä»¶
        url = f"https://d37ci6vzurychx.cloudfront.net/trip-data/{parquet_file}"
        
        try:
            print(f"ğŸŒ ä» {url} ä¸‹è½½æ•°æ®...")
            
            response = requests.get(url, stream=True, timeout=60)
            response.raise_for_status()
            
            # æ˜¾ç¤ºä¸‹è½½è¿›åº¦
            total_size = int(response.headers.get('content-length', 0))
            if total_size > 0:
                print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {total_size / 1024 / 1024:.1f} MB")
            
            with open(parquet_path, 'wb') as f:
                for chunk in tqdm(response.iter_content(chunk_size=8192), 
                                total=total_size//8192 if total_size > 0 else None, 
                                desc="ä¸‹è½½è¿›åº¦", 
                                unit="KB"):
                    f.write(chunk)
            
            print(f"âœ… ä¸‹è½½å®Œæˆ: {parquet_path}")
            print(f"ğŸ“Š å®é™…æ–‡ä»¶å¤§å°: {os.path.getsize(parquet_path) / 1024 / 1024:.1f} MB")
            
            # è½¬æ¢ä¸ºCSV
            return self.convert_parquet_to_csv(parquet_path)
            
        except Exception as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
            return None
    
    def convert_parquet_to_csv(self, parquet_path):
        """å°†Parquetæ–‡ä»¶è½¬æ¢ä¸ºCSV"""
        try:
            import pandas as pd
            
            csv_path = parquet_path.replace('.parquet', '.csv')
            
            print(f"ğŸ”„ è½¬æ¢Parquetæ–‡ä»¶ä¸ºCSV: {parquet_path}")
            
            # è¯»å–Parquetæ–‡ä»¶
            df = pd.read_parquet(parquet_path)
            print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {df.shape}")
            print(f"ğŸ“‹ åˆ—å: {list(df.columns)}")
            
            # ä¿å­˜ä¸ºCSV
            df.to_csv(csv_path, index=False)
            
            print(f"âœ… è½¬æ¢å®Œæˆ: {csv_path}")
            print(f"ğŸ“Š CSVæ–‡ä»¶å¤§å°: {os.path.getsize(csv_path) / 1024 / 1024:.1f} MB")
            
            return csv_path
            
        except ImportError:
            print("âŒ éœ€è¦å®‰è£…pandaså’Œpyarrow: pip install pandas pyarrow")
            return None
        except Exception as e:
            print(f"âŒ è½¬æ¢å¤±è´¥: {e}")
            return None
    

    
    def _load_taxi_zone_centroids(self):
        """ä¸‹è½½/è¯»å–Taxi Zones GeoJSONï¼Œè¿”å›LocationIDâ†’(lon, lat)çš„è´¨å¿ƒæ˜ å°„"""
        try:
            zones_path = os.path.join(self.data_dir, 'taxi_zones.geojson')
            if not os.path.exists(zones_path):
                url = "https://d37ci6vzurychx.cloudfront.net/misc/taxi_zones.geojson"
                print(f"ğŸ“¥ ä¸‹è½½Taxi Zones: {url}")
                r = requests.get(url, timeout=60)
                r.raise_for_status()
                with open(zones_path, 'wb') as f:
                    f.write(r.content)
            import json
            from shapely.geometry import shape
            with open(zones_path, 'r') as f:
                gj = json.load(f)
            id2centroid = {}
            for feat in gj['features']:
                props = feat['properties']
                loc_id = int(props.get('location_id', props.get('LocationID')))
                geom = shape(feat['geometry'])
                x, y = geom.representative_point().coords[0]  # lon, lat
                id2centroid[loc_id] = (x, y)
            return id2centroid
        except Exception as e:
            print(f"âŒ Taxi Zoneså¤„ç†å¤±è´¥: {e}")
            return None

    def load_and_clean_data(self, file_path):
        """åŠ è½½å¹¶æ¸…æ´—NYCå‡ºç§Ÿè½¦æ•°æ®ï¼ˆåªä¾èµ–PULocationID/DOLocationIDï¼Œä¸å†å°è¯•ç»çº¬åº¦ï¼‰"""
        print(f"ğŸ“– åŠ è½½å¹¶æ¸…æ´—æ•°æ®: {file_path}")
        try:
            print("ğŸ“Š è¯»å–CSVæ–‡ä»¶(åˆ†åŒºSchema)...")
            usecols = [
                'tpep_pickup_datetime','tpep_dropoff_datetime',
                'PULocationID','DOLocationID',
                'passenger_count','trip_distance'
            ]
            df = pd.read_csv(file_path, usecols=usecols)

            # ç»Ÿä¸€æ—¶é—´åˆ—å
            df = df.rename(columns={
                'tpep_pickup_datetime':'pickup_datetime',
                'tpep_dropoff_datetime':'dropoff_datetime'
            })

            print(f"   åŸå§‹æ•°æ®é‡: {len(df):,} æ¡è®°å½•")
            print("ğŸ§¹ æ•°æ®æ¸…æ´—...")
            df = df.dropna()
            df = df[df['passenger_count'] > 0]
            df = df[df['trip_distance'] > 0]
            df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])
            df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'])

            # ä¸å†åŸºäºç»çº¬åº¦è£å‰ªåŒºåŸŸï¼Œç›´æ¥æŒ‰åŒºèšåˆ
            df = df.sort_values('pickup_datetime').reset_index(drop=True)
            print("âœ… æ•°æ®æ¸…æ´—å®Œæˆ (æŒ‰åŒºèšåˆ)")
            return df
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return None
    
    def process_nyc_taxi_to_zones(self, csv_path, time_interval='30T'):
        """å°†NYCå‡ºç§Ÿè½¦CSVæ•°æ®è½¬æ¢ä¸º'åˆ†åŒº'ç©ºé—´æ ¼å¼ï¼ˆæ— éœ€ç»çº¬åº¦ï¼‰
        è¾“å‡ºå½¢çŠ¶: [T, C=2, H=1, W=num_zones]
        """
        print(f"ğŸ”„ å°†NYCå‡ºç§Ÿè½¦æ•°æ®è½¬æ¢ä¸ºåˆ†åŒºç©ºé—´æ ¼å¼...")
        # è¯»å–å¿…è¦åˆ—ï¼ˆæ–°Schemaï¼‰
        usecols = [
            'tpep_pickup_datetime','tpep_dropoff_datetime',
            'PULocationID','DOLocationID'
        ]
        df = pd.read_csv(csv_path, usecols=usecols)
        df = df.rename(columns={
            'tpep_pickup_datetime':'pickup_datetime',
            'tpep_dropoff_datetime':'dropoff_datetime'
        })
        df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])
        # ç”Ÿæˆæ—¶é—´æ§½
        df['time_slot'] = df['pickup_datetime'].dt.floor(time_interval)
        time_slots = sorted(df['time_slot'].unique())

        # æ„å»ºåˆ†åŒºé›†åˆ
        zones = sorted(set(df['PULocationID'].unique()).union(set(df['DOLocationID'].unique())))
        zone_to_idx = {z:i for i,z in enumerate(zones)}
        S = len(zones)
        print(f"ğŸ“ åˆ†åŒºæ•°é‡: {S}")

        # èšåˆåˆ° [T, 2, 1, S]
        grid_data = []
        for ts in tqdm(time_slots, desc="å¤„ç†æ—¶é—´æ§½"):
            slot = df[df['time_slot']==ts]
            inflow = np.zeros((S,), dtype=np.float32)
            outflow = np.zeros((S,), dtype=np.float32)
            pc = slot['PULocationID'].value_counts()
            dc = slot['DOLocationID'].value_counts()
            for z, c in pc.items():
                inflow[zone_to_idx[int(z)]] = c
            for z, c in dc.items():
                outflow[zone_to_idx[int(z)]] = c
            frame = np.stack([inflow, outflow], axis=0)  # [2, S]
            frame = frame.reshape(2, 1, S)               # [2, 1, S]
            grid_data.append(frame)

        grid_data = np.stack(grid_data, axis=0)  # [T, 2, 1, S]
        height, width = 1, S
        save_path = os.path.join(self.data_dir, f'nyc_taxi_zones_{width}.npz')
        np.savez_compressed(save_path,
                            data=grid_data,
                            time_slots=time_slots,
                            grid_info={'height': height, 'width': width, 'zones': zones})
        print(f"âœ… åˆ†åŒºæ•°æ®å·²ä¿å­˜: {save_path}")
        print(f"   æ•°æ®å½¢çŠ¶: {grid_data.shape}")
        return grid_data, save_path

# æ‰§è¡ŒNYCå‡ºç§Ÿè½¦æ•°æ®ä¸‹è½½å’Œå¤„ç†
print("ğŸš• å¼€å§‹ä¸‹è½½å’Œå¤„ç†NYCå‡ºç§Ÿè½¦æ•°æ®...")
processor = NYCTaxiDataProcessor()

# ä¸‹è½½NYCå‡ºç§Ÿè½¦æ•°æ® (2016å¹´1æœˆï¼Œçº¦144MB Parquetæ–‡ä»¶)
print("\nğŸ“¥ æ­¥éª¤1: ä¸‹è½½çœŸå®NYCå‡ºç§Ÿè½¦æ•°æ®")
csv_path = processor.download_nyc_taxi_data(year=2016, month=1)

zones_geojson_path = os.path.join("./spatiotemporal-forecasting/data", "NYC Taxi Zones_20250922.geojson")
if not os.path.exists(zones_geojson_path):
    # å…¼å®¹æ”¾åœ¨./data/çš„æƒ…å†µ
    alt_path = os.path.join("./data", "taxi_zones.geojson")
    if os.path.exists(alt_path):
        zones_geojson_path = alt_path

if csv_path:
    # åŠ è½½å¹¶æ¸…æ´—æ•°æ®ï¼ˆæŒ‰åˆ†åŒºSchemaï¼‰
    print("\nğŸ§¹ æ­¥éª¤2: åŠ è½½å¹¶æ¸…æ´—æ•°æ® (æŒ‰åˆ†åŒº)")
    df = processor.load_and_clean_data(csv_path)
    
    if df is not None:
        # åˆ†åŒºèšåˆ
        print("\nğŸ—ºï¸ æ­¥éª¤3: è½¬æ¢ä¸ºåˆ†åŒºç©ºé—´æ ¼å¼ï¼ˆæŒ‰PULocationID/DOLocationIDï¼‰")
        zones_data, zones_npz_path = processor.process_nyc_taxi_to_zones(csv_path)
        
        # è‹¥æä¾›äº†GeoJSONï¼Œåˆ™å°†åˆ†åŒºæŠ•å½±åˆ°32x32è§„åˆ™æ …æ ¼ï¼Œä¾¿äºçƒ­åŠ›å›¾å±•ç¤º
        if os.path.exists(zones_geojson_path):
            print("\nğŸ—ºï¸ æ­¥éª¤4: åˆ†åŒºâ†’32Ã—32æ …æ ¼æŠ•å½±ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰")
            import json
            import numpy as np
            from shapely.geometry import shape, Polygon, MultiPolygon, Point

            # åŠ è½½åˆ†åŒºæ–‡ä»¶
            with open(zones_geojson_path, 'r') as f:
                gj = json.load(f)
            # è¯»å–åˆ†åŒºIDåˆ—è¡¨
            loaded = np.load(zones_npz_path, allow_pickle=True)
            zones = loaded['grid_info'].item().get('zones')
            zones_set = set(int(z) for z in zones)

            # å»ºç«‹ LocationID -> shapely geometry
            id2geom = {}
            for feat in gj['features']:
                props = feat['properties']
                loc_id = props.get('locationid') or props.get('location_id') or props.get('LocationID')
                if loc_id is None:
                    continue
                try:
                    loc_id = int(loc_id)
                except Exception:
                    continue
                if loc_id in zones_set:
                    geom = shape(feat['geometry'])
                    id2geom[loc_id] = geom

            # è®¡ç®—å…¨å±€è¾¹ç•Œï¼ˆä»…ä½¿ç”¨å­˜åœ¨äºæœ¬æ•°æ®é›†çš„åˆ†åŒºï¼‰
            all_bounds = [g.bounds for g in id2geom.values()]
            if len(all_bounds) == 0:
                print("âŒ GeoJSONä¸­æ‰¾ä¸åˆ°ä¸æœ¬CSVå¯¹åº”çš„åˆ†åŒºIDï¼Œè·³è¿‡æ …æ ¼åŒ–")
                grid_path = zones_npz_path
            else:
                minx = min(b[0] for b in all_bounds); miny = min(b[1] for b in all_bounds)
                maxx = max(b[2] for b in all_bounds); maxy = max(b[3] for b in all_bounds)

                H, W = 32, 32
                xs = np.linspace(minx, maxx, W+1)
                ys = np.linspace(miny, maxy, H+1)
                # é¢„è®¡ç®—æ¯ä¸ªç½‘æ ¼åƒå…ƒçš„ä¸­å¿ƒç‚¹
                cx = (xs[:-1] + xs[1:]) / 2
                cy = (ys[:-1] + ys[1:]) / 2
                # shapelyç‚¹æµ‹è¯•
                def zone_mask(geom):
                    mask = np.zeros((H, W), dtype=bool)
                    for iy, y in enumerate(cy):
                        for ix, x in enumerate(cx):
                            if geom.contains(Point(x, y)) or geom.touches(Point(x, y)):
                                mask[iy, ix] = True
                    return mask

                # ä¸ºæ¯ä¸ªåˆ†åŒºç”Ÿæˆæ©è†œ
                zone_to_mask = {}
                for zid, geom in id2geom.items():
                    zone_to_mask[zid] = zone_mask(geom)

                data_zones = loaded['data']  # [T, 2, 1, S]
                T = data_zones.shape[0]; S = data_zones.shape[3]
                zones_list = list(zones)
                # ç”Ÿæˆ [T, 2, H, W]
                grid = np.zeros((T, 2, H, W), dtype=np.float32)
                for s, zid in enumerate(zones_list):
                    zid = int(zid)
                    mask = zone_to_mask.get(zid)
                    if mask is None:
                        continue
                    # å°†è¯¥åˆ†åŒºçš„æ•°å€¼å¤åˆ¶åˆ°å…¶æ©è†œè¦†ç›–çš„ç½‘æ ¼åƒå…ƒ
                    # ä¸Šå®¢é‡
                    grid[:, 0][..., mask] += data_zones[:, 0, 0, s][:, None]
                    # ä¸‹å®¢é‡
                    grid[:, 1][..., mask] += data_zones[:, 1, 0, s][:, None]

                # ä¿å­˜æ–°çš„è§„åˆ™æ …æ ¼æ•°æ®
                grid_path = os.path.join(processor.data_dir, f'nyc_taxi_grid_32x32.npz')
                np.savez_compressed(grid_path,
                                    data=grid,
                                    time_slots=loaded['time_slots'],
                                    grid_info={'height': H, 'width': W, 'bbox':[minx, miny, maxx, maxy]})
                print(f"âœ… è§„åˆ™æ …æ ¼æ•°æ®å·²ä¿å­˜: {grid_path}")
        else:
            grid_path = zones_npz_path

        print(f"\nğŸ‰ NYCå‡ºç§Ÿè½¦æ•°æ®å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“Š æœ€ç»ˆæ•°æ®å½¢çŠ¶æ–‡ä»¶: {grid_path}")
        print("âœ… ä½¿ç”¨çœŸå®çš„NYCå‡ºç§Ÿè½¦æ•°æ®ï¼ˆåˆ†åŒºèšåˆï¼Œè‹¥æä¾›GeoJSONåˆ™æ …æ ¼åŒ–ä¸º32Ã—32ï¼‰")
    else:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼")
else:
    print("âŒ æ•°æ®ä¸‹è½½å¤±è´¥")
    print("ğŸ’¡ è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æ‰‹åŠ¨ä¸‹è½½æ•°æ®")






# åˆ†åŒºâ†’32Ã—32 æ …æ ¼åŒ–ï¼ˆä½¿ç”¨æä¾›çš„ GeoJSONï¼‰ï¼Œå¹¶ä¿å­˜ä¸º ./data/nyc_taxi_grid_32x32.npz
import os, json, numpy as np

zones_geojson_path = os.path.join("./data", "Zones_20250922.geojson")
if not os.path.exists(zones_geojson_path):
    alt_path = os.path.join("./data", "taxi_zones.geojson")
    if os.path.exists(alt_path):
        zones_geojson_path = alt_path

zones_npz_path = os.path.join("./data", "nyc_taxi_zones_262.npz")
print("\nğŸ§­ æ …æ ¼åŒ–å‡†å¤‡ï¼š")
print(f"   â€¢ åˆ†åŒºNPZ: {zones_npz_path} ({'å­˜åœ¨' if os.path.exists(zones_npz_path) else 'ä¸å­˜åœ¨'})")
print(f"   â€¢ GeoJSON: {zones_geojson_path} ({'å­˜åœ¨' if os.path.exists(zones_geojson_path) else 'ä¸å­˜åœ¨'})")

if not os.path.exists(zones_npz_path):
    raise FileNotFoundError("æœªæ‰¾åˆ°åˆ†åŒºèšåˆçš„NPZï¼Œè¯·å…ˆè¿è¡Œæ•°æ®ä¸‹è½½ä¸åˆ†åŒºèšåˆæ­¥éª¤")
if not os.path.exists(zones_geojson_path):
    raise FileNotFoundError("æœªæ‰¾åˆ°GeoJSONï¼Œè¯·å°†å®Œæ•´ taxi zones GeoJSON æ”¾ç½®åˆ°ä¸Šè¿°è·¯å¾„")

# ä¾èµ–ï¼šshapely
try:
    from shapely.geometry import shape, Point
except Exception:
    import sys, subprocess
    print("ğŸ“¥ å®‰è£… shapely...")
    subprocess.check_call([sys.executable, "-m", "pip", "install", "shapely"])
    from shapely.geometry import shape, Point

# åŠ è½½GeoJSON
with open(zones_geojson_path, 'r') as f:
    gj = json.load(f)
features = gj.get('features', [])
print(f"   â€¢ GeoJSONè¦ç´ æ•°: {len(features)}")

# åŠ è½½åˆ†åŒºæ•°æ®
loaded = np.load(zones_npz_path, allow_pickle=True)
data_zones = loaded['data']             # [T, 2, 1, S]
time_slots = loaded['time_slots']
info = loaded['grid_info'].item() if hasattr(loaded['grid_info'], 'item') else dict(loaded['grid_info'])
zones_list = info.get('zones')
if zones_list is None:
    raise KeyError("grid_info ä¸­ç¼ºå°‘ 'zones' åˆ—è¡¨ï¼Œæ— æ³•å»ºç«‹ LocationID æ˜ å°„")

zones_set = set(int(z) for z in zones_list)

# å»ºç«‹ LocationID -> geometry æ˜ å°„
id2geom = {}
for feat in features:
    props = feat.get('properties', {})
    loc_id = props.get('locationid') or props.get('location_id') or props.get('LocationID')
    if loc_id is None:
        continue
    try:
        loc_id = int(loc_id)
    except Exception:
        continue
    if loc_id in zones_set:
        id2geom[loc_id] = shape(feat['geometry'])

print(f"   â€¢ åŒ¹é…åˆ°çš„åˆ†åŒºå‡ ä½•: {len(id2geom)} / {len(zones_set)}")
if len(id2geom) == 0:
    raise RuntimeError("GeoJSON ä¸­æœªåŒ¹é…åˆ°ä»»ä½•ä¸ NPZ å¯¹åº”çš„åˆ†åŒºIDï¼Œè¯·ç¡®è®¤GeoJSONæ˜¯å¦å®Œæ•´")

# è®¡ç®—æ•´ä½“è¾¹ç•Œ
all_bounds = [g.bounds for g in id2geom.values()]
minx = min(b[0] for b in all_bounds); miny = min(b[1] for b in all_bounds)
maxx = max(b[2] for b in all_bounds); maxy = max(b[3] for b in all_bounds)

H, W = 32, 32
xs = np.linspace(minx, maxx, W+1); ys = np.linspace(miny, maxy, H+1)
cx = (xs[:-1] + xs[1:]) / 2
cy = (ys[:-1] + ys[1:]) / 2

# ä¸ºæ¯ä¸ªåˆ†åŒºç”Ÿæˆæ©è†œï¼ˆåƒå…ƒä¸­å¿ƒç‚¹è½å…¥æˆ–æ¥è§¦å¤šè¾¹å½¢ï¼‰
from functools import lru_cache

@lru_cache(maxsize=None)
def pt(x, y):
    return Point(float(x), float(y))

zone_to_mask = {}
for zid, geom in id2geom.items():
    mask = np.zeros((H, W), dtype=bool)
    for iy, y in enumerate(cy):
        for ix, x in enumerate(cx):
            p = pt(x, y)
            if geom.contains(p) or geom.touches(p):
                mask[iy, ix] = True
    zone_to_mask[zid] = mask

# å°†åˆ†åŒºæ—¶é—´åºåˆ—é“ºåˆ°æ …æ ¼
T = data_zones.shape[0]
grid = np.zeros((T, 2, H, W), dtype=np.float32)
for s, zid in enumerate(zones_list):
    zid = int(zid)
    mask = zone_to_mask.get(zid)
    if mask is None:
        continue
    # å°†åˆ†åŒºé‡å¤åˆ¶åˆ°å…¶æ©è†œè¦†ç›–çš„åƒå…ƒ
    grid[:, 0][..., mask] += data_zones[:, 0, 0, s][:, None]
    grid[:, 1][..., mask] += data_zones[:, 1, 0, s][:, None]

# ä¿å­˜ç»“æœ
grid_path = os.path.join("./data", "nyc_taxi_grid_32x32.npz")
np.savez_compressed(
    grid_path,
    data=grid,
    time_slots=time_slots,
    grid_info={'height': H, 'width': W, 'bbox': [float(minx), float(miny), float(maxx), float(maxy)]}
)
print(f"âœ… è§„åˆ™æ …æ ¼æ•°æ®å·²ä¿å­˜: {grid_path}")



# å¦‚æœå­˜åœ¨32x32æ …æ ¼æ•°æ®ï¼Œå±•ç¤ºå¹³å‡è¾“å…¥è¾“å‡ºå€¼çš„ç©ºé—´åˆ†å¸ƒ
grid_32x32_path = os.path.join("./data", "nyc_taxi_grid_32x32.npz")
if os.path.exists(grid_32x32_path):
    print("ğŸ—ºï¸ å±•ç¤º32Ã—32æ …æ ¼çš„å¹³å‡è¾“å…¥è¾“å‡ºå€¼åˆ†å¸ƒ...")
    
    # åŠ è½½32x32æ …æ ¼æ•°æ®
    grid_data = np.load(grid_32x32_path, allow_pickle=True)
    grid_array = grid_data['data']  # [T, 2, 32, 32]
    grid_info_32x32 = grid_data['grid_info'].item()
    
    # è®¡ç®—å¹³å‡è¾“å…¥è¾“å‡ºå€¼
    avg_pickup = np.mean(grid_array[:, 0, :, :], axis=0)  # å¹³å‡ä¸Šå®¢é‡ [32, 32]
    avg_dropoff = np.mean(grid_array[:, 1, :, :], axis=0)  # å¹³å‡ä¸‹å®¢é‡ [32, 32]
    
    # åˆ›å»ºå¯è§†åŒ–
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    
    # ä¸Šå®¢é‡çƒ­åŠ›å›¾
    im1 = axes[0].imshow(avg_pickup, cmap='Reds', aspect='equal')
    axes[0].set_title('å¹³å‡ä¸Šå®¢é‡åˆ†å¸ƒ (32Ã—32æ …æ ¼)', fontsize=14, fontweight='bold')
    axes[0].set_xlabel('ç»åº¦æ–¹å‘ (ç½‘æ ¼)')
    axes[0].set_ylabel('çº¬åº¦æ–¹å‘ (ç½‘æ ¼)')
    plt.colorbar(im1, ax=axes[0], label='å¹³å‡ä¸Šå®¢é‡')
    
    # ä¸‹å®¢é‡çƒ­åŠ›å›¾
    im2 = axes[1].imshow(avg_dropoff, cmap='Blues', aspect='equal')
    axes[1].set_title('å¹³å‡ä¸‹å®¢é‡åˆ†å¸ƒ (32Ã—32æ …æ ¼)', fontsize=14, fontweight='bold')
    axes[1].set_xlabel('ç»åº¦æ–¹å‘ (ç½‘æ ¼)')
    axes[1].set_ylabel('çº¬åº¦æ–¹å‘ (ç½‘æ ¼)')
    plt.colorbar(im2, ax=axes[1], label='å¹³å‡ä¸‹å®¢é‡')
    
    # æ€»æµé‡çƒ­åŠ›å›¾ (ä¸Šå®¢+ä¸‹å®¢)
    total_flow = avg_pickup + avg_dropoff
    im3 = axes[2].imshow(total_flow, cmap='viridis', aspect='equal')
    axes[2].set_title('å¹³å‡æ€»æµé‡åˆ†å¸ƒ (ä¸Šå®¢+ä¸‹å®¢)', fontsize=14, fontweight='bold')
    axes[2].set_xlabel('ç»åº¦æ–¹å‘ (ç½‘æ ¼)')
    axes[2].set_ylabel('çº¬åº¦æ–¹å‘ (ç½‘æ ¼)')
    plt.colorbar(im3, ax=axes[2], label='å¹³å‡æ€»æµé‡')
    
    plt.tight_layout()
    plt.show()
    
    # ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“ˆ 32Ã—32æ …æ ¼ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   â€¢ ä¸Šå®¢é‡: å¹³å‡={avg_pickup.mean():.2f}, æœ€å¤§={avg_pickup.max():.2f}, æœ€å°={avg_pickup.min():.2f}")
    print(f"   â€¢ ä¸‹å®¢é‡: å¹³å‡={avg_dropoff.mean():.2f}, æœ€å¤§={avg_dropoff.max():.2f}, æœ€å°={avg_dropoff.min():.2f}")
    print(f"   â€¢ æ€»æµé‡: å¹³å‡={total_flow.mean():.2f}, æœ€å¤§={total_flow.max():.2f}, æœ€å°={total_flow.min():.2f}")
    
    # æ‰¾å‡ºçƒ­ç‚¹åŒºåŸŸ (å‰5ä¸ªæœ€é«˜æµé‡çš„ç½‘æ ¼)
    flat_total = total_flow.flatten()
    top5_indices = np.argsort(flat_total)[-5:][::-1]
    top5_coords = [(idx // 32, idx % 32) for idx in top5_indices]
    
    print(f"\nğŸ”¥ æµé‡çƒ­ç‚¹åŒºåŸŸ (å‰5å):")
    for i, (y, x) in enumerate(top5_coords):
        pickup_val = avg_pickup[y, x]
        dropoff_val = avg_dropoff[y, x]
        total_val = total_flow[y, x]
        print(f"   {i+1}. ç½‘æ ¼({y:2d},{x:2d}): ä¸Šå®¢={pickup_val:.1f}, ä¸‹å®¢={dropoff_val:.1f}, æ€»è®¡={total_val:.1f}")
else:
    print("âŒ æœªæ‰¾åˆ°32Ã—32æ …æ ¼æ•°æ®æ–‡ä»¶")
    print("ğŸ’¡ è¯·å…ˆè¿è¡Œæ•°æ®ä¸‹è½½å’Œå¤„ç†æ­¥éª¤ï¼Œç¡®ä¿ç”Ÿæˆäº† nyc_taxi_grid_32x32.npz æ–‡ä»¶")



# åŠ è½½å¹¶å±•ç¤ºNYCå‡ºç§Ÿè½¦ç½‘æ ¼æ•°æ®
def visualize_taxi_data(data_path):
    """å¯è§†åŒ–NYCå‡ºç§Ÿè½¦æ—¶ç©ºæ•°æ®"""
    
    # åŠ è½½æ•°æ®
    loaded = np.load(data_path, allow_pickle=True)
    data = loaded['data']  # [T, C, H, W]
    time_slots = loaded['time_slots']
    grid_info = loaded['grid_info'].item()
    
    print(f"ğŸ“Š NYCå‡ºç§Ÿè½¦ç½‘æ ¼æ•°æ®åˆ†æ")
    print(f"   æ•°æ®å½¢çŠ¶: {data.shape}")
    print(f"   æ—¶é—´æ­¥æ•°: {data.shape[0]} (æ¯30åˆ†é’Ÿä¸€ä¸ªæ—¶é—´æ­¥)")
    print(f"   é€šé“æ•°: {data.shape[1]} (ä¸Šå®¢é‡ + ä¸‹å®¢é‡)")
    print(f"   ç½‘æ ¼å¤§å°: {data.shape[2]} Ã— {data.shape[3]}")
    print(f"   æ—¶é—´èŒƒå›´: {time_slots[0]} åˆ° {time_slots[-1]}")
    
    # æ•°æ®ç»Ÿè®¡
    print(f"\nğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
    print(f"   æœ€å¤§æµé‡: {data.max():.0f}")
    print(f"   å¹³å‡æµé‡: {data.mean():.2f}")
    print(f"   æ ‡å‡†å·®: {data.std():.2f}")
    print(f"   éé›¶æ¯”ä¾‹: {(data > 0).mean()*100:.1f}%")
    
    return data, time_slots, grid_info



def visualize_data_overview(data, time_slots, grid_info):
    """æ•°æ®æ¦‚è§ˆå¯è§†åŒ–"""
    print("ğŸ“Š æ•°æ®æ¦‚è§ˆå¯è§†åŒ–...")
    
    # 1. é™æ€æ—¶é—´ç‚¹å¯¹æ¯”
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    
    # é€‰æ‹©å‡ ä¸ªæ—¶é—´ç‚¹è¿›è¡Œå¯è§†åŒ–
    time_indices = [0, len(data)//4, len(data)//2, 3*len(data)//4, len(data)-1]
    
    for i, t_idx in enumerate(time_indices[:3]):
        # ä¸Šå®¢é‡
        im1 = axes[0, i].imshow(data[t_idx, 0], cmap='Reds', interpolation='nearest')
        axes[0, i].set_title(f'ä¸Šå®¢é‡ - {time_slots[t_idx].strftime("%m-%d %H:%M")}')
        axes[0, i].axis('off')
        plt.colorbar(im1, ax=axes[0, i], fraction=0.046, pad=0.04)
        
        # ä¸‹å®¢é‡
        im2 = axes[1, i].imshow(data[t_idx, 1], cmap='Blues', interpolation='nearest')
        axes[1, i].set_title(f'ä¸‹å®¢é‡ - {time_slots[t_idx].strftime("%m-%d %H:%M")}')
        axes[1, i].axis('off')
        plt.colorbar(im2, ax=axes[1, i], fraction=0.046, pad=0.04)
    
    plt.suptitle('NYCå‡ºç§Ÿè½¦æµé‡æ—¶ç©ºåˆ†å¸ƒå¯¹æ¯”', fontsize=16)
    plt.tight_layout()
    plt.show()
    
    # 2. æ—¶é—´åºåˆ—åˆ†æ
    plt.figure(figsize=(15, 8))
    
    # åˆ†åŒºä¸€ç»´æ ¼å¼ï¼šé€‰å–è‹¥å¹²åˆ†åŒºç´¢å¼•è¿›è¡Œæ—¶é—´åºåˆ—å¯è§†åŒ–
    S = data.shape[3]
    zone_indices = [0, max(1, S//4), max(2, S//2), max(3, (3*S)//4), S-1]
    
    plt.subplot(2, 2, 1)
    for idx in zone_indices:
        pickup_ts = data[:, 0, 0, idx]
        plt.plot(pickup_ts, label=f'åŒº{idx}', alpha=0.8, linewidth=1)
    plt.title('ä¸åŒåˆ†åŒºçš„ä¸Šå®¢é‡æ—¶é—´åºåˆ—')
    plt.xlabel('æ—¶é—´æ­¥ (æ¯30åˆ†é’Ÿ)')
    plt.ylabel('ä¸Šå®¢é‡')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(2, 2, 2)
    for idx in zone_indices:
        dropoff_ts = data[:, 1, 0, idx]
        plt.plot(dropoff_ts, label=f'åŒº{idx}', alpha=0.8, linewidth=1)
    plt.title('ä¸åŒåˆ†åŒºçš„ä¸‹å®¢é‡æ—¶é—´åºåˆ—')
    plt.xlabel('æ—¶é—´æ­¥ (æ¯30åˆ†é’Ÿ)')
    plt.ylabel('ä¸‹å®¢é‡')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # å…¨å±€å¹³å‡æµé‡
    plt.subplot(2, 2, 3)
    global_pickup = data[:, 0].mean(axis=(1, 2))
    global_dropoff = data[:, 1].mean(axis=(1, 2))
    
    plt.plot(global_pickup, label='å¹³å‡ä¸Šå®¢é‡', color='red', alpha=0.7, linewidth=2)
    plt.plot(global_dropoff, label='å¹³å‡ä¸‹å®¢é‡', color='blue', alpha=0.7, linewidth=2)
    plt.title('å…¨å±€å¹³å‡å®¢æµé‡å˜åŒ–')
    plt.xlabel('æ—¶é—´æ­¥ (æ¯30åˆ†é’Ÿ)')
    plt.ylabel('å¹³å‡å®¢æµé‡')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # å®¢æµé‡åˆ†å¸ƒç›´æ–¹å›¾
    plt.subplot(2, 2, 4)
    all_pickup = data[:, 0].flatten()
    all_dropoff = data[:, 1].flatten()
    
    plt.hist(all_pickup[all_pickup > 0], bins=50, alpha=0.7, label='ä¸Šå®¢é‡', color='red', density=True)
    plt.hist(all_dropoff[all_dropoff > 0], bins=50, alpha=0.7, label='ä¸‹å®¢é‡', color='blue', density=True)
    plt.title('å®¢æµé‡åˆ†å¸ƒç›´æ–¹å›¾')
    plt.xlabel('å®¢æµé‡å€¼')
    plt.ylabel('å¯†åº¦')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.yscale('log')
    
    plt.tight_layout()
    plt.show()
    
    # 3. ç©ºé—´ç»Ÿè®¡
    plt.figure(figsize=(12, 5))
    
    # åˆ†åŒºä¸€ç»´çš„ç©ºé—´ç»Ÿè®¡ï¼šæŒ‰åˆ†åŒºç´¢å¼•ç»˜åˆ¶å‡å€¼æ›²çº¿
    plt.subplot(1, 2, 1)
    mean_pickup = data[:, 0, 0, :].mean(axis=0)
    plt.plot(mean_pickup, color='red')
    plt.title('å¹³å‡ä¸Šå®¢é‡ï¼ˆæŒ‰åˆ†åŒºï¼‰')
    plt.xlabel('åˆ†åŒºç´¢å¼•'); plt.ylabel('ä¸Šå®¢é‡')
    
    plt.subplot(1, 2, 2)
    mean_dropoff = data[:, 1, 0, :].mean(axis=0)
    plt.plot(mean_dropoff, color='blue')
    plt.title('å¹³å‡ä¸‹å®¢é‡ï¼ˆæŒ‰åˆ†åŒºï¼‰')
    plt.xlabel('åˆ†åŒºç´¢å¼•'); plt.ylabel('ä¸‹å®¢é‡')
    
    plt.suptitle('ç©ºé—´å¹³å‡å®¢æµé‡åˆ†å¸ƒ', fontsize=16)
    plt.tight_layout()
    plt.show()

# æ£€æŸ¥grid_pathæ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
try:
    if 'grid_path' not in locals():
        # å°è¯•ä»dataç›®å½•åŠ è½½
        data_dir = "./data"
        grid_path = os.path.join(data_dir, 'nyc_taxi_real_32x32.npz')
        
        if not os.path.exists(grid_path):
            print("âŒ ç½‘æ ¼æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®ä¸‹è½½å’Œå¤„ç†æ­¥éª¤")
            print(f"   æœŸæœ›æ–‡ä»¶è·¯å¾„: {grid_path}")
        else:
            print(f"ğŸ“ æ‰¾åˆ°ç½‘æ ¼æ•°æ®æ–‡ä»¶: {grid_path}")
    
    # åŠ è½½å’Œå¯è§†åŒ–æ•°æ®
    data, time_slots, grid_info = visualize_taxi_data(grid_path)
    
    # æ•°æ®æ¦‚è§ˆ
    print("\nğŸ“Š æ•°æ®æ¦‚è§ˆ...")
    visualize_data_overview(data, time_slots, grid_info)
    
except NameError as e:
    print(f"âŒ é”™è¯¯: {e}")
    print("ğŸ’¡ è¯·å…ˆè¿è¡Œæ•°æ®ä¸‹è½½å’Œå¤„ç†æ­¥éª¤ (Cell 7)")
except FileNotFoundError as e:
    print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
    print("ğŸ’¡ è¯·å…ˆè¿è¡Œæ•°æ®ä¸‹è½½å’Œå¤„ç†æ­¥éª¤ (Cell 7)")






class SpatialPatchTST(nn.Module):
    """
    å‘é‡åŒ–çš„ç©ºé—´PatchTSTï¼š
    - å°†ç©ºé—´ç»´(H*W)åˆå¹¶è¿›æ‰¹æ¬¡ï¼Œä¸€æ¬¡æ€§è¿›è¡Œæ—¶é—´patch + Transformerç¼–ç 
    - æ˜¾è‘—å‡å°‘Pythonå¾ªç¯ä¸kernel launchå¼€é”€
    """
    def __init__(self, 
                 seq_len=12,
                 pred_len=6,
                 channels=2,
                 height=32,
                 width=32,
                 patch_size=4,
                 d_model=128,
                 n_heads=8,
                 n_layers=3,
                 dropout=0.1):
        super().__init__()
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.channels = channels
        self.height = height
        self.width = width
        self.patch_size = patch_size
        self.d_model = d_model
        self.spatial_size = height * width
        self.d_s = d_model // 4  # ç©ºé—´æŠ•å½±åçš„é€šé“ç»´åº¦

        # é¢„è®¡ç®—patchæ•°ï¼ˆæŒ‰seq_lenæ•´é™¤patch_sizeçš„ä¸Šå–æ•´ï¼‰
        self.n_patches = (seq_len + patch_size - 1) // patch_size

        # ç©ºé—´é€šé“å‡ç»´
        self.spatial_proj = nn.Linear(channels, self.d_s)

        # æ—¶é—´patchåµŒå…¥ï¼ˆçº¿æ€§ï¼‰ï¼š[patch_size*d_s] â†’ [d_model]
        self.patch_embedding = nn.Linear(patch_size * self.d_s, d_model)

        # ä½ç½®ç¼–ç 
        self.pos_encoding = nn.Parameter(torch.randn(1, self.n_patches, d_model))       # æ—¶é—´patchä½ç½®ç¼–ç 
        self.spatial_pos = nn.Parameter(torch.randn(1, self.spatial_size, self.d_s))     # ç©ºé—´ä½ç½®ç¼–ç 

        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=n_heads,
            dim_feedforward=d_model * 2,
            dropout=dropout,
            activation='gelu',
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)

        # é¢„æµ‹å¤´ + è¾“å‡ºæŠ•å½±
        self.predictor = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, pred_len * self.d_s)
        )
        self.output_proj = nn.Linear(self.d_s, channels)

        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)

    def forward(self, x):
        """
        x: [B, T, C, H, W]
        return: [B, pred_len, C, H, W]
        """
        B, T, C, H, W = x.shape
        d_s = self.d_s
        S = H * W

        # ç¡®ä¿è¾“å…¥å¼ é‡åœ¨MPSè®¾å¤‡ä¸Šä½¿ç”¨æ­£ç¡®çš„å†…å­˜æ ¼å¼
        if x.device.type == 'mps':
            x = x.contiguous()

        # é‡æ’ä¸º [B, T, H*W, C] å¹¶åšç©ºé—´é€šé“å‡ç»´
        x = x.permute(0, 1, 3, 4, 2).reshape(B, T, S, C)             # [B, T, S, C]
        x = self.spatial_proj(x)                                      # [B, T, S, d_s]
        x = x + self.spatial_pos                                      # + [1, S, d_s] å¹¿æ’­åˆ° [B, T, S, d_s]

        # åˆå¹¶ç©ºé—´åˆ°æ‰¹æ¬¡ï¼šä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰ç½‘æ ¼çš„æ—¶é—´åºåˆ—
        x = x.permute(0, 2, 1, 3).reshape(B * S, T, d_s)              # [B*S, T, d_s]

        # æ—¶é—´ç»´paddingåˆ°patchå¯¹é½
        if T % self.patch_size != 0:
            pad_len = self.patch_size - (T % self.patch_size)
            # åœ¨æ—¶é—´ç»´æœ«å°¾pad
            x = F.pad(x, (0, 0, 0, pad_len))                           # [B*S, T_pad, d_s]
            T_padded = T + pad_len
        else:
            T_padded = T

        # ç”Ÿæˆéé‡å patch: [B*S, n_patches, patch_size, d_s]
        n_patches = T_padded // self.patch_size
        x = x.view(B * S, n_patches, self.patch_size, d_s)
        # åˆå¹¶patchç»´åº¦åšçº¿æ€§åµŒå…¥: [B*S, n_patches, patch_size*d_s] â†’ [B*S, n_patches, d_model]
        patches = self.patch_embedding(x.reshape(B * S, n_patches, self.patch_size * d_s))

        # æ·»åŠ æ—¶é—´ä½ç½®ç¼–ç 
        patches = patches + self.pos_encoding[:, :n_patches, :]

        # ç¡®ä¿Transformerè¾“å…¥æ˜¯è¿ç»­çš„
        if patches.device.type == 'mps':
            patches = patches.contiguous()

        # Transformerç¼–ç 
        encoded = self.transformer(patches)                            # [B*S, n_patches, d_model]

        # å–æœ€åä¸€ä¸ªpatchçš„è¡¨ç¤ºå¹¶åšé¢„æµ‹
        last_feat = encoded[:, -1, :]                                  # [B*S, d_model]
        pred = self.predictor(last_feat)                               # [B*S, pred_len * d_s]
        pred = pred.view(B * S, self.pred_len, d_s)                    # [B*S, pred_len, d_s]

        # è¾“å‡ºæŠ•å½±åˆ°é€šé“Cï¼Œå¹¶æ¢å¤ç©ºé—´
        pred = self.output_proj(pred)                                  # [B*S, pred_len, C]
        pred = pred.view(B, S, self.pred_len, C).permute(0, 2, 3, 1)   # [B, pred_len, C, S]
        pred = pred.view(B, self.pred_len, C, H, W)                    # [B, pred_len, C, H, W]
        
        # ç¡®ä¿è¾“å‡ºå¼ é‡æ˜¯è¿ç»­çš„
        if pred.device.type == 'mps':
            pred = pred.contiguous()
            
        return pred



# ä½¿ç”¨å‘é‡åŒ–æ¨¡å‹è¿›è¡Œå¿«é€Ÿå‰å‘ä¸è®­ç»ƒ
print("ğŸ§  åˆ›å»ºå¿«é€Ÿç‰ˆSpatialPatchTSTFastæ¨¡å‹...")
model = SpatialPatchTST(
    seq_len=12,
    pred_len=6,
    channels=2,
    height=32,
    width=32,
    patch_size=4,
    d_model=64,     # è½»é‡é…ç½®
    n_heads=4,
    n_layers=2,
    dropout=0.1
).to(device)

# æ¨¡å‹ä¿¡æ¯
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
model_size_mb = total_params * 4 / 1024 / 1024

print(f"âœ… å¿«é€Ÿæ¨¡å‹åˆ›å»ºå®Œæˆï¼")
print(f"   æ€»å‚æ•°é‡: {total_params:,}")
print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
print(f"   æ¨¡å‹å¤§å°: {model_size_mb:.2f} MB")
print(f"   è®¾å¤‡: {device}")

# å¿«é€Ÿå‰å‘æµ‹è¯•
print(f"\nğŸ§ª æµ‹è¯•å¿«é€Ÿæ¨¡å‹å‰å‘ä¼ æ’­...")
x_test = torch.randn(2, 12, 2, 32, 32).to(device)
y_test = model(x_test)
print(f"   è¾“å…¥å½¢çŠ¶: {x_test.shape}")
print(f"   è¾“å‡ºå½¢çŠ¶: {y_test.shape}")
print(f"   å‰å‘ä¼ æ’­æˆåŠŸï¼âœ…")






class NYCTaxiDataset(Dataset):
    """NYCå‡ºç§Ÿè½¦ç©ºé—´æ•°æ®é›†"""
    
    def __init__(self, data_path, seq_len=12, pred_len=6, train_ratio=0.7, val_ratio=0.2, split='train'):
        """
        Args:
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            seq_len: è¾“å…¥åºåˆ—é•¿åº¦ (é»˜è®¤12 = 6å°æ—¶)
            pred_len: é¢„æµ‹åºåˆ—é•¿åº¦ (é»˜è®¤6 = 3å°æ—¶)
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
            split: 'train', 'val', 'test'
        """
        # åŠ è½½æ•°æ®
        loaded = np.load(data_path, allow_pickle=True)
        data = loaded['data'].astype(np.float32)  # [T, C, H, W]
        
        self.seq_len = seq_len
        self.pred_len = pred_len
        
        # æ•°æ®æ ‡å‡†åŒ–
        self.data_mean = data.mean()
        self.data_std = data.std()
        data = (data - self.data_mean) / (self.data_std + 1e-8)
        
        # æ•°æ®é›†åˆ’åˆ†
        total_samples = len(data) - seq_len - pred_len + 1
        train_size = int(total_samples * train_ratio)
        val_size = int(total_samples * val_ratio)
        
        if split == 'train':
            self.data = data[:train_size + seq_len + pred_len - 1]
            self.start_idx = 0
            self.end_idx = train_size
        elif split == 'val':
            self.data = data[train_size:train_size + val_size + seq_len + pred_len - 1]
            self.start_idx = 0
            self.end_idx = val_size
        else:  # test
            self.data = data[train_size + val_size:]
            self.start_idx = 0
            self.end_idx = len(self.data) - seq_len - pred_len + 1
        
        print(f"ğŸ“Š {split.upper()}æ•°æ®é›†:")
        print(f"   æ•°æ®å½¢çŠ¶: {self.data.shape}")
        print(f"   æ ·æœ¬æ•°é‡: {self.end_idx - self.start_idx}")
        print(f"   è¾“å…¥é•¿åº¦: {seq_len} (ä»£è¡¨{seq_len*0.5:.1f}å°æ—¶)")
        print(f"   é¢„æµ‹é•¿åº¦: {pred_len} (ä»£è¡¨{pred_len*0.5:.1f}å°æ—¶)")
    
    def __len__(self):
        return self.end_idx - self.start_idx
    
    def __getitem__(self, idx):
        actual_idx = self.start_idx + idx
        x = self.data[actual_idx:actual_idx + self.seq_len]
        y = self.data[actual_idx + self.seq_len:actual_idx + self.seq_len + self.pred_len]
        return torch.tensor(x), torch.tensor(y)

# åˆ›å»ºæ•°æ®é›†
print("ğŸ“Š åˆ›å»ºæ•°æ®é›†...")
train_dataset = NYCTaxiDataset(grid_path, seq_len=12, pred_len=6, split='train')
val_dataset = NYCTaxiDataset(grid_path, seq_len=12, pred_len=6, split='val')
test_dataset = NYCTaxiDataset(grid_path, seq_len=12, pred_len=6, split='test')

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
batch_size = 16
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)

print(f"\nâœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼")
print(f"   è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
print(f"   éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
print(f"   æµ‹è¯•æ‰¹æ¬¡æ•°: {len(test_loader)}")
print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")

# æ£€æŸ¥ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®
sample_x, sample_y = next(iter(train_loader))
print(f"\nğŸ” æ ·æœ¬æ£€æŸ¥:")
print(f"   è¾“å…¥å½¢çŠ¶: {sample_x.shape}")  # [batch, seq_len, channels, height, width]
print(f"   è¾“å‡ºå½¢çŠ¶: {sample_y.shape}")  # [batch, pred_len, channels, height, width]






class TrafficPredictor:
    """äº¤é€šé¢„æµ‹å™¨"""
    
    def __init__(self, model, device=device):
        self.model = model.to(device)
        self.device = device
        self.train_losses = []
        self.val_losses = []
    
    def train_epoch(self, train_loader, optimizer, criterion):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        
        for batch_x, batch_y in train_loader:
            # ç¡®ä¿å¼ é‡åœ¨MPSè®¾å¤‡ä¸Šä½¿ç”¨æ­£ç¡®çš„å†…å­˜æ ¼å¼
            batch_x = batch_x.to(self.device, non_blocking=True)
            batch_y = batch_y.to(self.device, non_blocking=True)
            
            # ç¡®ä¿å¼ é‡æ˜¯è¿ç»­çš„ï¼ˆè§£å†³MPS channels_lastæ ¼å¼é—®é¢˜ï¼‰
            if self.device.type == 'mps':
                batch_x = batch_x.contiguous()
                batch_y = batch_y.contiguous()
            
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            pred = self.model(batch_x)
            loss = criterion(pred, batch_y)
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        return total_loss / len(train_loader)
    
    def validate(self, val_loader, criterion):
        """éªŒè¯"""
        self.model.eval()
        total_loss = 0
        
        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                # ç¡®ä¿å¼ é‡åœ¨MPSè®¾å¤‡ä¸Šä½¿ç”¨æ­£ç¡®çš„å†…å­˜æ ¼å¼
                batch_x = batch_x.to(self.device, non_blocking=True)
                batch_y = batch_y.to(self.device, non_blocking=True)
                
                # ç¡®ä¿å¼ é‡æ˜¯è¿ç»­çš„ï¼ˆè§£å†³MPS channels_lastæ ¼å¼é—®é¢˜ï¼‰
                if self.device.type == 'mps':
                    batch_x = batch_x.contiguous()
                    batch_y = batch_y.contiguous()
                
                pred = self.model(batch_x)
                loss = criterion(pred, batch_y)
                
                total_loss += loss.item()
        
        return total_loss / len(val_loader)
    
    def train(self, train_loader, val_loader, epochs=30, lr=1e-3, patience=8):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒäº¤é€šé¢„æµ‹æ¨¡å‹")
        print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"ğŸ§  æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        
        # ä¸šåŠ¡ç›®æ ‡è¯´æ˜
        print(f"\nğŸ¯ ä¸šåŠ¡ç›®æ ‡:")
        print(f"   è¾“å…¥: è¿‡å»6å°æ—¶çš„äº¤é€šæµé‡åˆ†å¸ƒ")
        print(f"   è¾“å‡º: æœªæ¥3å°æ—¶çš„äº¤é€šæµé‡åˆ†å¸ƒ") 
        print(f"   åº”ç”¨: å‡ºç§Ÿè½¦è°ƒåº¦ä¼˜åŒ–ï¼Œå¸æœºå¯¼èˆªå»ºè®®")
        
        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=1e-4)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
        criterion = nn.MSELoss()
        
        # æ—©åœæœºåˆ¶
        best_val_loss = float('inf')
        patience_counter = 0
        
        # è®­ç»ƒå¾ªç¯
        print(f"\nğŸ“ˆ å¼€å§‹è®­ç»ƒ...")
        for epoch in tqdm(range(epochs), desc="è®­ç»ƒè¿›åº¦"):
            # è®­ç»ƒ
            train_loss = self.train_epoch(train_loader, optimizer, criterion)
            
            # éªŒè¯
            val_loss = self.validate(val_loader, criterion)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step()
            
            # è®°å½•æŸå¤±
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            
            # æ—©åœæ£€æŸ¥
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                torch.save(self.model.state_dict(), 'best_traffic_model.pth')
            else:
                patience_counter += 1
            
            # æ‰“å°è¿›åº¦
            if (epoch + 1) % 10 == 0:
                print(f"\\nEpoch {epoch+1}/{epochs}")
                print(f"è®­ç»ƒæŸå¤±: {train_loss:.6f}")
                print(f"éªŒè¯æŸå¤±: {val_loss:.6f}")
                print(f"å­¦ä¹ ç‡: {scheduler.get_last_lr()[0]:.6f}")
            
            # æ—©åœ
            if patience_counter >= patience:
                print(f"\\nğŸ›‘ æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬{epoch+1}è½®åœæ­¢è®­ç»ƒ")
                break
        
        print(f"âœ… è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        self.model.load_state_dict(torch.load('best_traffic_model.pth'))
        
        return self.train_losses, self.val_losses

# åˆ›å»ºè®­ç»ƒå™¨
predictor = TrafficPredictor(model, device)

# å¼€å§‹è®­ç»ƒ
train_losses, val_losses = predictor.train(
    train_loader, val_loader,
    epochs=30, lr=1e-3, patience=8
)






# ç»˜åˆ¶è®­ç»ƒå†å²
print(f"ğŸ“ˆ ç»˜åˆ¶è®­ç»ƒå†å²...")
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(train_losses, label='è®­ç»ƒæŸå¤±', alpha=0.8)
plt.plot(val_losses, label='éªŒè¯æŸå¤±', alpha=0.8)
plt.title('è®­ç»ƒæŸå¤±æ›²çº¿')
plt.xlabel('Epoch')
plt.ylabel('MSE Loss')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
epochs_to_show = min(20, len(train_losses))
plt.plot(train_losses[-epochs_to_show:], label='è®­ç»ƒæŸå¤± (æœ€å20è½®)', alpha=0.8)
plt.plot(val_losses[-epochs_to_show:], label='éªŒè¯æŸå¤± (æœ€å20è½®)', alpha=0.8)
plt.title('è®­ç»ƒæŸå¤±æ›²çº¿ (æœ€å20è½®)')
plt.xlabel('Epoch')
plt.ylabel('MSE Loss')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()



# æµ‹è¯•é¢„æµ‹å¹¶å¯è§†åŒ–ç»“æœ
def visualize_predictions(predictor, test_loader, dataset_stats):
    """å¯è§†åŒ–é¢„æµ‹ç»“æœ"""
    print("ğŸ“Š å¯è§†åŒ–é¢„æµ‹ç»“æœ...")
    
    predictor.model.eval()
    
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x = batch_x.to(predictor.device)
            batch_y = batch_y.to(predictor.device)
            
            pred = predictor.model(batch_x)
            
            # åæ ‡å‡†åŒ–
            data_mean, data_std = dataset_stats
            batch_x = batch_x * data_std + data_mean
            batch_y = batch_y * data_std + data_mean
            pred = pred * data_std + data_mean
            
            # è½¬æ¢ä¸ºnumpy
            batch_x = batch_x.cpu().numpy()
            batch_y = batch_y.cpu().numpy()
            pred = pred.cpu().numpy()
            
            break
    
    # å¯è§†åŒ–
    sample_idx = 0
    fig, axes = plt.subplots(3, 4, figsize=(16, 12))
    
    # è¾“å…¥åºåˆ—çš„æœ€åä¸€å¸§
    axes[0, 0].imshow(batch_x[sample_idx, -1, 0], cmap='Reds')
    axes[0, 0].set_title('è¾“å…¥æœ€åå¸§ - æµå…¥é‡')
    axes[0, 0].axis('off')
    
    axes[0, 1].imshow(batch_x[sample_idx, -1, 1], cmap='Blues')
    axes[0, 1].set_title('è¾“å…¥æœ€åå¸§ - æµå‡ºé‡')
    axes[0, 1].axis('off')
    
    # çœŸå®çš„é¢„æµ‹ç›®æ ‡
    axes[1, 0].imshow(batch_y[sample_idx, 0, 0], cmap='Reds')
    axes[1, 0].set_title('çœŸå®å€¼ç¬¬1å¸§ - æµå…¥é‡')
    axes[1, 0].axis('off')
    
    axes[1, 1].imshow(batch_y[sample_idx, 0, 1], cmap='Blues')
    axes[1, 1].set_title('çœŸå®å€¼ç¬¬1å¸§ - æµå‡ºé‡')
    axes[1, 1].axis('off')
    
    # æ¨¡å‹é¢„æµ‹ç»“æœ
    axes[2, 0].imshow(pred[sample_idx, 0, 0], cmap='Reds')
    axes[2, 0].set_title('é¢„æµ‹å€¼ç¬¬1å¸§ - æµå…¥é‡')
    axes[2, 0].axis('off')
    
    axes[2, 1].imshow(pred[sample_idx, 0, 1], cmap='Blues')
    axes[2, 1].set_title('é¢„æµ‹å€¼ç¬¬1å¸§ - æµå‡ºé‡')
    axes[2, 1].axis('off')
    
    # è¯¯å·®åˆ†æ
    error_in = np.abs(batch_y[sample_idx, 0, 0] - pred[sample_idx, 0, 0])
    error_out = np.abs(batch_y[sample_idx, 0, 1] - pred[sample_idx, 0, 1])
    
    axes[0, 2].imshow(error_in, cmap='Reds')
    axes[0, 2].set_title('æµå…¥é‡é¢„æµ‹è¯¯å·®')
    axes[0, 2].axis('off')
    
    axes[1, 2].imshow(error_out, cmap='Blues')
    axes[1, 2].set_title('æµå‡ºé‡é¢„æµ‹è¯¯å·®')
    axes[1, 2].axis('off')
    
    # æ—¶é—´åºåˆ—å¯¹æ¯” (é€‰æ‹©ä¸€ä¸ªæ´»è·ƒä½ç½®)
    pos_y, pos_x = 16, 16  # ä¸­å¿ƒä½ç½®
    
    input_ts_in = batch_x[sample_idx, :, 0, pos_y, pos_x]
    target_ts_in = batch_y[sample_idx, :, 0, pos_y, pos_x]
    pred_ts_in = pred[sample_idx, :, 0, pos_y, pos_x]
    
    full_time = np.concatenate([input_ts_in, target_ts_in])
    pred_time = np.concatenate([input_ts_in, pred_ts_in])
    
    axes[0, 3].plot(range(len(input_ts_in)), input_ts_in, 'g-', label='å†å²', linewidth=2)
    axes[0, 3].plot(range(len(input_ts_in), len(full_time)), target_ts_in, 'b-', label='çœŸå®', linewidth=2)
    axes[0, 3].plot(range(len(input_ts_in), len(pred_time)), pred_ts_in, 'r--', label='é¢„æµ‹', linewidth=2)
    axes[0, 3].axvline(x=len(input_ts_in), color='black', linestyle=':', alpha=0.7)
    axes[0, 3].set_title(f'ä½ç½®({pos_y},{pos_x}) æµå…¥é‡æ—¶åº')
    axes[0, 3].legend()
    axes[0, 3].grid(True, alpha=0.3)
    
    # ç±»ä¼¼çš„æµå‡ºé‡æ—¶åº
    input_ts_out = batch_x[sample_idx, :, 1, pos_y, pos_x]
    target_ts_out = batch_y[sample_idx, :, 1, pos_y, pos_x]
    pred_ts_out = pred[sample_idx, :, 1, pos_y, pos_x]
    
    full_time_out = np.concatenate([input_ts_out, target_ts_out])
    pred_time_out = np.concatenate([input_ts_out, pred_ts_out])
    
    axes[1, 3].plot(range(len(input_ts_out)), input_ts_out, 'g-', label='å†å²', linewidth=2)
    axes[1, 3].plot(range(len(input_ts_out), len(full_time_out)), target_ts_out, 'b-', label='çœŸå®', linewidth=2)
    axes[1, 3].plot(range(len(input_ts_out), len(pred_time_out)), pred_ts_out, 'r--', label='é¢„æµ‹', linewidth=2)
    axes[1, 3].axvline(x=len(input_ts_out), color='black', linestyle=':', alpha=0.7)
    axes[1, 3].set_title(f'ä½ç½®({pos_y},{pos_x}) æµå‡ºé‡æ—¶åº')
    axes[1, 3].legend()
    axes[1, 3].grid(True, alpha=0.3)
    
    # ä¸šåŠ¡è§£é‡Š
    axes[2, 2].text(0.1, 0.8, 'ğŸ¯ ä¸šåŠ¡ä»·å€¼:', transform=axes[2, 2].transAxes, fontsize=12, weight='bold')
    axes[2, 2].text(0.1, 0.6, 'â€¢ é¢„æµ‹æœªæ¥3å°æ—¶éœ€æ±‚', transform=axes[2, 2].transAxes, fontsize=10)
    axes[2, 2].text(0.1, 0.4, 'â€¢ ä¼˜åŒ–å¸æœºè°ƒåº¦', transform=axes[2, 2].transAxes, fontsize=10)
    axes[2, 2].text(0.1, 0.2, 'â€¢ å‡å°‘ç©ºé©¶æ—¶é—´', transform=axes[2, 2].transAxes, fontsize=10)
    axes[2, 2].axis('off')
    
    axes[2, 3].text(0.1, 0.8, 'ğŸ“Š æ¨¡å‹è¾“å‡º:', transform=axes[2, 3].transAxes, fontsize=12, weight='bold')
    axes[2, 3].text(0.1, 0.6, f'â€¢ é¢„æµ‹æ—¶é•¿: {pred.shape[1]*0.5:.1f}å°æ—¶', transform=axes[2, 3].transAxes, fontsize=10)
    axes[2, 3].text(0.1, 0.4, f'â€¢ ç©ºé—´åˆ†è¾¨ç‡: {pred.shape[3]}Ã—{pred.shape[4]}', transform=axes[2, 3].transAxes, fontsize=10)
    axes[2, 3].text(0.1, 0.2, f'â€¢ é€šé“æ•°: {pred.shape[2]}', transform=axes[2, 3].transAxes, fontsize=10)
    axes[2, 3].axis('off')
    
    plt.suptitle('NYCå‡ºç§Ÿè½¦æµé‡é¢„æµ‹ç»“æœ', fontsize=16)
    plt.tight_layout()
    plt.show()
    
    # è®¡ç®—é¢„æµ‹è¯¯å·®
    mse = np.mean((pred - batch_y)**2)
    mae = np.mean(np.abs(pred - batch_y))
    
    print(f"\\nğŸ“Š é¢„æµ‹æ€§èƒ½:")
    print(f"   MSE: {mse:.6f}")
    print(f"   MAE: {mae:.6f}")
    print(f"   ç›¸å¯¹è¯¯å·®: {mae/np.mean(batch_y)*100:.2f}%")
    
    return mse, mae

# æ‰§è¡Œé¢„æµ‹å¯è§†åŒ–
dataset_stats = (train_dataset.data_mean, train_dataset.data_std)
mse, mae = visualize_predictions(predictor, test_loader, dataset_stats)

# ä¸šåŠ¡ä»·å€¼åˆ†æ
print(f"\\nğŸ’¼ ä¸šåŠ¡ä»·å€¼åˆ†æ:")
print(f"   ğŸ“ˆ é¢„æµ‹å‡†ç¡®åº¦: MAE = {mae:.3f} (æ¯ç½‘æ ¼æ¯30åˆ†é’Ÿè¯¯å·®{mae:.1f}è¾†è½¦)")
print(f"   â° é¢„æµ‹æ—¶é—´èŒƒå›´: æœªæ¥3å°æ—¶")
print(f"   ğŸ—ºï¸ ç©ºé—´è¦†ç›–: æ•´ä¸ªæ›¼å“ˆé¡¿32Ã—32ç½‘æ ¼")
print(f"   ğŸ’° å•†ä¸šåº”ç”¨:")
print(f"      - å¸æœºå¯¼èˆª: æ¨èé«˜éœ€æ±‚åŒºåŸŸ")
print(f"      - è°ƒåº¦ä¼˜åŒ–: æå‰è°ƒé…è½¦è¾†")
print(f"      - åŠ¨æ€å®šä»·: æ ¹æ®é¢„æµ‹éœ€æ±‚è°ƒä»·")
print(f"      - è¿åŠ›è§„åˆ’: åˆç†å®‰æ’ç­æ¬¡")




