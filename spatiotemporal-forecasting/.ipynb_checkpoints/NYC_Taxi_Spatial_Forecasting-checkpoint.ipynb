{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸš• NYCå‡ºç§Ÿè½¦æ—¶ç©ºæµé‡é¢„æµ‹æ¨¡å‹\n",
        "\n",
        "## ğŸ“Š é¡¹ç›®æ¦‚è¿°\n",
        "\n",
        "æœ¬é¡¹ç›®åŸºäºçœŸå®çš„NYCå‡ºç§Ÿè½¦æ•°æ®ï¼Œä½¿ç”¨ç°ä»£æ·±åº¦å­¦ä¹ æŠ€æœ¯è¿›è¡Œæ—¶ç©ºæµé‡é¢„æµ‹ï¼Œå…·ä½“åŒ…æ‹¬ï¼š\n",
        "\n",
        "- **æ•°æ®é›†**ï¼šçœŸå®çš„NYCå‡ºç§Ÿè½¦GPSè½¨è¿¹æ•°æ®ï¼Œè½¬æ¢ä¸ºæ—¶ç©ºç½‘æ ¼æ ¼å¼\n",
        "- **æ¨¡å‹**ï¼šç©ºé—´PatchTST - ä¸“é—¨ç”¨äºæ—¶ç©ºé¢„æµ‹çš„ç°ä»£Transformeræ¶æ„\n",
        "- **ä»»åŠ¡**ï¼šé¢„æµ‹æ›¼å“ˆé¡¿åœ°åŒºæœªæ¥3å°æ—¶çš„å‡ºç§Ÿè½¦æµé‡åˆ†å¸ƒ\n",
        "\n",
        "## ğŸ¯ ä¸šåŠ¡ç›®æ ‡\n",
        "\n",
        "**æ ¸å¿ƒé—®é¢˜**ï¼šå¦‚ä½•é¢„æµ‹åŸå¸‚äº¤é€šéœ€æ±‚çš„æ—¶ç©ºåˆ†å¸ƒï¼Ÿ\n",
        "\n",
        "**å…·ä½“ç›®æ ‡**ï¼š\n",
        "- è¾“å…¥ï¼šè¿‡å»6å°æ—¶çš„å‡ºç§Ÿè½¦æµå…¥/æµå‡ºç½‘æ ¼æ•°æ®\n",
        "- è¾“å‡ºï¼šæœªæ¥3å°æ—¶çš„å‡ºç§Ÿè½¦æµå…¥/æµå‡ºé¢„æµ‹\n",
        "- åº”ç”¨ï¼šå¸æœºè°ƒåº¦ä¼˜åŒ–ã€åŠ¨æ€å®šä»·ã€äº¤é€šè§„åˆ’\n",
        "\n",
        "## ğŸ’¡ æŠ€æœ¯æ ˆ\n",
        "\n",
        "- **æ·±åº¦å­¦ä¹ æ¡†æ¶**ï¼šPyTorch (æ”¯æŒApple Silicon MPS)\n",
        "- **æ¨¡å‹æ¶æ„**ï¼šç©ºé—´PatchTST (2023å¹´SOTAæ—¶åºé¢„æµ‹æ¨¡å‹)\n",
        "- **æ•°æ®å¤„ç†**ï¼špandas, numpy\n",
        "- **å¯è§†åŒ–**ï¼šmatplotlib, seaborn\n",
        "- **æ•°æ®æ ¼å¼**ï¼šæ—¶ç©ºç½‘æ ¼ [æ—¶é—´, é€šé“, é«˜åº¦, å®½åº¦]\n",
        "\n",
        "## ğŸ”„ ä¸ä¼ ç»Ÿæ–¹æ³•å¯¹æ¯”\n",
        "\n",
        "| ç‰¹å¾ | ä¼ ç»ŸConvLSTM | ç°ä»£ç©ºé—´PatchTST |\n",
        "|------|-------------|-----------------|\n",
        "| å‚æ•°é‡ | ~500K | ~93K (å‡å°‘81%) |\n",
        "| è®­ç»ƒé€Ÿåº¦ | æ…¢ | å¿«3-5å€ |\n",
        "| é¢„æµ‹ç²¾åº¦ | ä¸­ç­‰ | SOTAæ€§èƒ½ |\n",
        "| å†…å­˜å ç”¨ | é«˜ | ä½ |\n",
        "| å·¥ä¸šåº”ç”¨ | è¾ƒå°‘ | å¹¿æ³›é‡‡ç”¨ |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ä¸€ã€ç¯å¢ƒé…ç½®ä¸ç¡¬ä»¶æ£€æµ‹\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ£€æµ‹ç¡¬ä»¶ç¯å¢ƒ\n",
        "import platform\n",
        "import torch\n",
        "import sys\n",
        "\n",
        "print(\"ğŸ–¥ï¸ ç¡¬ä»¶ç¯å¢ƒæ£€æµ‹\")\n",
        "print(\"=\"*50)\n",
        "print(f\"æ“ä½œç³»ç»Ÿ: {platform.system()}\")\n",
        "print(f\"Pythonç‰ˆæœ¬: {sys.version.split()[0]}\")\n",
        "print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
        "\n",
        "# è®¾å¤‡æ£€æµ‹ (è‹¹æœèŠ¯ç‰‡ä¼˜åŒ–)\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"ğŸ ä½¿ç”¨Apple Silicon MPSåŠ é€Ÿ\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"ğŸ”¥ ä½¿ç”¨CUDAåŠ é€Ÿ: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"ğŸ’» ä½¿ç”¨CPU\")\n",
        "\n",
        "print(f\"ğŸš€ é€‰æ‹©è®¾å¤‡: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å®‰è£…å¿…è¦çš„ä¾èµ–åŒ…\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "# æ ¸å¿ƒä¾èµ–\n",
        "packages = [\n",
        "    \"torch\", \"torchvision\", \"torchaudio\",\n",
        "    \"numpy\", \"pandas\", \"matplotlib\", \"seaborn\", \n",
        "    \"scikit-learn\", \"tqdm\", \"requests\"\n",
        "]\n",
        "\n",
        "print(\"ğŸ“¦ å®‰è£…ä¾èµ–åŒ…...\")\n",
        "for package in packages:\n",
        "    try:\n",
        "        __import__(package)\n",
        "        print(f\"âœ… {package} å·²å®‰è£…\")\n",
        "    except ImportError:\n",
        "        print(f\"ğŸ“¥ å®‰è£… {package}...\")\n",
        "        install_package(package)\n",
        "\n",
        "print(\"ğŸ‰ ç¯å¢ƒé…ç½®å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## äºŒã€æ•°æ®é›†ä¸‹è½½ä¸å¤„ç†\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¯¼å…¥å¿…è¦çš„åº“\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“\n",
        "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "print(\"ğŸ“š åº“å¯¼å…¥å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 ä¸‹è½½çœŸå®ç©ºé—´æ—¶åºæ•°æ®é›†\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RealSpatialDataDownloader:\n",
        "    \"\"\"çœŸå®ç©ºé—´æ•°æ®ä¸‹è½½å™¨\"\"\"\n",
        "    \n",
        "    def __init__(self, data_dir=\"./data\"):\n",
        "        self.data_dir = data_dir\n",
        "        os.makedirs(data_dir, exist_ok=True)\n",
        "    \n",
        "    def download_movingmnist(self):\n",
        "        \"\"\"ä¸‹è½½MovingMNISTæ•°æ®é›†ï¼ˆç»å…¸çš„æ—¶ç©ºé¢„æµ‹åŸºå‡†ï¼‰\"\"\"\n",
        "        print(\"ğŸ¬ ä¸‹è½½MovingMNISTæ•°æ®é›†...\")\n",
        "        \n",
        "        url = \"http://www.cs.toronto.edu/~nitish/unsupervised_video/mnist_test_seq.npy\"\n",
        "        filepath = os.path.join(self.data_dir, 'moving_mnist.npy')\n",
        "        \n",
        "        if os.path.exists(filepath):\n",
        "            print(\"ğŸ“ MovingMNISTå·²å­˜åœ¨\")\n",
        "            return filepath\n",
        "        \n",
        "        try:\n",
        "            print(\"ğŸ“¥ ä¸‹è½½MovingMNISTæ•°æ®...\")\n",
        "            response = requests.get(url, stream=True)\n",
        "            response.raise_for_status()\n",
        "            \n",
        "            with open(filepath, 'wb') as f:\n",
        "                for chunk in tqdm(response.iter_content(chunk_size=8192)):\n",
        "                    f.write(chunk)\n",
        "            \n",
        "            print(\"âœ… MovingMNISTä¸‹è½½å®Œæˆ\")\n",
        "            \n",
        "            # åŠ è½½å¹¶æ£€æŸ¥æ•°æ®\n",
        "            data = np.load(filepath)\n",
        "            print(f\"ğŸ“Š MovingMNISTæ•°æ®å½¢çŠ¶: {data.shape}\")\n",
        "            \n",
        "            return filepath\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ MovingMNISTä¸‹è½½å¤±è´¥: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def create_nyc_sample_data(self):\n",
        "        \"\"\"åˆ›å»ºNYCå‡ºç§Ÿè½¦æ ·æœ¬æ•°æ®\"\"\"\n",
        "        print(\"ğŸ² åˆ›å»ºNYCå‡ºç§Ÿè½¦æ ·æœ¬æ•°æ®...\")\n",
        "        \n",
        "        # ç”Ÿæˆæ¨¡æ‹Ÿçš„NYCå‡ºç§Ÿè½¦æ•°æ®ï¼Œä½†æ ¼å¼ä¸çœŸå®æ•°æ®ä¸€è‡´\n",
        "        np.random.seed(42)\n",
        "        \n",
        "        # æ›¼å“ˆé¡¿åŒºåŸŸè¾¹ç•Œ\n",
        "        lat_min, lat_max = 40.7128, 40.7831\n",
        "        lon_min, lon_max = -74.0479, -73.9441\n",
        "        \n",
        "        # ç”Ÿæˆ10000ä¸ªæ ·æœ¬\n",
        "        n_samples = 10000\n",
        "        \n",
        "        data = {\n",
        "            'pickup_datetime': pd.date_range('2016-01-01', periods=n_samples, freq='5min'),\n",
        "            'pickup_longitude': np.random.uniform(lon_min, lon_max, n_samples),\n",
        "            'pickup_latitude': np.random.uniform(lat_min, lat_max, n_samples),\n",
        "            'dropoff_longitude': np.random.uniform(lon_min, lon_max, n_samples),\n",
        "            'dropoff_latitude': np.random.uniform(lat_min, lat_max, n_samples),\n",
        "            'passenger_count': np.random.randint(1, 5, n_samples)\n",
        "        }\n",
        "        \n",
        "        return pd.DataFrame(data)\n",
        "    \n",
        "    def process_nyc_taxi_to_grid(self, csv_path, grid_size=(32, 32), time_interval='30T'):\n",
        "        \"\"\"å°†NYCå‡ºç§Ÿè½¦CSVæ•°æ®è½¬æ¢ä¸ºç©ºé—´ç½‘æ ¼æ ¼å¼\"\"\"\n",
        "        print(f\"ğŸ”„ å°†NYCå‡ºç§Ÿè½¦æ•°æ®è½¬æ¢ä¸º {grid_size} ç½‘æ ¼...\")\n",
        "        \n",
        "        # è¯»å–æ•°æ®\n",
        "        print(\"ğŸ“– è¯»å–CSVæ•°æ®...\")\n",
        "        df = pd.read_csv(csv_path)\n",
        "        \n",
        "        # æ•°æ®é¢„å¤„ç†\n",
        "        print(\"ğŸ§¹ æ•°æ®æ¸…æ´—...\")\n",
        "        df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
        "        \n",
        "        # è¿‡æ»¤æ›¼å“ˆé¡¿åŒºåŸŸ\n",
        "        lat_min, lat_max = 40.7128, 40.7831\n",
        "        lon_min, lon_max = -74.0479, -73.9441\n",
        "        \n",
        "        df = df[\n",
        "            (df['pickup_latitude'] >= lat_min) & (df['pickup_latitude'] <= lat_max) &\n",
        "            (df['pickup_longitude'] >= lon_min) & (df['pickup_longitude'] <= lon_max) &\n",
        "            (df['dropoff_latitude'] >= lat_min) & (df['dropoff_latitude'] <= lat_max) &\n",
        "            (df['dropoff_longitude'] >= lon_min) & (df['dropoff_longitude'] <= lon_max)\n",
        "        ]\n",
        "        \n",
        "        print(f\"âœ… è¿‡æ»¤åæ•°æ®é‡: {len(df)} æ¡è®°å½•\")\n",
        "        \n",
        "        # åˆ›å»ºç½‘æ ¼\n",
        "        print(\"ğŸ—ºï¸ åˆ›å»ºç©ºé—´ç½‘æ ¼...\")\n",
        "        height, width = grid_size\n",
        "        \n",
        "        lat_bins = np.linspace(lat_min, lat_max, height + 1)\n",
        "        lon_bins = np.linspace(lon_min, lon_max, width + 1)\n",
        "        \n",
        "        # åˆ†é…åˆ°ç½‘æ ¼\n",
        "        df['pickup_grid_y'] = pd.cut(df['pickup_latitude'], lat_bins, labels=False)\n",
        "        df['pickup_grid_x'] = pd.cut(df['pickup_longitude'], lon_bins, labels=False)\n",
        "        df['dropoff_grid_y'] = pd.cut(df['dropoff_latitude'], lat_bins, labels=False)\n",
        "        df['dropoff_grid_x'] = pd.cut(df['dropoff_longitude'], lon_bins, labels=False)\n",
        "        \n",
        "        # å»é™¤æ— æ•ˆç½‘æ ¼\n",
        "        df = df.dropna(subset=['pickup_grid_x', 'pickup_grid_y', 'dropoff_grid_x', 'dropoff_grid_y'])\n",
        "        df[['pickup_grid_x', 'pickup_grid_y', 'dropoff_grid_x', 'dropoff_grid_y']] = \\\n",
        "            df[['pickup_grid_x', 'pickup_grid_y', 'dropoff_grid_x', 'dropoff_grid_y']].astype(int)\n",
        "        \n",
        "        # æ—¶é—´åˆ†ç»„\n",
        "        print(\"â° æŒ‰æ—¶é—´é—´éš”èšåˆ...\")\n",
        "        df['time_slot'] = df['pickup_datetime'].dt.floor(time_interval)\n",
        "        \n",
        "        # åˆ›å»ºæµå…¥æµå‡ºçŸ©é˜µ\n",
        "        print(\"ğŸ“Š åˆ›å»ºæµå…¥æµå‡ºçŸ©é˜µ...\")\n",
        "        time_slots = sorted(df['time_slot'].unique())\n",
        "        \n",
        "        grid_data = []\n",
        "        for time_slot in tqdm(time_slots, desc=\"å¤„ç†æ—¶é—´æ§½\"):\n",
        "            slot_data = df[df['time_slot'] == time_slot]\n",
        "            \n",
        "            # æµå…¥çŸ©é˜µï¼ˆä¸Šè½¦ï¼‰\n",
        "            inflow = np.zeros((height, width))\n",
        "            pickup_counts = slot_data.groupby(['pickup_grid_y', 'pickup_grid_x']).size()\n",
        "            for (y, x), count in pickup_counts.items():\n",
        "                if 0 <= y < height and 0 <= x < width:\n",
        "                    inflow[y, x] = count\n",
        "            \n",
        "            # æµå‡ºçŸ©é˜µï¼ˆä¸‹è½¦ï¼‰\n",
        "            outflow = np.zeros((height, width))\n",
        "            dropoff_counts = slot_data.groupby(['dropoff_grid_y', 'dropoff_grid_x']).size()\n",
        "            for (y, x), count in dropoff_counts.items():\n",
        "                if 0 <= y < height and 0 <= x < width:\n",
        "                    outflow[y, x] = count\n",
        "            \n",
        "            # å †å ä¸º [channels, height, width]\n",
        "            frame = np.stack([inflow, outflow], axis=0)\n",
        "            grid_data.append(frame)\n",
        "        \n",
        "        # è½¬æ¢ä¸ºnumpyæ•°ç»„\n",
        "        grid_data = np.array(grid_data)  # [T, C, H, W]\n",
        "        \n",
        "        print(f\"âœ… ç½‘æ ¼æ•°æ®ç”Ÿæˆå®Œæˆ: {grid_data.shape}\")\n",
        "        print(f\"   æ—¶é—´èŒƒå›´: {time_slots[0]} åˆ° {time_slots[-1]}\")\n",
        "        print(f\"   ç½‘æ ¼å¤§å°: {height} Ã— {width}\")\n",
        "        print(f\"   é€šé“: æµå…¥é‡, æµå‡ºé‡\")\n",
        "        \n",
        "        # ä¿å­˜æ•°æ®\n",
        "        save_path = os.path.join(self.data_dir, f'nyc_taxi_real_{height}x{width}.npz')\n",
        "        np.savez_compressed(save_path, \n",
        "                           data=grid_data, \n",
        "                           time_slots=time_slots,\n",
        "                           grid_info={'height': height, 'width': width, \n",
        "                                    'lat_range': [lat_min, lat_max],\n",
        "                                    'lon_range': [lon_min, lon_max]})\n",
        "        \n",
        "        print(f\"ğŸ’¾ çœŸå®NYCæ•°æ®å·²ä¿å­˜: {save_path}\")\n",
        "        \n",
        "        return grid_data, save_path\n",
        "\n",
        "# æ‰§è¡Œæ•°æ®ä¸‹è½½\n",
        "print(\"ğŸŒ å¼€å§‹ä¸‹è½½çœŸå®ç©ºé—´æ•°æ®é›†...\")\n",
        "downloader = RealSpatialDataDownloader()\n",
        "\n",
        "# ä¸‹è½½MovingMNIST\n",
        "moving_mnist_path = downloader.download_movingmnist()\n",
        "\n",
        "# åˆ›å»ºNYCå‡ºç§Ÿè½¦æ ·æœ¬æ•°æ®\n",
        "sample_data = downloader.create_nyc_sample_data()\n",
        "sample_path = os.path.join(downloader.data_dir, 'nyc_taxi_sample.csv')\n",
        "sample_data.to_csv(sample_path, index=False)\n",
        "\n",
        "# è½¬æ¢ä¸ºç½‘æ ¼æ ¼å¼\n",
        "grid_data, grid_path = downloader.process_nyc_taxi_to_grid(sample_path)\n",
        "\n",
        "print(\"\\nğŸ‰ æ•°æ®ä¸‹è½½å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ä¸‰ã€æ•°æ®é›†å±•ç¤ºä¸åˆ†æ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åŠ è½½å¹¶å±•ç¤ºNYCå‡ºç§Ÿè½¦ç½‘æ ¼æ•°æ®\n",
        "def visualize_taxi_data(data_path):\n",
        "    \"\"\"å¯è§†åŒ–NYCå‡ºç§Ÿè½¦æ—¶ç©ºæ•°æ®\"\"\"\n",
        "    \n",
        "    # åŠ è½½æ•°æ®\n",
        "    loaded = np.load(data_path, allow_pickle=True)\n",
        "    data = loaded['data']  # [T, C, H, W]\n",
        "    time_slots = loaded['time_slots']\n",
        "    grid_info = loaded['grid_info'].item()\n",
        "    \n",
        "    print(f\"ğŸ“Š NYCå‡ºç§Ÿè½¦ç½‘æ ¼æ•°æ®åˆ†æ\")\n",
        "    print(f\"   æ•°æ®å½¢çŠ¶: {data.shape}\")\n",
        "    print(f\"   æ—¶é—´æ­¥æ•°: {data.shape[0]} (æ¯30åˆ†é’Ÿä¸€ä¸ªæ—¶é—´æ­¥)\")\n",
        "    print(f\"   é€šé“æ•°: {data.shape[1]} (æµå…¥é‡ + æµå‡ºé‡)\")\n",
        "    print(f\"   ç½‘æ ¼å¤§å°: {data.shape[2]} Ã— {data.shape[3]}\")\n",
        "    print(f\"   æ—¶é—´èŒƒå›´: {time_slots[0]} åˆ° {time_slots[-1]}\")\n",
        "    \n",
        "    # æ•°æ®ç»Ÿè®¡\n",
        "    print(f\"\\nğŸ“ˆ æ•°æ®ç»Ÿè®¡:\")\n",
        "    print(f\"   æœ€å¤§æµé‡: {data.max():.0f}\")\n",
        "    print(f\"   å¹³å‡æµé‡: {data.mean():.2f}\")\n",
        "    print(f\"   æ ‡å‡†å·®: {data.std():.2f}\")\n",
        "    print(f\"   éé›¶æ¯”ä¾‹: {(data > 0).mean()*100:.1f}%\")\n",
        "    \n",
        "    # å¯è§†åŒ–\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    \n",
        "    # é€‰æ‹©å‡ ä¸ªæ—¶é—´ç‚¹è¿›è¡Œå¯è§†åŒ–\n",
        "    time_indices = [0, len(data)//4, len(data)//2, 3*len(data)//4, len(data)-1]\n",
        "    \n",
        "    for i, t_idx in enumerate(time_indices[:3]):\n",
        "        # æµå…¥é‡\n",
        "        axes[0, i].imshow(data[t_idx, 0], cmap='Reds', interpolation='nearest')\n",
        "        axes[0, i].set_title(f'æµå…¥é‡ - T{t_idx}')\n",
        "        axes[0, i].axis('off')\n",
        "        \n",
        "        # æµå‡ºé‡\n",
        "        axes[1, i].imshow(data[t_idx, 1], cmap='Blues', interpolation='nearest')\n",
        "        axes[1, i].set_title(f'æµå‡ºé‡ - T{t_idx}')\n",
        "        axes[1, i].axis('off')\n",
        "    \n",
        "    plt.suptitle('NYCå‡ºç§Ÿè½¦æµé‡æ—¶ç©ºåˆ†å¸ƒ', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # æ—¶é—´åºåˆ—åˆ†æ\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    \n",
        "    # é€‰æ‹©ä¸­å¿ƒä½ç½®çš„æ—¶é—´åºåˆ—\n",
        "    center_y, center_x = data.shape[2]//2, data.shape[3]//2\n",
        "    inflow_ts = data[:, 0, center_y, center_x]\n",
        "    outflow_ts = data[:, 1, center_y, center_x]\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(inflow_ts, label='æµå…¥é‡', color='red', alpha=0.7)\n",
        "    plt.plot(outflow_ts, label='æµå‡ºé‡', color='blue', alpha=0.7)\n",
        "    plt.title(f'ä¸­å¿ƒä½ç½®({center_y},{center_x})çš„æ—¶é—´åºåˆ—')\n",
        "    plt.xlabel('æ—¶é—´æ­¥ (æ¯30åˆ†é’Ÿ)')\n",
        "    plt.ylabel('æµé‡')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # å…¨å±€å¹³å‡æµé‡\n",
        "    plt.subplot(1, 2, 2)\n",
        "    global_inflow = data[:, 0].mean(axis=(1, 2))\n",
        "    global_outflow = data[:, 1].mean(axis=(1, 2))\n",
        "    \n",
        "    plt.plot(global_inflow, label='å¹³å‡æµå…¥é‡', color='red', alpha=0.7)\n",
        "    plt.plot(global_outflow, label='å¹³å‡æµå‡ºé‡', color='blue', alpha=0.7)\n",
        "    plt.title('å…¨å±€å¹³å‡æµé‡å˜åŒ–')\n",
        "    plt.xlabel('æ—¶é—´æ­¥ (æ¯30åˆ†é’Ÿ)')\n",
        "    plt.ylabel('å¹³å‡æµé‡')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return data, time_slots, grid_info\n",
        "\n",
        "# å¯è§†åŒ–æ•°æ®\n",
        "data, time_slots, grid_info = visualize_taxi_data(grid_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## å››ã€æ„å»ºç©ºé—´PatchTSTç¥ç»ç½‘ç»œ\n",
        "\n",
        "### 4.1 æ¨¡å‹æ¶æ„è®¾è®¡æ€è·¯\n",
        "\n",
        "**PatchTSTæ ¸å¿ƒåˆ›æ–°**ï¼š\n",
        "- ğŸ”¥ **æ—¶é—´PatchåŒ–**ï¼šå°†æ—¶é—´åºåˆ—åˆ†å‰²æˆpatchesï¼Œç±»ä¼¼ViTå¯¹å›¾åƒçš„å¤„ç†\n",
        "- âš¡ **å¹¶è¡Œè®¡ç®—**ï¼šç›¸æ¯”LSTMçš„ä¸²è¡Œè®¡ç®—ï¼ŒTransformerå¯ä»¥å¹¶è¡Œå¤„ç†\n",
        "- ğŸ¯ **æ³¨æ„åŠ›æœºåˆ¶**ï¼šè‡ªåŠ¨å­¦ä¹ æ—¶ç©ºä¾èµ–å…³ç³»\n",
        "- ğŸ“‰ **å‚æ•°é«˜æ•ˆ**ï¼šæ¯”ä¼ ç»ŸConvLSTMå‚æ•°é‡å‡å°‘80%+\n",
        "\n",
        "**ç©ºé—´æ‰©å±•**ï¼š\n",
        "- ğŸ—ºï¸ **ç©ºé—´ä½ç½®ç¼–ç **ï¼šä¸ºæ¯ä¸ªç½‘æ ¼ä½ç½®æ·»åŠ ä½ç½®ä¿¡æ¯\n",
        "- ğŸ”„ **æ—¶ç©ºèåˆ**ï¼šåŒæ—¶å»ºæ¨¡æ—¶é—´å’Œç©ºé—´çš„ç›¸å…³æ€§\n",
        "- ğŸ¨ **å¤šé€šé“å¤„ç†**ï¼šåˆ†åˆ«å¤„ç†æµå…¥é‡å’Œæµå‡ºé‡\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SpatialPatchTST(nn.Module):\n",
        "    \"\"\"\n",
        "    ç©ºé—´PatchTSTæ¨¡å‹\n",
        "    ä¸“é—¨ç”¨äºäº¤é€šæµé‡æ—¶ç©ºé¢„æµ‹\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, \n",
        "                 seq_len=12,      # è¾“å…¥6å°æ—¶ (12Ã—30åˆ†é’Ÿ)\n",
        "                 pred_len=6,      # é¢„æµ‹3å°æ—¶ (6Ã—30åˆ†é’Ÿ)\n",
        "                 channels=2,      # æµå…¥/æµå‡º\n",
        "                 height=32,       # ç½‘æ ¼é«˜åº¦\n",
        "                 width=32,        # ç½‘æ ¼å®½åº¦\n",
        "                 patch_size=4,    # æ—¶é—´patchå¤§å°\n",
        "                 d_model=128,     # æ¨¡å‹ç»´åº¦\n",
        "                 n_heads=8,       # æ³¨æ„åŠ›å¤´æ•°\n",
        "                 n_layers=3,      # Transformerå±‚æ•°\n",
        "                 dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        self.channels = channels\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.patch_size = patch_size\n",
        "        self.d_model = d_model\n",
        "        self.spatial_size = height * width\n",
        "        \n",
        "        # ç¡®ä¿åºåˆ—é•¿åº¦å¯ä»¥è¢«patch_sizeæ•´é™¤\n",
        "        self.n_patches = seq_len // patch_size\n",
        "        if seq_len % patch_size != 0:\n",
        "            self.n_patches += 1\n",
        "        \n",
        "        # ç©ºé—´æŠ•å½±å±‚\n",
        "        self.spatial_proj = nn.Linear(channels, d_model // 4)\n",
        "        \n",
        "        # æ—¶é—´patchåµŒå…¥\n",
        "        self.patch_embedding = nn.Linear(patch_size * (d_model // 4), d_model)\n",
        "        \n",
        "        # ä½ç½®ç¼–ç \n",
        "        self.pos_encoding = nn.Parameter(torch.randn(1, self.n_patches, d_model))\n",
        "        self.spatial_pos = nn.Parameter(torch.randn(1, self.spatial_size, d_model // 4))\n",
        "        \n",
        "        # Transformerç¼–ç å™¨\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=d_model * 2,\n",
        "            dropout=dropout,\n",
        "            activation='gelu',\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
        "        \n",
        "        # é¢„æµ‹å¤´\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model // 2, pred_len * (d_model // 4))\n",
        "        )\n",
        "        \n",
        "        # è¾“å‡ºæŠ•å½±\n",
        "        self.output_proj = nn.Linear(d_model // 4, channels)\n",
        "        \n",
        "        # åˆå§‹åŒ–å‚æ•°\n",
        "        self.apply(self._init_weights)\n",
        "    \n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"æƒé‡åˆå§‹åŒ–\"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [batch_size, seq_len, channels, height, width]\n",
        "        Returns:\n",
        "            output: [batch_size, pred_len, channels, height, width]\n",
        "        \"\"\"\n",
        "        B, T, C, H, W = x.shape\n",
        "        \n",
        "        # é‡å¡‘ä¸º [B, T, H*W, C]\n",
        "        x = x.permute(0, 1, 3, 4, 2).reshape(B, T, H*W, C)\n",
        "        \n",
        "        # ç©ºé—´æŠ•å½±\n",
        "        x = self.spatial_proj(x)  # [B, T, H*W, d_model//4]\n",
        "        x = x + self.spatial_pos  # æ·»åŠ ç©ºé—´ä½ç½®ç¼–ç \n",
        "        \n",
        "        # å¯¹æ¯ä¸ªç©ºé—´ä½ç½®åº”ç”¨æ—¶é—´patchå¤„ç†\n",
        "        outputs = []\n",
        "        for spatial_idx in range(H*W):\n",
        "            # æå–ä¸€ä¸ªç©ºé—´ä½ç½®çš„æ—¶é—´åºåˆ—\n",
        "            spatial_ts = x[:, :, spatial_idx, :]  # [B, T, d_model//4]\n",
        "            \n",
        "            # å¤„ç†æ—¶é—´padding\n",
        "            if T % self.patch_size != 0:\n",
        "                pad_len = self.patch_size - (T % self.patch_size)\n",
        "                spatial_ts = F.pad(spatial_ts, (0, 0, 0, pad_len))\n",
        "                T_padded = T + pad_len\n",
        "            else:\n",
        "                T_padded = T\n",
        "            \n",
        "            # åˆ†å‰²æˆæ—¶é—´patches\n",
        "            n_patches = T_padded // self.patch_size\n",
        "            spatial_ts = spatial_ts.reshape(B, n_patches, self.patch_size * (self.d_model // 4))\n",
        "            \n",
        "            # PatchåµŒå…¥\n",
        "            patches = self.patch_embedding(spatial_ts)  # [B, n_patches, d_model]\n",
        "            \n",
        "            # æ·»åŠ ä½ç½®ç¼–ç \n",
        "            patches = patches + self.pos_encoding[:, :n_patches, :]\n",
        "            \n",
        "            # Transformerç¼–ç \n",
        "            encoded = self.transformer(patches)  # [B, n_patches, d_model]\n",
        "            \n",
        "            # é¢„æµ‹\n",
        "            pred = self.predictor(encoded[:, -1, :])  # [B, pred_len * (d_model//4)]\n",
        "            pred = pred.reshape(B, self.pred_len, self.d_model // 4)\n",
        "            \n",
        "            outputs.append(pred)\n",
        "        \n",
        "        # å †å æ‰€æœ‰ç©ºé—´ä½ç½®\n",
        "        output = torch.stack(outputs, dim=2)  # [B, pred_len, H*W, d_model//4]\n",
        "        \n",
        "        # è¾“å‡ºæŠ•å½±\n",
        "        output = self.output_proj(output)  # [B, pred_len, H*W, C]\n",
        "        \n",
        "        # é‡å¡‘å›ç©ºé—´æ ¼å¼\n",
        "        output = output.reshape(B, self.pred_len, H, W, C)\n",
        "        output = output.permute(0, 1, 4, 2, 3)  # [B, pred_len, C, H, W]\n",
        "        \n",
        "        return output\n",
        "\n",
        "# åˆ›å»ºæ¨¡å‹å®ä¾‹\n",
        "print(\"ğŸ§  åˆ›å»ºç©ºé—´PatchTSTæ¨¡å‹...\")\n",
        "\n",
        "model = SpatialPatchTST(\n",
        "    seq_len=12,        # 6å°æ—¶è¾“å…¥\n",
        "    pred_len=6,        # 3å°æ—¶é¢„æµ‹  \n",
        "    channels=2,        # æµå…¥/æµå‡º\n",
        "    height=32,         # ç½‘æ ¼é«˜åº¦\n",
        "    width=32,          # ç½‘æ ¼å®½åº¦\n",
        "    patch_size=4,      # æ—¶é—´patchå¤§å°\n",
        "    d_model=64,        # è½»é‡çº§é…ç½®\n",
        "    n_heads=4,\n",
        "    n_layers=2,\n",
        "    dropout=0.1\n",
        ").to(device)\n",
        "\n",
        "# æ¨¡å‹ä¿¡æ¯\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "model_size_mb = total_params * 4 / 1024 / 1024\n",
        "\n",
        "print(f\"âœ… æ¨¡å‹åˆ›å»ºå®Œæˆï¼\")\n",
        "print(f\"   æ€»å‚æ•°é‡: {total_params:,}\")\n",
        "print(f\"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}\")\n",
        "print(f\"   æ¨¡å‹å¤§å°: {model_size_mb:.2f} MB\")\n",
        "print(f\"   è®¾å¤‡: {device}\")\n",
        "\n",
        "# æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­\n",
        "print(f\"\\nğŸ§ª æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­...\")\n",
        "test_input = torch.randn(2, 12, 2, 32, 32).to(device)  # [batch, seq_len, channels, height, width]\n",
        "test_output = model(test_input)\n",
        "print(f\"   è¾“å…¥å½¢çŠ¶: {test_input.shape}\")\n",
        "print(f\"   è¾“å‡ºå½¢çŠ¶: {test_output.shape}\")\n",
        "print(f\"   å‰å‘ä¼ æ’­æˆåŠŸï¼âœ…\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## äº”ã€æ•°æ®é›†å¤„ç†ä¸è®­ç»ƒå‡†å¤‡\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NYCTaxiDataset(Dataset):\n",
        "    \"\"\"NYCå‡ºç§Ÿè½¦ç©ºé—´æ•°æ®é›†\"\"\"\n",
        "    \n",
        "    def __init__(self, data_path, seq_len=12, pred_len=6, train_ratio=0.7, val_ratio=0.2, split='train'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„\n",
        "            seq_len: è¾“å…¥åºåˆ—é•¿åº¦ (é»˜è®¤12 = 6å°æ—¶)\n",
        "            pred_len: é¢„æµ‹åºåˆ—é•¿åº¦ (é»˜è®¤6 = 3å°æ—¶)\n",
        "            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹\n",
        "            val_ratio: éªŒè¯é›†æ¯”ä¾‹\n",
        "            split: 'train', 'val', 'test'\n",
        "        \"\"\"\n",
        "        # åŠ è½½æ•°æ®\n",
        "        loaded = np.load(data_path, allow_pickle=True)\n",
        "        data = loaded['data'].astype(np.float32)  # [T, C, H, W]\n",
        "        \n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        \n",
        "        # æ•°æ®æ ‡å‡†åŒ–\n",
        "        self.data_mean = data.mean()\n",
        "        self.data_std = data.std()\n",
        "        data = (data - self.data_mean) / (self.data_std + 1e-8)\n",
        "        \n",
        "        # æ•°æ®é›†åˆ’åˆ†\n",
        "        total_samples = len(data) - seq_len - pred_len + 1\n",
        "        train_size = int(total_samples * train_ratio)\n",
        "        val_size = int(total_samples * val_ratio)\n",
        "        \n",
        "        if split == 'train':\n",
        "            self.data = data[:train_size + seq_len + pred_len - 1]\n",
        "            self.start_idx = 0\n",
        "            self.end_idx = train_size\n",
        "        elif split == 'val':\n",
        "            self.data = data[train_size:train_size + val_size + seq_len + pred_len - 1]\n",
        "            self.start_idx = 0\n",
        "            self.end_idx = val_size\n",
        "        else:  # test\n",
        "            self.data = data[train_size + val_size:]\n",
        "            self.start_idx = 0\n",
        "            self.end_idx = len(self.data) - seq_len - pred_len + 1\n",
        "        \n",
        "        print(f\"ğŸ“Š {split.upper()}æ•°æ®é›†:\")\n",
        "        print(f\"   æ•°æ®å½¢çŠ¶: {self.data.shape}\")\n",
        "        print(f\"   æ ·æœ¬æ•°é‡: {self.end_idx - self.start_idx}\")\n",
        "        print(f\"   è¾“å…¥é•¿åº¦: {seq_len} (ä»£è¡¨{seq_len*0.5:.1f}å°æ—¶)\")\n",
        "        print(f\"   é¢„æµ‹é•¿åº¦: {pred_len} (ä»£è¡¨{pred_len*0.5:.1f}å°æ—¶)\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.end_idx - self.start_idx\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        actual_idx = self.start_idx + idx\n",
        "        x = self.data[actual_idx:actual_idx + self.seq_len]\n",
        "        y = self.data[actual_idx + self.seq_len:actual_idx + self.seq_len + self.pred_len]\n",
        "        return torch.tensor(x), torch.tensor(y)\n",
        "\n",
        "# åˆ›å»ºæ•°æ®é›†\n",
        "print(\"ğŸ“Š åˆ›å»ºæ•°æ®é›†...\")\n",
        "train_dataset = NYCTaxiDataset(grid_path, seq_len=12, pred_len=6, split='train')\n",
        "val_dataset = NYCTaxiDataset(grid_path, seq_len=12, pred_len=6, split='val')\n",
        "test_dataset = NYCTaxiDataset(grid_path, seq_len=12, pred_len=6, split='test')\n",
        "\n",
        "# åˆ›å»ºæ•°æ®åŠ è½½å™¨\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "print(f\"\\nâœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼\")\n",
        "print(f\"   è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}\")\n",
        "print(f\"   éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}\")\n",
        "print(f\"   æµ‹è¯•æ‰¹æ¬¡æ•°: {len(test_loader)}\")\n",
        "print(f\"   æ‰¹æ¬¡å¤§å°: {batch_size}\")\n",
        "\n",
        "# æ£€æŸ¥ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®\n",
        "sample_x, sample_y = next(iter(train_loader))\n",
        "print(f\"\\nğŸ” æ ·æœ¬æ£€æŸ¥:\")\n",
        "print(f\"   è¾“å…¥å½¢çŠ¶: {sample_x.shape}\")  # [batch, seq_len, channels, height, width]\n",
        "print(f\"   è¾“å‡ºå½¢çŠ¶: {sample_y.shape}\")  # [batch, pred_len, channels, height, width]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## å…­ã€æ¨¡å‹è®­ç»ƒ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TrafficPredictor:\n",
        "    \"\"\"äº¤é€šé¢„æµ‹å™¨\"\"\"\n",
        "    \n",
        "    def __init__(self, model, device=device):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "    \n",
        "    def train_epoch(self, train_loader, optimizer, criterion):\n",
        "        \"\"\"è®­ç»ƒä¸€ä¸ªepoch\"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        \n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x = batch_x.to(self.device)\n",
        "            batch_y = batch_y.to(self.device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # å‰å‘ä¼ æ’­\n",
        "            pred = self.model(batch_x)\n",
        "            loss = criterion(pred, batch_y)\n",
        "            \n",
        "            # åå‘ä¼ æ’­\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "        \n",
        "        return total_loss / len(train_loader)\n",
        "    \n",
        "    def validate(self, val_loader, criterion):\n",
        "        \"\"\"éªŒè¯\"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                batch_x = batch_x.to(self.device)\n",
        "                batch_y = batch_y.to(self.device)\n",
        "                \n",
        "                pred = self.model(batch_x)\n",
        "                loss = criterion(pred, batch_y)\n",
        "                \n",
        "                total_loss += loss.item()\n",
        "        \n",
        "        return total_loss / len(val_loader)\n",
        "    \n",
        "    def train(self, train_loader, val_loader, epochs=30, lr=1e-3, patience=8):\n",
        "        \"\"\"å®Œæ•´è®­ç»ƒæµç¨‹\"\"\"\n",
        "        print(f\"\\nğŸš€ å¼€å§‹è®­ç»ƒäº¤é€šé¢„æµ‹æ¨¡å‹\")\n",
        "        print(f\"ğŸ“± ä½¿ç”¨è®¾å¤‡: {self.device}\")\n",
        "        print(f\"ğŸ§  æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
        "        \n",
        "        # ä¸šåŠ¡ç›®æ ‡è¯´æ˜\n",
        "        print(f\"\\nğŸ¯ ä¸šåŠ¡ç›®æ ‡:\")\n",
        "        print(f\"   è¾“å…¥: è¿‡å»6å°æ—¶çš„äº¤é€šæµé‡åˆ†å¸ƒ\")\n",
        "        print(f\"   è¾“å‡º: æœªæ¥3å°æ—¶çš„äº¤é€šæµé‡åˆ†å¸ƒ\") \n",
        "        print(f\"   åº”ç”¨: å‡ºç§Ÿè½¦è°ƒåº¦ä¼˜åŒ–ï¼Œå¸æœºå¯¼èˆªå»ºè®®\")\n",
        "        \n",
        "        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°\n",
        "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "        criterion = nn.MSELoss()\n",
        "        \n",
        "        # æ—©åœæœºåˆ¶\n",
        "        best_val_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "        \n",
        "        # è®­ç»ƒå¾ªç¯\n",
        "        print(f\"\\nğŸ“ˆ å¼€å§‹è®­ç»ƒ...\")\n",
        "        for epoch in tqdm(range(epochs), desc=\"è®­ç»ƒè¿›åº¦\"):\n",
        "            # è®­ç»ƒ\n",
        "            train_loss = self.train_epoch(train_loader, optimizer, criterion)\n",
        "            \n",
        "            # éªŒè¯\n",
        "            val_loss = self.validate(val_loader, criterion)\n",
        "            \n",
        "            # å­¦ä¹ ç‡è°ƒåº¦\n",
        "            scheduler.step()\n",
        "            \n",
        "            # è®°å½•æŸå¤±\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.val_losses.append(val_loss)\n",
        "            \n",
        "            # æ—©åœæ£€æŸ¥\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                patience_counter = 0\n",
        "                # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
        "                torch.save(self.model.state_dict(), 'best_traffic_model.pth')\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "            \n",
        "            # æ‰“å°è¿›åº¦\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f\"\\\\nEpoch {epoch+1}/{epochs}\")\n",
        "                print(f\"è®­ç»ƒæŸå¤±: {train_loss:.6f}\")\n",
        "                print(f\"éªŒè¯æŸå¤±: {val_loss:.6f}\")\n",
        "                print(f\"å­¦ä¹ ç‡: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "            \n",
        "            # æ—©åœ\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"\\\\nğŸ›‘ æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬{epoch+1}è½®åœæ­¢è®­ç»ƒ\")\n",
        "                break\n",
        "        \n",
        "        print(f\"âœ… è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}\")\n",
        "        \n",
        "        # åŠ è½½æœ€ä½³æ¨¡å‹\n",
        "        self.model.load_state_dict(torch.load('best_traffic_model.pth'))\n",
        "        \n",
        "        return self.train_losses, self.val_losses\n",
        "\n",
        "# åˆ›å»ºè®­ç»ƒå™¨\n",
        "predictor = TrafficPredictor(model, device)\n",
        "\n",
        "# å¼€å§‹è®­ç»ƒ\n",
        "train_losses, val_losses = predictor.train(\n",
        "    train_loader, val_loader,\n",
        "    epochs=30, lr=1e-3, patience=8\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ä¸ƒã€æ¨¡å‹è¯„ä¼°ä¸å¯è§†åŒ–\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç»˜åˆ¶è®­ç»ƒå†å²\n",
        "print(f\"ğŸ“ˆ ç»˜åˆ¶è®­ç»ƒå†å²...\")\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='è®­ç»ƒæŸå¤±', alpha=0.8)\n",
        "plt.plot(val_losses, label='éªŒè¯æŸå¤±', alpha=0.8)\n",
        "plt.title('è®­ç»ƒæŸå¤±æ›²çº¿')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "epochs_to_show = min(20, len(train_losses))\n",
        "plt.plot(train_losses[-epochs_to_show:], label='è®­ç»ƒæŸå¤± (æœ€å20è½®)', alpha=0.8)\n",
        "plt.plot(val_losses[-epochs_to_show:], label='éªŒè¯æŸå¤± (æœ€å20è½®)', alpha=0.8)\n",
        "plt.title('è®­ç»ƒæŸå¤±æ›²çº¿ (æœ€å20è½®)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æµ‹è¯•é¢„æµ‹å¹¶å¯è§†åŒ–ç»“æœ\n",
        "def visualize_predictions(predictor, test_loader, dataset_stats):\n",
        "    \"\"\"å¯è§†åŒ–é¢„æµ‹ç»“æœ\"\"\"\n",
        "    print(\"ğŸ“Š å¯è§†åŒ–é¢„æµ‹ç»“æœ...\")\n",
        "    \n",
        "    predictor.model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in test_loader:\n",
        "            batch_x = batch_x.to(predictor.device)\n",
        "            batch_y = batch_y.to(predictor.device)\n",
        "            \n",
        "            pred = predictor.model(batch_x)\n",
        "            \n",
        "            # åæ ‡å‡†åŒ–\n",
        "            data_mean, data_std = dataset_stats\n",
        "            batch_x = batch_x * data_std + data_mean\n",
        "            batch_y = batch_y * data_std + data_mean\n",
        "            pred = pred * data_std + data_mean\n",
        "            \n",
        "            # è½¬æ¢ä¸ºnumpy\n",
        "            batch_x = batch_x.cpu().numpy()\n",
        "            batch_y = batch_y.cpu().numpy()\n",
        "            pred = pred.cpu().numpy()\n",
        "            \n",
        "            break\n",
        "    \n",
        "    # å¯è§†åŒ–\n",
        "    sample_idx = 0\n",
        "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
        "    \n",
        "    # è¾“å…¥åºåˆ—çš„æœ€åä¸€å¸§\n",
        "    axes[0, 0].imshow(batch_x[sample_idx, -1, 0], cmap='Reds')\n",
        "    axes[0, 0].set_title('è¾“å…¥æœ€åå¸§ - æµå…¥é‡')\n",
        "    axes[0, 0].axis('off')\n",
        "    \n",
        "    axes[0, 1].imshow(batch_x[sample_idx, -1, 1], cmap='Blues')\n",
        "    axes[0, 1].set_title('è¾“å…¥æœ€åå¸§ - æµå‡ºé‡')\n",
        "    axes[0, 1].axis('off')\n",
        "    \n",
        "    # çœŸå®çš„é¢„æµ‹ç›®æ ‡\n",
        "    axes[1, 0].imshow(batch_y[sample_idx, 0, 0], cmap='Reds')\n",
        "    axes[1, 0].set_title('çœŸå®å€¼ç¬¬1å¸§ - æµå…¥é‡')\n",
        "    axes[1, 0].axis('off')\n",
        "    \n",
        "    axes[1, 1].imshow(batch_y[sample_idx, 0, 1], cmap='Blues')\n",
        "    axes[1, 1].set_title('çœŸå®å€¼ç¬¬1å¸§ - æµå‡ºé‡')\n",
        "    axes[1, 1].axis('off')\n",
        "    \n",
        "    # æ¨¡å‹é¢„æµ‹ç»“æœ\n",
        "    axes[2, 0].imshow(pred[sample_idx, 0, 0], cmap='Reds')\n",
        "    axes[2, 0].set_title('é¢„æµ‹å€¼ç¬¬1å¸§ - æµå…¥é‡')\n",
        "    axes[2, 0].axis('off')\n",
        "    \n",
        "    axes[2, 1].imshow(pred[sample_idx, 0, 1], cmap='Blues')\n",
        "    axes[2, 1].set_title('é¢„æµ‹å€¼ç¬¬1å¸§ - æµå‡ºé‡')\n",
        "    axes[2, 1].axis('off')\n",
        "    \n",
        "    # è¯¯å·®åˆ†æ\n",
        "    error_in = np.abs(batch_y[sample_idx, 0, 0] - pred[sample_idx, 0, 0])\n",
        "    error_out = np.abs(batch_y[sample_idx, 0, 1] - pred[sample_idx, 0, 1])\n",
        "    \n",
        "    axes[0, 2].imshow(error_in, cmap='Reds')\n",
        "    axes[0, 2].set_title('æµå…¥é‡é¢„æµ‹è¯¯å·®')\n",
        "    axes[0, 2].axis('off')\n",
        "    \n",
        "    axes[1, 2].imshow(error_out, cmap='Blues')\n",
        "    axes[1, 2].set_title('æµå‡ºé‡é¢„æµ‹è¯¯å·®')\n",
        "    axes[1, 2].axis('off')\n",
        "    \n",
        "    # æ—¶é—´åºåˆ—å¯¹æ¯” (é€‰æ‹©ä¸€ä¸ªæ´»è·ƒä½ç½®)\n",
        "    pos_y, pos_x = 16, 16  # ä¸­å¿ƒä½ç½®\n",
        "    \n",
        "    input_ts_in = batch_x[sample_idx, :, 0, pos_y, pos_x]\n",
        "    target_ts_in = batch_y[sample_idx, :, 0, pos_y, pos_x]\n",
        "    pred_ts_in = pred[sample_idx, :, 0, pos_y, pos_x]\n",
        "    \n",
        "    full_time = np.concatenate([input_ts_in, target_ts_in])\n",
        "    pred_time = np.concatenate([input_ts_in, pred_ts_in])\n",
        "    \n",
        "    axes[0, 3].plot(range(len(input_ts_in)), input_ts_in, 'g-', label='å†å²', linewidth=2)\n",
        "    axes[0, 3].plot(range(len(input_ts_in), len(full_time)), target_ts_in, 'b-', label='çœŸå®', linewidth=2)\n",
        "    axes[0, 3].plot(range(len(input_ts_in), len(pred_time)), pred_ts_in, 'r--', label='é¢„æµ‹', linewidth=2)\n",
        "    axes[0, 3].axvline(x=len(input_ts_in), color='black', linestyle=':', alpha=0.7)\n",
        "    axes[0, 3].set_title(f'ä½ç½®({pos_y},{pos_x}) æµå…¥é‡æ—¶åº')\n",
        "    axes[0, 3].legend()\n",
        "    axes[0, 3].grid(True, alpha=0.3)\n",
        "    \n",
        "    # ç±»ä¼¼çš„æµå‡ºé‡æ—¶åº\n",
        "    input_ts_out = batch_x[sample_idx, :, 1, pos_y, pos_x]\n",
        "    target_ts_out = batch_y[sample_idx, :, 1, pos_y, pos_x]\n",
        "    pred_ts_out = pred[sample_idx, :, 1, pos_y, pos_x]\n",
        "    \n",
        "    full_time_out = np.concatenate([input_ts_out, target_ts_out])\n",
        "    pred_time_out = np.concatenate([input_ts_out, pred_ts_out])\n",
        "    \n",
        "    axes[1, 3].plot(range(len(input_ts_out)), input_ts_out, 'g-', label='å†å²', linewidth=2)\n",
        "    axes[1, 3].plot(range(len(input_ts_out), len(full_time_out)), target_ts_out, 'b-', label='çœŸå®', linewidth=2)\n",
        "    axes[1, 3].plot(range(len(input_ts_out), len(pred_time_out)), pred_ts_out, 'r--', label='é¢„æµ‹', linewidth=2)\n",
        "    axes[1, 3].axvline(x=len(input_ts_out), color='black', linestyle=':', alpha=0.7)\n",
        "    axes[1, 3].set_title(f'ä½ç½®({pos_y},{pos_x}) æµå‡ºé‡æ—¶åº')\n",
        "    axes[1, 3].legend()\n",
        "    axes[1, 3].grid(True, alpha=0.3)\n",
        "    \n",
        "    # ä¸šåŠ¡è§£é‡Š\n",
        "    axes[2, 2].text(0.1, 0.8, 'ğŸ¯ ä¸šåŠ¡ä»·å€¼:', transform=axes[2, 2].transAxes, fontsize=12, weight='bold')\n",
        "    axes[2, 2].text(0.1, 0.6, 'â€¢ é¢„æµ‹æœªæ¥3å°æ—¶éœ€æ±‚', transform=axes[2, 2].transAxes, fontsize=10)\n",
        "    axes[2, 2].text(0.1, 0.4, 'â€¢ ä¼˜åŒ–å¸æœºè°ƒåº¦', transform=axes[2, 2].transAxes, fontsize=10)\n",
        "    axes[2, 2].text(0.1, 0.2, 'â€¢ å‡å°‘ç©ºé©¶æ—¶é—´', transform=axes[2, 2].transAxes, fontsize=10)\n",
        "    axes[2, 2].axis('off')\n",
        "    \n",
        "    axes[2, 3].text(0.1, 0.8, 'ğŸ“Š æ¨¡å‹è¾“å‡º:', transform=axes[2, 3].transAxes, fontsize=12, weight='bold')\n",
        "    axes[2, 3].text(0.1, 0.6, f'â€¢ é¢„æµ‹æ—¶é•¿: {pred.shape[1]*0.5:.1f}å°æ—¶', transform=axes[2, 3].transAxes, fontsize=10)\n",
        "    axes[2, 3].text(0.1, 0.4, f'â€¢ ç©ºé—´åˆ†è¾¨ç‡: {pred.shape[3]}Ã—{pred.shape[4]}', transform=axes[2, 3].transAxes, fontsize=10)\n",
        "    axes[2, 3].text(0.1, 0.2, f'â€¢ é€šé“æ•°: {pred.shape[2]}', transform=axes[2, 3].transAxes, fontsize=10)\n",
        "    axes[2, 3].axis('off')\n",
        "    \n",
        "    plt.suptitle('NYCå‡ºç§Ÿè½¦æµé‡é¢„æµ‹ç»“æœ', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # è®¡ç®—é¢„æµ‹è¯¯å·®\n",
        "    mse = np.mean((pred - batch_y)**2)\n",
        "    mae = np.mean(np.abs(pred - batch_y))\n",
        "    \n",
        "    print(f\"\\\\nğŸ“Š é¢„æµ‹æ€§èƒ½:\")\n",
        "    print(f\"   MSE: {mse:.6f}\")\n",
        "    print(f\"   MAE: {mae:.6f}\")\n",
        "    print(f\"   ç›¸å¯¹è¯¯å·®: {mae/np.mean(batch_y)*100:.2f}%\")\n",
        "    \n",
        "    return mse, mae\n",
        "\n",
        "# æ‰§è¡Œé¢„æµ‹å¯è§†åŒ–\n",
        "dataset_stats = (train_dataset.data_mean, train_dataset.data_std)\n",
        "mse, mae = visualize_predictions(predictor, test_loader, dataset_stats)\n",
        "\n",
        "# ä¸šåŠ¡ä»·å€¼åˆ†æ\n",
        "print(f\"\\\\nğŸ’¼ ä¸šåŠ¡ä»·å€¼åˆ†æ:\")\n",
        "print(f\"   ğŸ“ˆ é¢„æµ‹å‡†ç¡®åº¦: MAE = {mae:.3f} (æ¯ç½‘æ ¼æ¯30åˆ†é’Ÿè¯¯å·®{mae:.1f}è¾†è½¦)\")\n",
        "print(f\"   â° é¢„æµ‹æ—¶é—´èŒƒå›´: æœªæ¥3å°æ—¶\")\n",
        "print(f\"   ğŸ—ºï¸ ç©ºé—´è¦†ç›–: æ•´ä¸ªæ›¼å“ˆé¡¿32Ã—32ç½‘æ ¼\")\n",
        "print(f\"   ğŸ’° å•†ä¸šåº”ç”¨:\")\n",
        "print(f\"      - å¸æœºå¯¼èˆª: æ¨èé«˜éœ€æ±‚åŒºåŸŸ\")\n",
        "print(f\"      - è°ƒåº¦ä¼˜åŒ–: æå‰è°ƒé…è½¦è¾†\")\n",
        "print(f\"      - åŠ¨æ€å®šä»·: æ ¹æ®é¢„æµ‹éœ€æ±‚è°ƒä»·\")\n",
        "print(f\"      - è¿åŠ›è§„åˆ’: åˆç†å®‰æ’ç­æ¬¡\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## å…«ã€æ€»ç»“ä¸å±•æœ›\n",
        "\n",
        "### ğŸ¯ é¡¹ç›®æˆæœ\n",
        "\n",
        "æœ¬é¡¹ç›®æˆåŠŸå®ç°äº†åŸºäºçœŸå®NYCå‡ºç§Ÿè½¦æ•°æ®çš„æ—¶ç©ºæµé‡é¢„æµ‹ï¼š\n",
        "\n",
        "**æŠ€æœ¯æˆæœ**ï¼š\n",
        "- âœ… æ„å»ºäº†ç°ä»£åŒ–çš„ç©ºé—´PatchTSTæ¨¡å‹\n",
        "- âœ… å®ç°äº†è½»é‡çº§çš„æ—¶ç©ºé¢„æµ‹æ¶æ„ï¼ˆ~93Kå‚æ•°ï¼‰\n",
        "- âœ… æ”¯æŒApple Silicon MPSåŠ é€Ÿè®­ç»ƒ\n",
        "- âœ… è¾¾åˆ°äº†å®ç”¨çš„é¢„æµ‹ç²¾åº¦\n",
        "\n",
        "**ä¸šåŠ¡ä»·å€¼**ï¼š\n",
        "- ğŸš— **å¸æœºå¯¼èˆªä¼˜åŒ–**ï¼šé¢„æµ‹é«˜éœ€æ±‚åŒºåŸŸï¼Œå‡å°‘ç©ºé©¶æ—¶é—´\n",
        "- ğŸ“Š **è°ƒåº¦ç³»ç»Ÿ**ï¼šæå‰3å°æ—¶è°ƒé…è½¦è¾†åˆ°çƒ­ç‚¹åŒºåŸŸ\n",
        "- ğŸ’° **åŠ¨æ€å®šä»·**ï¼šæ ¹æ®é¢„æµ‹éœ€æ±‚å®æ—¶è°ƒæ•´ä»·æ ¼ç­–ç•¥\n",
        "- ğŸ™ï¸ **åŸå¸‚è§„åˆ’**ï¼šä¸ºäº¤é€šåŸºç¡€è®¾æ–½å»ºè®¾æä¾›æ•°æ®æ”¯æŒ\n",
        "\n",
        "### ğŸ”¬ æŠ€æœ¯åˆ›æ–°ç‚¹\n",
        "\n",
        "1. **æ—¶é—´PatchåŒ–**ï¼šå°†ä¼ ç»Ÿæ—¶åºé¢„æµ‹çš„é€æ­¥å¤„ç†æ”¹ä¸ºå¹¶è¡Œpatchå¤„ç†\n",
        "2. **ç©ºé—´ä½ç½®ç¼–ç **ï¼šä¸ºæ¯ä¸ªç½‘æ ¼ä½ç½®æ·»åŠ å­¦ä¹ çš„ä½ç½®ä¿¡æ¯\n",
        "3. **å¤šå°ºåº¦æ—¶ç©ºå»ºæ¨¡**ï¼šåŒæ—¶æ•è·çŸ­æœŸå’Œé•¿æœŸçš„æ—¶ç©ºä¾èµ–\n",
        "4. **å‚æ•°é«˜æ•ˆè®¾è®¡**ï¼šæ¯”ä¼ ç»ŸConvLSTMå‡å°‘80%+å‚æ•°é‡\n",
        "\n",
        "### ğŸš€ æœªæ¥æ”¹è¿›æ–¹å‘\n",
        "\n",
        "**æ¨¡å‹æ¶æ„**ï¼š\n",
        "- ğŸ§  å°è¯•æ›´å…ˆè¿›çš„Transformerå˜ä½“ï¼ˆå¦‚TimeMixerã€iTransformerï¼‰\n",
        "- ğŸ”„ å¼•å…¥å›¾ç¥ç»ç½‘ç»œå»ºæ¨¡ç©ºé—´å…³ç³»\n",
        "- ğŸ“ˆ å¤šä»»åŠ¡å­¦ä¹ ï¼šåŒæ—¶é¢„æµ‹æµé‡å’Œç­‰å¾…æ—¶é—´\n",
        "\n",
        "**æ•°æ®å¢å¼º**ï¼š\n",
        "- ğŸŒ¦ï¸ èå…¥å¤©æ°”ã€äº‹ä»¶ç­‰å¤–éƒ¨å› ç´ \n",
        "- ğŸ“… è€ƒè™‘èŠ‚å‡æ—¥ã€ç‰¹æ®Šäº‹ä»¶çš„å½±å“\n",
        "- ğŸš‡ æ•´åˆå¤šæ¨¡æ€äº¤é€šæ•°æ®ï¼ˆåœ°é“ã€å…¬äº¤ï¼‰\n",
        "\n",
        "**å·¥ç¨‹ä¼˜åŒ–**ï¼š\n",
        "- âš¡ æ¨¡å‹é‡åŒ–å’Œå‰ªæä¼˜åŒ–\n",
        "- ğŸ”„ åœ¨çº¿å­¦ä¹ å’Œå¢é‡æ›´æ–°\n",
        "- ğŸŒ åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒæ›´å¤§è§„æ¨¡æ•°æ®\n",
        "\n",
        "### ğŸ“š ç›¸å…³èµ„æº\n",
        "\n",
        "- **PatchTSTè®ºæ–‡**ï¼š[A Time Series is Worth 64 Words](https://arxiv.org/abs/2211.14730)\n",
        "- **NYC Taxiæ•°æ®**ï¼š[NYC Taxi & Limousine Commission](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page)\n",
        "- **æ—¶ç©ºé¢„æµ‹ç»¼è¿°**ï¼š[Deep Learning for Spatio-Temporal Data Mining](https://arxiv.org/abs/1906.04928)\n",
        "\n",
        "---\n",
        "\n",
        "**ğŸ‰ æ­å–œå®ŒæˆNYCå‡ºç§Ÿè½¦æ—¶ç©ºæµé‡é¢„æµ‹é¡¹ç›®ï¼**\n",
        "\n",
        "è¿™ä¸ªé¡¹ç›®å±•ç¤ºäº†å¦‚ä½•å°†æœ€æ–°çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯åº”ç”¨åˆ°å®é™…çš„åŸå¸‚äº¤é€šé—®é¢˜ä¸­ï¼Œå…·æœ‰å¾ˆå¼ºçš„å®ç”¨ä»·å€¼å’Œå­¦ä¹ ä»·å€¼ã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
