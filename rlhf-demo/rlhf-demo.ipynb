{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5a11e90",
   "metadata": {},
   "source": [
    "## RLHFï¼ˆReinforcement Learning from Human Feedbackï¼‰\n",
    "\n",
    "- **ç›®æ ‡**ï¼šè®©æ¨¡å‹æ›´ç¬¦åˆäººç±»æ„å›¾ã€æ›´å®‰å…¨ã€æ›´æœ‰ç”¨\n",
    "- **æ ¸å¿ƒæ€æƒ³**ï¼š\n",
    "  - ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ•™ä¼šæ¨¡å‹åŸºæœ¬çš„æŒ‡ä»¤è·Ÿéš\n",
    "  - ç”¨åå¥½æ•°æ®è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ï¼Œå­¦ä¼šæ‰“åˆ†â€œæ›´å¥½/æ›´å·®â€çš„å›ç­”\n",
    "  - ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆPPOï¼‰åœ¨å¥–åŠ±ä¿¡å·ä¸‹ä¼˜åŒ–ç­–ç•¥ï¼Œæƒè¡¡è´¨é‡ã€ç¨³å®šæ€§ä¸å¤šæ ·æ€§\n",
    "- **å…³é”®ç»„ä»¶**ï¼šæŒ‡ä»¤æ•°æ®ã€åå¥½æ•°æ®ï¼ˆA/B å¯¹æ¯”ï¼‰ã€å¥–åŠ±æ¨¡å‹ã€å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€KL çº¦æŸ/å‚è€ƒç­–ç•¥\n",
    "- **å…¸å‹äº§ç‰©**ï¼š\n",
    "  - SFT æ¨¡å‹ï¼ˆä¼šåšäº‹ï¼‰\n",
    "  - RM å¥–åŠ±æ¨¡å‹ï¼ˆä¼šæ‰“åˆ†ï¼‰\n",
    "  - PPO åçš„å¯¹é½æ¨¡å‹ï¼ˆåšå¾—æ›´å¥½ï¼‰\n",
    "  - DPOï¼ˆå–ä»£ RM+PPO çš„ç›´æ¥åå¥½ä¼˜åŒ–ï¼‰\n",
    "\n",
    "### RLHF çš„ä¸‰é˜¶æ®µæµç¨‹ï¼ˆå·¥ç¨‹åŒ–è§†è§’ï¼‰\n",
    "\n",
    "| é˜¶æ®µ | åç§° | ä½œç”¨ | æŠ€æœ¯ |\n",
    "|---|---|---|---|\n",
    "| 1ï¸âƒ£ | SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰ | æ•™æ¨¡å‹æ‰§è¡ŒæŒ‡ä»¤ | CrossEntropyLoss |\n",
    "| 2ï¸âƒ£ | å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰è®­ç»ƒ | å­¦ä¼šâ€œä»€ä¹ˆæ ·çš„å›ç­”æ›´å¥½â€ | Pairwise ranking (A > B) |\n",
    "| 3ï¸âƒ£ | PPO å¼ºåŒ–ä¼˜åŒ– | ç”¨å¥–åŠ±ä¿¡å·ä¼˜åŒ–ç”Ÿæˆç­–ç•¥ | PPO ç®—æ³•ï¼ˆPolicy Gradientï¼‰ |\n",
    "\n",
    "### å®éªŒè®¾ç½®ï¼šæ¨¡å‹ä¸æ•°æ®é›†é€‰æ‹©\n",
    "- **æ¨¡å‹**ï¼šQwen2.5-1.5B-Instructï¼ˆä¸­æ–‡æŒ‡ä»¤èƒ½åŠ›å¼ºï¼Œå°å‚æ•°ã€æ˜“äº LoRA/QLoRAï¼‰\n",
    "- **SFT æ•°æ®**ï¼šBelleGroup/train_0.5M_CNï¼ˆä¸­æ–‡æŒ‡ä»¤-å›ç­”å¯¹ï¼Œä½“é‡é€‚ä¸­ï¼Œå¯é‡‡æ ·ï¼‰\n",
    "- **åå¥½æ•°æ®ï¼ˆç”¨äº DPO/RMï¼‰**ï¼šargilla/ultrafeedback-binarized-preferencesï¼ˆæˆå¯¹åå¥½ï¼Œæ˜“ç›´æ¥ç”¨äº DPOï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3104c69a",
   "metadata": {},
   "source": [
    "## ç¯å¢ƒå®‰è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b3c68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…éœ€è¦çš„åº“\n",
    "%env TOKENIZERS_PARALLELISM=false\n",
    "%pip install -q torch torchvision torchaudio\n",
    "%pip install -q \"transformers>=4.44.0\" \"datasets>=2.18.0\" \"accelerate>=0.33.0\" \\\n",
    "                \"peft>=0.12.0\" \"trl>=0.9.6\" \"sentencepiece>=0.1.99\" \"safetensors>=0.4.5\" \\\n",
    "                \"huggingface_hub>=0.24.0\" \"modelscope>=1.14.0\" \"protobuf>=4.25.0\" \\\n",
    "                \"numpy>=1.24.0\" \"scipy>=1.10.0\" \"tiktoken>=0.7.0\" \n",
    "\n",
    "# ä»…åœ¨ CUDA å¯ç”¨æ—¶å®‰è£… bitsandbytesï¼ˆå¯é€‰ï¼‰\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    %pip install -q \"bitsandbytes>=0.43.0\"\n",
    "\n",
    "# æ‰“å°å…³é”®ç‰ˆæœ¬ï¼Œä¾¿äºæ’æŸ¥\n",
    "import importlib.metadata as im\n",
    "v = lambda n: (im.version(n) if n in {d.metadata['Name'] for d in im.distributions()} else 'N/A')\n",
    "print(\"[Versions]\",\n",
    "      \"torch=\", v(\"torch\"),\n",
    "      \"transformers=\", v(\"transformers\"),\n",
    "      \"datasets=\", v(\"datasets\"),\n",
    "      \"accelerate=\", v(\"accelerate\"),\n",
    "      \"peft=\", v(\"peft\"),\n",
    "      \"trl=\", v(\"trl\"),\n",
    "      \"modelscope=\", v(\"modelscope\"),\n",
    "      \"sentencepiece=\", v(\"sentencepiece\"),\n",
    "      \"bitsandbytes=\", v(\"bitsandbytes\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a578b03c",
   "metadata": {},
   "source": [
    "## æ¨¡å‹ä¸‹è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a203c0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /Users/arkin/.cache/modelscope/hub/models/Qwen/Qwen2.5-1.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-01 23:44:09,291 - modelscope - INFO - Target directory already exists, skipping creation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Device] mps\n",
      "ä½ å¥½ï¼Œç®€è¦ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚ ä½œä¸ºä¸€ä¸ªAIåŠ©æ‰‹ï¼Œæˆ‘å«é€šä¹‰åƒé—®ã€‚æˆ‘å¯ä»¥å›ç­”å„ç§é—®é¢˜ã€æä¾›ä¿¡æ¯å’Œå¸®åŠ©æ‚¨å®Œæˆä»»åŠ¡ã€‚æœ‰ä»€ä¹ˆæˆ‘èƒ½å¸®æ‚¨çš„å—ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "# ä¸‹è½½Qwen/Qwen2.5-1.5B-Instruct\n",
    "import torch\n",
    "from modelscope.hub.snapshot_download import snapshot_download\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"  # ModelScope ä¸Šçš„æ¨¡å‹æ ‡è¯†ï¼ˆå…¬å…±å¯ç›´æ¥ä¸‹è½½ï¼‰\n",
    "\n",
    "# é€‰æ‹©è®¾å¤‡\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# é€šè¿‡ ModelScope ä¸‹è½½åˆ°æœ¬åœ°ç¼“å­˜ï¼Œç„¶åç”¨ Transformers ä»æœ¬åœ°ç›®å½•åŠ è½½\n",
    "model_dir = snapshot_download(model_id)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir, trust_remote_code=True)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"[Device] {device}\")\n",
    "\n",
    "# ç®€å•è‡ªæ£€\n",
    "txt = \"ä½ å¥½ï¼Œç®€è¦ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚\"\n",
    "inputs = tokenizer(txt, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "    out = model.generate(**inputs, max_new_tokens=64, do_sample=False)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8784880",
   "metadata": {},
   "source": [
    "## æ•°æ®é›†ä¸‹è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fb0edcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-01 23:44:37,906 - modelscope - INFO - Fetching dataset repo file list...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Dataset to directory: /Users/arkin/.cache/modelscope/hub/datasets/AI-ModelScope/train_0.5M_CN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-01 23:44:39,194 - modelscope - INFO - Fetching dataset repo file list...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Dataset to directory: /Users/arkin/.cache/modelscope/hub/datasets/HuggingFaceH4/ultrafeedback_binarized\n"
     ]
    }
   ],
   "source": [
    "# ä¸‹è½½sftæ•°æ®é›†å’Œåå¥½æ•°æ®é›†\n",
    "from modelscope.hub.snapshot_download import snapshot_download\n",
    "\n",
    "# æŒ‡å®šä¸¤ä¸ªæ•°æ®é›†åç§°ï¼ˆå¯æŒ‰éœ€ä¿®æ”¹ï¼‰\n",
    "sft_id = \"AI-ModelScope/train_0.5M_CN\"  # ä¸­æ–‡æ•°æ®é›†\n",
    "pref_id_primary = \"HuggingFaceH4/ultrafeedback_binarized\" # å¤šè¯­ï¼Œè‹±æ–‡ä¸ºä¸»ï¼Œå¯ç­›å‡ºä¸­æ–‡å­é›†\n",
    " \n",
    "# ä»…ä½¿ç”¨ ModelScope ä¸‹è½½åˆ°æœ¬åœ°ç¼“å­˜ï¼ˆä¸åšå›é€€ï¼‰\n",
    "sft_dir = snapshot_download(sft_id, repo_type=\"dataset\")\n",
    "pref_dir = snapshot_download(pref_id_primary, repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6915e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“˜ åŠ è½½å¹¶é¢„è§ˆ SFT æ•°æ®\n",
      "[SFT] é¢„è§ˆ 2 æ¡ / split=train[:2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>instr_len</th>\n",
       "      <th>input_len</th>\n",
       "      <th>output_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ç»™å®šä¸€ä¸ªè‹±æ–‡å¥å­ï¼Œç¿»è¯‘æˆä¸­æ–‡ã€‚\\nI love to learn new things every day.\\n</td>\n",
       "      <td></td>\n",
       "      <td>æˆ‘æ¯å¤©å–œæ¬¢å­¦ä¹ æ–°äº‹ç‰©ã€‚</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ç»™å®šä¸€ä¸ªæ–‡å­—è¾“å…¥ï¼Œå°†å…¶ä¸­çš„æ‰€æœ‰æ•°å­—åŠ 1ã€‚\\nâ€œæ˜å¤©çš„ä¼šè®®åœ¨9ç‚¹å¼€å§‹ï¼Œè®°å¾—å‡†æ—¶åˆ°è¾¾ã€‚â€\\n</td>\n",
       "      <td></td>\n",
       "      <td>â€œæ˜å¤©çš„ä¼šè®®åœ¨10ç‚¹å¼€å§‹ï¼Œè®°å¾—å‡†æ—¶åˆ°è¾¾ã€‚â€</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                instruction input  \\\n",
       "0  ç»™å®šä¸€ä¸ªè‹±æ–‡å¥å­ï¼Œç¿»è¯‘æˆä¸­æ–‡ã€‚\\nI love to learn new things every day.\\n         \n",
       "1              ç»™å®šä¸€ä¸ªæ–‡å­—è¾“å…¥ï¼Œå°†å…¶ä¸­çš„æ‰€æœ‰æ•°å­—åŠ 1ã€‚\\nâ€œæ˜å¤©çš„ä¼šè®®åœ¨9ç‚¹å¼€å§‹ï¼Œè®°å¾—å‡†æ—¶åˆ°è¾¾ã€‚â€\\n         \n",
       "\n",
       "                  output  instr_len  input_len  output_len  \n",
       "0            æˆ‘æ¯å¤©å–œæ¬¢å­¦ä¹ æ–°äº‹ç‰©ã€‚         54          0          11  \n",
       "1  â€œæ˜å¤©çš„ä¼šè®®åœ¨10ç‚¹å¼€å§‹ï¼Œè®°å¾—å‡†æ—¶åˆ°è¾¾ã€‚â€         42          0          21  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“— åŠ è½½å¹¶é¢„è§ˆ Preference æ•°æ®\n",
      "[Preference Primary] é¢„è§ˆ 2 æ¡\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>y_pos</th>\n",
       "      <th>y_neg</th>\n",
       "      <th>prompt_len</th>\n",
       "      <th>y_pos_len</th>\n",
       "      <th>y_neg_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how can i develop a habit of drawing daily</td>\n",
       "      <td>user: how can i develop a habit of drawing daily\\n---\\nassistant: Developing a daily habit of drawing can be challenging but with consistent practice and a few tips, it can become an enjoyable and rewarding part of your daily routine. Here are some strategies to help you develop the habit of drawing daily:\\n\\n1. Set a specific time: Allocate a specific time of the day to draw. It could be in the morning, afternoon, or evening. Make drawing a part of your daily routine.\\n2. Set a specific duration: Determine the amount of time you want to spend on drawing each day. It can be as little as 10 minutes or as long as an hour. Be consistent with the duration to help build the habit.\\n3. Start small and simple: Don't try to create a masterpiece every day, start with simple and easy-to-do sketches. Focus on improving your skills gradually.\\n4. Use a variety of tools and mediums: Experiment with different tools like pencils, pens, markers, and different mediums like paper, canvas, or digital apps to keep your drawing practice interesting and engaging.\\n5. Take breaks and rest: Taking breaks and resting after some time of drawing can help you avoid burnout and stay motivated.\\n6. Challenge yourself: Set challenges like drawing objects from memory or a specific subject to improve your skills and keep your drawing practice interesting.\\n7. Track your progress: Keep a record of your daily drawing practice and track your progress. This can be a source of motivation and help you see how far you've come.\\n\\nRemember, developing a habit takes time and patience. Stay consistent with your drawing practice, be flexible and open to trying new things, and with time, you'll develop a habit of daily drawing that brings you joy and satisfaction.</td>\n",
       "      <td>user: how can i develop a habit of drawing daily\\n---\\nassistant: As an AI language model, I cannot personally develop habits for you. But, here are some tips for developing a habit of drawing daily:\\n\\n1. Start small: Start with simple drawings or doodles and gradually increase the complexity of your drawings.\\n\\n2. Set a schedule: Set a specific time every day to draw, and try to stick to it as much as possible.\\n\\n3. Make it fun: Don't pressure yourself to create masterpieces every time you draw. Make it a relaxing and enjoyable experience.\\n\\n4. Use resources: There are many drawing tutorials available online. Use resources like YouTube or online drawing courses to help you improve your skills.\\n\\n5. Surround yourself with inspiration: Expose yourself to a variety of art forms, such as paintings, illustrations, and photographs, to inspire and motivate you.\\n\\nRemember, everyone has their own creative style and pace. Just keep practicing and enjoying the process of drawing.</td>\n",
       "      <td>42</td>\n",
       "      <td>1737</td>\n",
       "      <td>977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how can I transform the getPosition method of antv/g's group in zrender?</td>\n",
       "      <td>user: how can I transform the getPosition method of antv/g's group in zrender?\\n---\\nassistant: It is not recommended to modify built-in methods as it can lead to unexpected results and potential bugs. You may consider developing a new method or exploring other methods to achieve your desired outcome. Alternatively, you can search for other libraries or modules that offer similar functionalities or reach out to the library's support team for assistance.</td>\n",
       "      <td>user: how can I transform the getPosition method of antv/g's group in zrender?\\n---\\nassistant: Thank you for reaching out for assistance! I'm here to help you with your question. However, I must point out that the question itself may not be meaningful.\\n\\nThe `getPosition` method is a part of the AntV/G library, which is a gradient boosting framework. It is not clear what you mean by \"transforming\" this method, as it is not a functional programming concept. Additionally, the concept of \"zrender\" is not related to AntV/G.\\n\\nCould you please provide more context or clarify your question? I'd be happy to help if there's a specific issue you're facing or if you have a misunderstanding about the library. I'm here to assist you in a safe, respectful, and helpful manner.</td>\n",
       "      <td>72</td>\n",
       "      <td>455</td>\n",
       "      <td>770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     prompt  \\\n",
       "0                                how can i develop a habit of drawing daily   \n",
       "1  how can I transform the getPosition method of antv/g's group in zrender?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   y_pos  \\\n",
       "0  user: how can i develop a habit of drawing daily\\n---\\nassistant: Developing a daily habit of drawing can be challenging but with consistent practice and a few tips, it can become an enjoyable and rewarding part of your daily routine. Here are some strategies to help you develop the habit of drawing daily:\\n\\n1. Set a specific time: Allocate a specific time of the day to draw. It could be in the morning, afternoon, or evening. Make drawing a part of your daily routine.\\n2. Set a specific duration: Determine the amount of time you want to spend on drawing each day. It can be as little as 10 minutes or as long as an hour. Be consistent with the duration to help build the habit.\\n3. Start small and simple: Don't try to create a masterpiece every day, start with simple and easy-to-do sketches. Focus on improving your skills gradually.\\n4. Use a variety of tools and mediums: Experiment with different tools like pencils, pens, markers, and different mediums like paper, canvas, or digital apps to keep your drawing practice interesting and engaging.\\n5. Take breaks and rest: Taking breaks and resting after some time of drawing can help you avoid burnout and stay motivated.\\n6. Challenge yourself: Set challenges like drawing objects from memory or a specific subject to improve your skills and keep your drawing practice interesting.\\n7. Track your progress: Keep a record of your daily drawing practice and track your progress. This can be a source of motivation and help you see how far you've come.\\n\\nRemember, developing a habit takes time and patience. Stay consistent with your drawing practice, be flexible and open to trying new things, and with time, you'll develop a habit of daily drawing that brings you joy and satisfaction.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              user: how can I transform the getPosition method of antv/g's group in zrender?\\n---\\nassistant: It is not recommended to modify built-in methods as it can lead to unexpected results and potential bugs. You may consider developing a new method or exploring other methods to achieve your desired outcome. Alternatively, you can search for other libraries or modules that offer similar functionalities or reach out to the library's support team for assistance.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             y_neg  \\\n",
       "0  user: how can i develop a habit of drawing daily\\n---\\nassistant: As an AI language model, I cannot personally develop habits for you. But, here are some tips for developing a habit of drawing daily:\\n\\n1. Start small: Start with simple drawings or doodles and gradually increase the complexity of your drawings.\\n\\n2. Set a schedule: Set a specific time every day to draw, and try to stick to it as much as possible.\\n\\n3. Make it fun: Don't pressure yourself to create masterpieces every time you draw. Make it a relaxing and enjoyable experience.\\n\\n4. Use resources: There are many drawing tutorials available online. Use resources like YouTube or online drawing courses to help you improve your skills.\\n\\n5. Surround yourself with inspiration: Expose yourself to a variety of art forms, such as paintings, illustrations, and photographs, to inspire and motivate you.\\n\\nRemember, everyone has their own creative style and pace. Just keep practicing and enjoying the process of drawing.   \n",
       "1                                                                                                                                                                                                                         user: how can I transform the getPosition method of antv/g's group in zrender?\\n---\\nassistant: Thank you for reaching out for assistance! I'm here to help you with your question. However, I must point out that the question itself may not be meaningful.\\n\\nThe `getPosition` method is a part of the AntV/G library, which is a gradient boosting framework. It is not clear what you mean by \"transforming\" this method, as it is not a functional programming concept. Additionally, the concept of \"zrender\" is not related to AntV/G.\\n\\nCould you please provide more context or clarify your question? I'd be happy to help if there's a specific issue you're facing or if you have a misunderstanding about the library. I'm here to assist you in a safe, respectful, and helpful manner.   \n",
       "\n",
       "   prompt_len  y_pos_len  y_neg_len  \n",
       "0          42       1737        977  \n",
       "1          72        455        770  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Preference Primary] æ•°æ®é›†: ultrafeedback_binarized\n",
      "  - å­—æ®µ: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected']\n",
      "  - æ”¯æŒ: âœ… RM/DPO\n"
     ]
    }
   ],
   "source": [
    "# é¢„è§ˆ SFT å’Œåå¥½æ•°æ®é›†\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd, os, json, glob\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "# ========== é€šç”¨å·¥å…·å‡½æ•° ==========\n",
    "def normalize_text(x):\n",
    "    \"\"\"å°†åµŒå¥—ç»“æ„æ ‡å‡†åŒ–ä¸ºå¯è¯»å­—ç¬¦ä¸²\"\"\"\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, str):\n",
    "        return x\n",
    "    if isinstance(x, dict):\n",
    "        return x.get(\"content\") or x.get(\"text\") or x.get(\"value\") or str(x)\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        parts = []\n",
    "        for it in x:\n",
    "            if isinstance(it, str):\n",
    "                parts.append(it)\n",
    "            elif isinstance(it, dict):\n",
    "                role = it.get(\"role\")\n",
    "                content = it.get(\"content\") or it.get(\"text\") or it.get(\"value\")\n",
    "                if content:\n",
    "                    parts.append((f\"{role}: \" if role else \"\") + str(content))\n",
    "        return \"\\n---\\n\".join(parts)\n",
    "    return str(x)\n",
    "# ========== SFT æ•°æ®åŠ è½½ ==========\n",
    "print(\"ğŸ“˜ åŠ è½½å¹¶é¢„è§ˆ SFT æ•°æ®\")\n",
    "sft_preview = load_dataset(sft_dir, split=\"train[:2]\")\n",
    "sft_rows = []\n",
    "for ex in sft_preview:\n",
    "    sft_rows.append({\n",
    "        \"instruction\": normalize_text(ex.get(\"instruction\")),\n",
    "        \"input\": normalize_text(ex.get(\"input\")),\n",
    "        \"output\": normalize_text(ex.get(\"output\")),\n",
    "        \"instr_len\": len(str(ex.get(\"instruction\", \"\"))),\n",
    "        \"input_len\": len(str(ex.get(\"input\", \"\"))),\n",
    "        \"output_len\": len(str(ex.get(\"output\", \"\"))),\n",
    "    })\n",
    "print(f\"[SFT] é¢„è§ˆ {len(sft_rows)} æ¡ / split=train[:2]\")\n",
    "display(pd.DataFrame(sft_rows))\n",
    "\n",
    "# ========== åå¥½æ•°æ®åŠ è½½ï¼ˆPrimaryï¼‰ ==========\n",
    "def load_pref_data(ds_dir, name=\"Primary\"):\n",
    "    \"\"\"å°è¯•å¤šç§æ–¹å¼åŠ è½½åå¥½æ•°æ®\"\"\"\n",
    "    for split in [\"train_prefs[:2]\", \"train[:2]\"]:\n",
    "        try:\n",
    "            ds = load_dataset(ds_dir, split=split)\n",
    "            return ds, f\"{name} ({split.split('[')[0]})\"\n",
    "        except:\n",
    "            continue\n",
    "    # å°è¯•ä» data ç›®å½•åŠ è½½\n",
    "    data_dir = os.path.join(ds_dir, \"data\")\n",
    "    if not os.path.exists(data_dir):\n",
    "        return None, None\n",
    "    parquet = glob.glob(os.path.join(data_dir, \"*.parquet\"))\n",
    "    if parquet:\n",
    "        ds = Dataset.from_parquet(parquet[0]).select(range(2))\n",
    "        return ds, f\"{name} (Parquet)\"\n",
    "    jsonl = glob.glob(os.path.join(data_dir, \"*.jsonl\"))\n",
    "    if jsonl:\n",
    "        data = []\n",
    "        with open(jsonl[0], \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= 2: break\n",
    "                try: data.append(json.loads(line))\n",
    "                except: pass\n",
    "        return Dataset.from_list(data), f\"{name} (JSONL)\" if data else (None, None)\n",
    "    return None, None\n",
    "\n",
    "def format_pref_data(pref_ds, name=\"Primary\"):\n",
    "    if pref_ds is None:\n",
    "        print(f\"[Preference {name}] âŒ æ— æ³•åŠ è½½\")\n",
    "        return\n",
    "    rows = []\n",
    "    for ex in pref_ds:\n",
    "        row = {\n",
    "            \"prompt\": normalize_text(ex.get(\"prompt\") or ex.get(\"instruction\") or ex.get(\"input\")),\n",
    "            \"y_pos\": normalize_text(ex.get(\"chosen\") or ex.get(\"better_response\") or ex.get(\"pos\")),\n",
    "            \"y_neg\": normalize_text(ex.get(\"rejected\") or ex.get(\"worse_response\") or ex.get(\"neg\")),\n",
    "        }\n",
    "        row[\"prompt_len\"] = len(str(row[\"prompt\"] or \"\"))\n",
    "        row[\"y_pos_len\"] = len(str(row[\"y_pos\"] or \"\"))\n",
    "        row[\"y_neg_len\"] = len(str(row[\"y_neg\"] or \"\"))\n",
    "        rows.append(row)\n",
    "    print(f\"[Preference {name}] é¢„è§ˆ {len(rows)} æ¡\")\n",
    "    display(pd.DataFrame(rows))\n",
    "\n",
    "print(\"ğŸ“— åŠ è½½å¹¶é¢„è§ˆ Preference æ•°æ®\")\n",
    "pref_preview, pref_source = load_pref_data(pref_dir, \"Primary\")\n",
    "format_pref_data(pref_preview, \"Primary\")\n",
    "\n",
    "print(f\"\\n[Preference Primary] æ•°æ®é›†: {pref_dir.split('/')[-1]}\")\n",
    "if pref_preview and len(pref_preview) > 0:\n",
    "    fields = list(pref_preview[0].keys())\n",
    "    has_chosen = any(k in fields for k in [\"chosen\", \"better_response\"])\n",
    "    has_rejected = any(k in fields for k in [\"rejected\", \"worse_response\"])\n",
    "    print(f\"  - å­—æ®µ: {fields}\")\n",
    "    print(f\"  - æ”¯æŒ: {'âœ… RM/DPO' if has_chosen and has_rejected else 'âš ï¸  éƒ¨åˆ†æ”¯æŒ'}\")\n",
    "else:\n",
    "    print(\"  - çŠ¶æ€: âŒ æœªåŠ è½½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c63d2f",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰\n",
    "- **è¾“å…¥**ï¼šæŒ‡ä»¤-å›ç­”å¯¹ï¼ˆé«˜è´¨é‡ã€äººç±»ä¹¦å†™/ç­›é€‰ï¼‰\n",
    "- **ç›®æ ‡**ï¼šè®©æ¨¡å‹åŸºæœ¬å­¦ä¼šâ€œæŒ‰æŒ‡ä»¤ä½œç­”â€\n",
    "- **è®­ç»ƒ**ï¼šæœ€å°åŒ–äº¤å‰ç†µæŸå¤±ï¼ˆå‚è€ƒå¸¸ç”¨æŒ‡ä»¤æ•°æ®é›†ï¼‰\n",
    "- **è¾“å‡º**ï¼šSFT æ¨¡å‹ï¼ˆä½œä¸ºåç»­ RM/PPO çš„å‚è€ƒç­–ç•¥ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd883faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[æ–‡ä»¶åˆ—è¡¨] ['.mdl', 'Belle_open_source_0.5M.jsonl', 'README.md', '.msc', '.gitattributes', '.mv', 'train_0.5M_CN.json']\n",
      "[åŠ è½½æ–‡ä»¶] /Users/arkin/.cache/modelscope/hub/datasets/AI-ModelScope/train_0___5M_CN/Belle_open_source_0.5M.jsonl\n",
      "[SFT] æˆåŠŸåŠ è½½ 519255 æ¡æ ·æœ¬ã€‚\n",
      "[ç¤ºä¾‹æ ·æœ¬] {'instruction': 'ç»™å®šä¸€ä¸ªè‹±æ–‡å¥å­ï¼Œç¿»è¯‘æˆä¸­æ–‡ã€‚\\nI love to learn new things every day.\\n', 'input': '', 'output': 'æˆ‘æ¯å¤©å–œæ¬¢å­¦ä¹ æ–°äº‹ç‰©ã€‚'}\n"
     ]
    }
   ],
   "source": [
    "# ã€åŠ è½½sftæ•°æ®é›†ã€‘ dataset\n",
    "from datasets import Dataset\n",
    "import os\n",
    "import json\n",
    "\n",
    "sft_path = os.path.expanduser(sft_dir)\n",
    "\n",
    "# æ£€æŸ¥ç›®å½•å†…å®¹\n",
    "print(\"[æ–‡ä»¶åˆ—è¡¨]\", os.listdir(sft_path))\n",
    "\n",
    "# å°è¯•æ‰¾åˆ° JSON æˆ– JSONL æ–‡ä»¶\n",
    "json_files = [f for f in os.listdir(sft_path) if f.endswith(\".json\") or f.endswith(\".jsonl\")]\n",
    "if not json_files:\n",
    "    raise RuntimeError(f\"åœ¨ {sft_path} æœªæ‰¾åˆ° JSON/JSONL æ–‡ä»¶ï¼Œè¯·æ‰‹åŠ¨æŸ¥çœ‹æ–‡ä»¶ç»“æ„ã€‚\")\n",
    "\n",
    "json_path = os.path.join(sft_path, json_files[0])\n",
    "print(f\"[åŠ è½½æ–‡ä»¶] {json_path}\")\n",
    "\n",
    "# åŠ è½½ JSON æ•°æ®\n",
    "data = []\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            data.append(json.loads(line))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# è½¬æ¢æˆ HuggingFace Dataset\n",
    "sft_ds = Dataset.from_list(data)\n",
    "print(f\"[SFT] æˆåŠŸåŠ è½½ {len(sft_ds)} æ¡æ ·æœ¬ã€‚\")\n",
    "print(\"[ç¤ºä¾‹æ ·æœ¬]\", sft_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "026ca34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd7836a8cb64abcbaab8976bede51eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/508869 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "501d73a17ab34c8286ce7dc81f23a7ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10386 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è®­ç»ƒé›†æ ·æœ¬æ•°: 508869\n",
      "éªŒè¯é›†æ ·æœ¬æ•°: 10386\n",
      "[ç¤ºä¾‹] {'prompt': 'æŸ¥æ‰¾ä¸€ä½è‘—åéŸ³ä¹å®¶çš„å‡ºç”Ÿåœ°å’Œæœ€è‘—åçš„ä½œå“ã€‚\\néŸ³ä¹å®¶ï¼šè´å¤šèŠ¬ ', 'completion': 'è´å¤šèŠ¬çš„å‡ºç”Ÿåœ°æ˜¯å¾·å›½çš„æ³¢æ©å¸‚ã€‚ä»–æœ€è‘—åçš„ä½œå“åŒ…æ‹¬ç¬¬ä¹äº¤å“æ›²ã€ç¬¬äº”äº¤å“æ›²ã€æœˆå…‰å¥é¸£æ›²å’Œè´¹åŠ ç½—çš„è¿·å®«ç­‰ã€‚'}\n"
     ]
    }
   ],
   "source": [
    "# ã€æ•°æ®é›†åˆ‡åˆ†ã€‘æŒ‰æ¯”ä¾‹åˆ’åˆ†ï¼Œæ¯”å¦‚ 98% è®­ç»ƒã€2% éªŒè¯\n",
    "sft_split = sft_ds.train_test_split(test_size=0.02, seed=42)\n",
    "\n",
    "# é‡æ„æ•°æ®æ ¼å¼ï¼šæ„é€  prompt-completion ç»“æ„ï¼ˆç¬¦åˆ TRL 0.24 æ ‡å‡†ï¼‰\n",
    "def _to_sft(example):\n",
    "    instr = example.get(\"instruction\", \"\")\n",
    "    inp = example.get(\"input\", \"\")\n",
    "    output = example.get(\"output\", None)\n",
    "    # ä½¿ç”¨ç©ºæ ¼ç»“å°¾ï¼Œé¿å…åˆ†è¯è¾¹ç•Œé—®é¢˜\n",
    "    prompt = (instr + (\"\\n\" + inp if inp else \"\")).strip() + \" \"\n",
    "    return {\"prompt\": prompt, \"completion\": output}\n",
    "\n",
    "# åˆ‡åˆ†å¹¶æ ¼å¼åŒ–\n",
    "train_ds = sft_split[\"train\"].map(_to_sft, remove_columns=sft_split[\"train\"].column_names)\n",
    "eval_ds = sft_split[\"test\"].map(_to_sft, remove_columns=sft_split[\"test\"].column_names)\n",
    "\n",
    "print(\"è®­ç»ƒé›†æ ·æœ¬æ•°:\", len(train_ds))\n",
    "print(\"éªŒè¯é›†æ ·æœ¬æ•°:\", len(eval_ds))\n",
    "print(\"[ç¤ºä¾‹]\", train_ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9549d028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] å½“å‰ä¸ºé CUDA ç¯å¢ƒï¼Œä½¿ç”¨å¸¸è§„ç²¾åº¦åŠ è½½\n"
     ]
    }
   ],
   "source": [
    "# ã€LoRA é…ç½®ä¸è®­ç»ƒå‚æ•°ã€‘\n",
    "from peft import LoraConfig\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# æ¨¡å‹è¾“å…¥è¾“å‡ºé…ç½®\n",
    "base_model_or_dir = model_dir  # å¤ç”¨ ModelScope ä¸‹è½½çš„æ¨¡å‹ç›®å½•\n",
    "output_dir = \"outputs/sft_qlora\"\n",
    "max_seq_length = 2048\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_mps = torch.backends.mps.is_available()\n",
    "\n",
    "# bitsandbytes QLoRA é…ç½® \n",
    "quantization_config = None\n",
    "if use_cuda:\n",
    "    try:\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "        print(\"[QLoRA] ä½¿ç”¨ bitsandbytes 4-bit é‡åŒ–åŠ è½½æ¨¡å‹\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Warn] æœªå¯ç”¨4bité‡åŒ–ï¼š{e}\")\n",
    "else:\n",
    "    print(\"[Info] å½“å‰ä¸ºé CUDA ç¯å¢ƒï¼Œä½¿ç”¨å¸¸è§„ç²¾åº¦åŠ è½½\")\n",
    "\n",
    "# Tokenizer åˆå§‹åŒ–\n",
    "if tokenizer is None:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_or_dir, use_fast=True, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528cc6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA é…ç½®ï¼šä½ç§©é€‚é…å™¨å‚æ•°\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                    # ç§©ï¼ˆrankï¼‰ï¼Œæ§åˆ¶é€‚é…å™¨çŸ©é˜µçš„ç»´åº¦ï¼Œè¶Šå¤§å®¹é‡è¶Šå¤§ä½†å‚æ•°é‡æ›´å¤šï¼ˆå¸¸ç”¨ 8/16/32ï¼‰\n",
    "    lora_alpha=32,          # ç¼©æ”¾ç³»æ•°ï¼Œä¸ r æˆæ¯”ä¾‹ä½¿ç”¨ï¼ˆé€šå¸¸ alpha=2*rï¼‰\n",
    "    lora_dropout=0.05,      # é€‚é…å™¨å±‚ dropoutï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œå¸¸ç”¨ 0.05-0.1ï¼‰\n",
    "    bias=\"none\",            # æ˜¯å¦è®­ç»ƒ biasï¼ˆ\"none\"/\"all\"/\"lora_only\"ï¼‰\n",
    "    task_type=\"CAUSAL_LM\",   # ä»»åŠ¡ç±»å‹ï¼šå› æœè¯­è¨€æ¨¡å‹ï¼ˆç”Ÿæˆä»»åŠ¡ï¼‰\n",
    ")\n",
    "\n",
    "# è®­ç»ƒå‚æ•°ï¼šæ§åˆ¶è®­ç»ƒæµç¨‹ä¸ä¼˜åŒ–\n",
    "args = TrainingArguments(\n",
    "    # è¾“å‡ºä¸ä¿å­˜\n",
    "    output_dir=output_dir,              # æ¨¡å‹/æ—¥å¿—è¾“å‡ºç›®å½•\n",
    "    save_steps=200,                     # æ¯ N æ­¥ä¿å­˜ä¸€æ¬¡ checkpoint\n",
    "    save_total_limit=2,                 # æœ€å¤šä¿ç•™ N ä¸ª checkpointï¼ˆé¿å…å ç£ç›˜ï¼‰\n",
    "    save_safetensors=True,              # ä½¿ç”¨ safetensors æ ¼å¼ä¿å­˜ï¼ˆæ›´å¿«æ›´å®‰å…¨ï¼‰\n",
    "    \n",
    "    # æ‰¹å¤§å°ä¸æ¢¯åº¦\n",
    "    per_device_train_batch_size=1,      # æ¯è®¾å¤‡è®­ç»ƒ batch å¤§å°ï¼ˆæ˜¾å­˜å—é™æ—¶å…ˆç”¨ 1ï¼‰\n",
    "    per_device_eval_batch_size=1,       # æ¯è®¾å¤‡éªŒè¯ batch å¤§å°\n",
    "    gradient_accumulation_steps=8,      # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆç­‰æ•ˆ batch = 1 * 8 = 8ï¼‰\n",
    "    \n",
    "    # è®­ç»ƒè½®æ¬¡ä¸å­¦ä¹ ç‡\n",
    "    num_train_epochs=1,                 # è®­ç»ƒè½®æ¬¡æ•°\n",
    "    learning_rate=2e-4,                 # åˆå§‹å­¦ä¹ ç‡ï¼ˆLoRA å¸¸ç”¨ 1e-4 åˆ° 5e-4ï¼‰\n",
    "    lr_scheduler_type=\"cosine\",         # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆä½™å¼¦é€€ç«ï¼‰\n",
    "    warmup_ratio=0.03,                  # warmup æ¯”ä¾‹ï¼ˆå‰ 3% æ­¥æ•°çº¿æ€§å¢é•¿ LRï¼‰\n",
    "    \n",
    "    # è¯„ä¼°ä¸æ—¥å¿—\n",
    "    eval_strategy=\"steps\",              # è¯„ä¼°ç­–ç•¥ï¼ˆ\"steps\"/\"epoch\"/\"no\"ï¼‰\n",
    "    eval_steps=100,                     # æ¯ N æ­¥è¯„ä¼°ä¸€æ¬¡\n",
    "    logging_steps=10,                   # æ¯ N æ­¥æ‰“å°ä¸€æ¬¡æ—¥å¿—\n",
    "    report_to=[\"none\"],                 # ä¸å‘å¤–éƒ¨ä¸ŠæŠ¥ï¼ˆå¯æ”¹ä¸º [\"wandb\"] ç­‰ï¼‰\n",
    "    \n",
    "    # æ¨¡å‹æ£€æŸ¥ç‚¹\n",
    "    load_best_model_at_end=True,        # è®­ç»ƒç»“æŸåŠ è½½æœ€ä½³æ¨¡å‹\n",
    "    metric_for_best_model=\"eval_loss\",  # æœ€ä½³æ¨¡å‹æŒ‡æ ‡\n",
    "    greater_is_better=False,            # è¯¥æŒ‡æ ‡è¶Šå°è¶Šå¥½\n",
    "    \n",
    "    # æ€§èƒ½ä¼˜åŒ–\n",
    "    bf16=use_cuda,                      # CUDA ä¸Šç”¨ bfloat16ï¼ˆç®—åŠ›ä¸ç¨³å®šæ€§å¹³è¡¡ï¼‰\n",
    "    fp16=False,                         # ç¦ç”¨ FP16ï¼ˆé¿å…å†²çªï¼‰\n",
    "    gradient_checkpointing=True,        # æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆç‰ºç‰²æ—¶é—´æ¢æ˜¾å­˜ï¼‰\n",
    ")\n",
    "\n",
    "print(\"è®­ç»ƒå‚æ•°é…ç½®å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5244aa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰æ ¼å¼åŒ–å‡½æ•°ï¼šæ„é€ è®­ç»ƒæ–‡æœ¬æ¨¡æ¿\n",
    "def formatting_func(example):\n",
    "    \"\"\"å°† prompt-response å¯¹è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ–‡æœ¬ï¼ˆå•æ¡å¤„ç†ï¼‰\"\"\"\n",
    "    prompt = str(example[\"prompt\"]).strip()\n",
    "    resp = str(example[\"response\"]).strip()\n",
    "    # æ‹¼æ¥æ ¼å¼ï¼šç¬¦åˆ Qwen çš„æ¨¡æ¿\n",
    "    text = f\"<|user|>\\n{prompt}\\n<|assistant|>\\n{resp}{tokenizer.eos_token}\"\n",
    "    return text\n",
    "\n",
    "print(\"[æ ¼å¼åŒ–å‡½æ•°å·²å®šä¹‰]\")\n",
    "print(\"[ç¤ºä¾‹]\", formatting_func({\"prompt\": \"ä½ å¥½\", \"response\": \"ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f75cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒå™¨çš„æ„é€ \n",
    "from trl import SFTTrainer\n",
    "\n",
    "# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆQLoRA æ¨¡å¼ä¸‹è‡ªåŠ¨åº”ç”¨é‡åŒ–ï¼‰\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_or_dir,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\" if use_cuda else None,\n",
    ")\n",
    "\n",
    "# æ„é€  SFT è®­ç»ƒå™¨\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    peft_config=lora_config,\n",
    "    args=args,\n",
    "    # ä¸ä½¿ç”¨ formatting_funcï¼Œç›´æ¥ç”¨ prompt-completion ç»“æ„\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eba8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¼€å§‹è®­ç»ƒ\n",
    "trainer.train()\n",
    "\n",
    "# === ä¿å­˜é€‚é…å™¨ï¼ˆLoRA æƒé‡ï¼‰===\n",
    "adapter_dir = os.path.join(output_dir, \"adapter\")\n",
    "trainer.model.save_pretrained(adapter_dir)\n",
    "print(f\"[Done] SFT + QLoRA è®­ç»ƒå®Œæˆï¼ŒLoRA æƒé‡å·²ä¿å­˜è‡³: {adapter_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcc3230",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰è®­ç»ƒ\n",
    "- **è¾“å…¥**ï¼šåŒä¸€æŒ‡ä»¤ä¸‹æˆå¯¹å›ç­”ï¼ˆAã€Bï¼‰ï¼Œä»¥åŠåå¥½æ ‡ç­¾ï¼ˆA > Bï¼‰\n",
    "- **ç›®æ ‡**ï¼šå­¦ä¹ â€œåå¥½è¯„åˆ†å‡½æ•°â€ r(x, y)\n",
    "- **è®­ç»ƒ**ï¼šPairwise rankingï¼ˆå¦‚ Bradleyâ€“Terry/Logistic lossï¼‰\n",
    "- **è¾“å‡º**ï¼šèƒ½å¯¹ä»»æ„å›ç­”æ‰“åˆ†çš„å¥–åŠ±æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36e6eb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RM] ä»æœ¬åœ°ç›®å½•åŠ è½½æˆåŠŸï¼ˆtrain_prefsï¼‰ï¼Œå…± 61135 æ¡æ ·æœ¬\n",
      "\n",
      "[RM] æ•°æ®é›†å­—æ®µï¼šdict_keys(['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'])\n",
      "[RM] ç¤ºä¾‹æ ·æœ¬ï¼š\n",
      "  prompt: how can i develop a habit of drawing daily\n",
      "  prompt_id: 086b3e24f29b8956a01059f79c56db35d118a06fb6b844b095737d042795cd43\n",
      "  chosen: [{'content': 'how can i develop a habit of drawing daily', 'role': 'user'}, {'content': \"Developing a daily habit of drawing can be challenging but with consistent practice and a few tips, it can become an enjoyable and rewarding part of your daily routine. Here are some strategies to help you develop the habit of drawing daily:\\n\\n1. Set a specific time: Allocate a specific time of the day to draw. It could be in the morning, afternoon, or evening. Make drawing a part of your daily routine.\\n2. Set a specific duration: Determine the amount of time you want to spend on drawing each day. It can be as little as 10 minutes or as long as an hour. Be consistent with the duration to help build the habit.\\n3. Start small and simple: Don't try to create a masterpiece every day, start with simple and easy-to-do sketches. Focus on improving your skills gradually.\\n4. Use a variety of tools and mediums: Experiment with different tools like pencils, pens, markers, and different mediums like paper, canvas, or digital apps to keep your drawing practice interesting and engaging.\\n5. Take breaks and rest: Taking breaks and resting after some time of drawing can help you avoid burnout and stay motivated.\\n6. Challenge yourself: Set challenges like drawing objects from memory or a specific subject to improve your skills and keep your drawing practice interesting.\\n7. Track your progress: Keep a record of your daily drawing practice and track your progress. This can be a source of motivation and help you see how far you've come.\\n\\nRemember, developing a habit takes time and patience. Stay consistent with your drawing practice, be flexible and open to trying new things, and with time, you'll develop a habit of daily drawing that brings you joy and satisfaction.\", 'role': 'assistant'}]\n",
      "  rejected: [{'content': 'how can i develop a habit of drawing daily', 'role': 'user'}, {'content': \"As an AI language model, I cannot personally develop habits for you. But, here are some tips for developing a habit of drawing daily:\\n\\n1. Start small: Start with simple drawings or doodles and gradually increase the complexity of your drawings.\\n\\n2. Set a schedule: Set a specific time every day to draw, and try to stick to it as much as possible.\\n\\n3. Make it fun: Don't pressure yourself to create masterpieces every time you draw. Make it a relaxing and enjoyable experience.\\n\\n4. Use resources: There are many drawing tutorials available online. Use resources like YouTube or online drawing courses to help you improve your skills.\\n\\n5. Surround yourself with inspiration: Expose yourself to a variety of art forms, such as paintings, illustrations, and photographs, to inspire and motivate you.\\n\\nRemember, everyone has their own creative style and pace. Just keep practicing and enjoying the process of drawing.\", 'role': 'assistant'}]\n",
      "  messages: [{'content': 'how can i develop a habit of drawing daily', 'role': 'user'}, {'content': \"Developing a daily habit of drawing can be challenging but with consistent practice and a few tips, it can become an enjoyable and rewarding part of your daily routine. Here are some strategies to help you develop the habit of drawing daily:\\n\\n1. Set a specific time: Allocate a specific time of the day to draw. It could be in the morning, afternoon, or evening. Make drawing a part of your daily routine.\\n2. Set a specific duration: Determine the amount of time you want to spend on drawing each day. It can be as little as 10 minutes or as long as an hour. Be consistent with the duration to help build the habit.\\n3. Start small and simple: Don't try to create a masterpiece every day, start with simple and easy-to-do sketches. Focus on improving your skills gradually.\\n4. Use a variety of tools and mediums: Experiment with different tools like pencils, pens, markers, and different mediums like paper, canvas, or digital apps to keep your drawing practice interesting and engaging.\\n5. Take breaks and rest: Taking breaks and resting after some time of drawing can help you avoid burnout and stay motivated.\\n6. Challenge yourself: Set challenges like drawing objects from memory or a specific subject to improve your skills and keep your drawing practice interesting.\\n7. Track your progress: Keep a record of your daily drawing practice and track your progress. This can be a source of motivation and help you see how far you've come.\\n\\nRemember, developing a habit takes time and patience. Stay consistent with your drawing practice, be flexible and open to trying new things, and with time, you'll develop a habit of daily drawing that brings you joy and satisfaction.\", 'role': 'assistant'}]\n",
      "  score_chosen: 8.5\n",
      "  score_rejected: 8.5\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½åå¥½æ•°æ®é›†\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "if 'pref_dir' in globals():\n",
    "    for split in [\"train_prefs\", \"train\"]:\n",
    "        try:\n",
    "            pref_ds = load_dataset(pref_dir, split=split)\n",
    "            print(f\"[RM] ä»æœ¬åœ°ç›®å½•åŠ è½½æˆåŠŸï¼ˆ{split}ï¼‰ï¼Œå…± {len(pref_ds)} æ¡æ ·æœ¬\")\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "    # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•ä» data ç›®å½•åŠ è½½ Parquet/JSONL\n",
    "    if pref_ds is None:\n",
    "        data_dir = os.path.join(pref_dir, \"data\") if os.path.exists(os.path.join(pref_dir, \"data\")) else pref_dir\n",
    "        parquet_files = glob.glob(os.path.join(data_dir, \"*.parquet\"))\n",
    "        if parquet_files:\n",
    "            pref_ds = Dataset.from_parquet(parquet_files[0])\n",
    "            print(f\"[RM] ä» Parquet åŠ è½½æˆåŠŸï¼Œå…± {len(pref_ds)} æ¡æ ·æœ¬\")\n",
    "        else:\n",
    "            jsonl_files = glob.glob(os.path.join(data_dir, \"*.jsonl\"))\n",
    "            if jsonl_files:\n",
    "                data = []\n",
    "                with open(jsonl_files[0], \"r\", encoding=\"utf-8\") as f:\n",
    "                    for line in f:\n",
    "                        try:\n",
    "                            data.append(json.loads(line))\n",
    "                        except:\n",
    "                            pass\n",
    "                pref_ds = Dataset.from_list(data)\n",
    "                print(f\"[RM] ä» JSONL åŠ è½½æˆåŠŸï¼Œå…± {len(pref_ds)} æ¡æ ·æœ¬\")\n",
    "if pref_ds is None:\n",
    "    raise RuntimeError(\"æ— æ³•åŠ è½½åå¥½æ•°æ®é›†ï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„\")\n",
    "\n",
    "# é¢„è§ˆæ•°æ®ç»“æ„\n",
    "print(f\"\\n[RM] æ•°æ®é›†å­—æ®µï¼š{pref_ds[0].keys()}\")\n",
    "print(f\"[RM] ç¤ºä¾‹æ ·æœ¬ï¼š\")\n",
    "example = pref_ds[0]\n",
    "for k, v in example.items():\n",
    "    if isinstance(v, str) and len(v) > 100:\n",
    "        print(f\"  {k}: {v[:100]}...\")\n",
    "    else:\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e66ae9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RM] å¼€å§‹è½¬æ¢æ•°æ®æ ¼å¼...\n",
      "[RM] æ ¼å¼è½¬æ¢å®Œæˆï¼Œæœ‰æ•ˆæ ·æœ¬æ•°ï¼š61135\n"
     ]
    }
   ],
   "source": [
    "# è½¬æ¢ä¸º RM è®­ç»ƒæ ¼å¼ï¼ˆprompt, chosen, rejectedï¼‰\n",
    "def normalize_text(x):\n",
    "    \"\"\"å°†åµŒå¥—ç»“æ„æ ‡å‡†åŒ–ä¸ºå¯è¯»å­—ç¬¦ä¸²\"\"\"\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, str):\n",
    "        return x\n",
    "    if isinstance(x, dict):\n",
    "        return x.get(\"content\") or x.get(\"text\") or x.get(\"value\") or str(x)\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        parts = []\n",
    "        for it in x:\n",
    "            if isinstance(it, str):\n",
    "                parts.append(it)\n",
    "            elif isinstance(it, dict):\n",
    "                role = it.get(\"role\")\n",
    "                content = it.get(\"content\") or it.get(\"text\") or it.get(\"value\")\n",
    "                if content:\n",
    "                    parts.append((f\"{role}: \" if role else \"\") + str(content))\n",
    "        return \"\\n---\\n\".join(parts)\n",
    "    return str(x)\n",
    "\n",
    "def _to_rm_format(example):\n",
    "    \"\"\"å°†åå¥½æ•°æ®è½¬æ¢ä¸º RM è®­ç»ƒæ ¼å¼ï¼ˆprompt, chosen, rejectedï¼‰\"\"\"\n",
    "    # å°è¯•å¤šç§å­—æ®µå\n",
    "    prompt = normalize_text(\n",
    "        example.get(\"prompt\") or \n",
    "        example.get(\"instruction\") or \n",
    "        example.get(\"input\") or\n",
    "        example.get(\"messages\")\n",
    "    )\n",
    "    \n",
    "    chosen = normalize_text(\n",
    "        example.get(\"chosen\") or \n",
    "        example.get(\"better_response\") or \n",
    "        example.get(\"positive\") or\n",
    "        example.get(\"response_j\")\n",
    "    )\n",
    "    \n",
    "    rejected = normalize_text(\n",
    "        example.get(\"rejected\") or \n",
    "        example.get(\"worse_response\") or \n",
    "        example.get(\"negative\") or\n",
    "        example.get(\"response_k\")\n",
    "    )\n",
    "    \n",
    "    # å¤„ç† messages æ ¼å¼ï¼ˆå¯¹è¯æ ¼å¼ï¼‰\n",
    "    if prompt is None and isinstance(example.get(\"messages\"), list):\n",
    "        msgs = example.get(\"messages\", [])\n",
    "        # æå–æœ€åä¸€ä¸ª user æ¶ˆæ¯ä½œä¸º promptï¼Œä¹‹å‰çš„ä½œä¸º context\n",
    "        user_msgs = [m.get(\"content\", \"\") for m in msgs if m.get(\"role\") == \"user\"]\n",
    "        if user_msgs:\n",
    "            prompt = user_msgs[-1]\n",
    "    \n",
    "    # å¤„ç† chosen/rejected å¦‚æœæ˜¯åˆ—è¡¨æ ¼å¼\n",
    "    if isinstance(chosen, list):\n",
    "        chosen = \"\\n---\\n\".join([normalize_text(c) for c in chosen])\n",
    "    if isinstance(rejected, list):\n",
    "        rejected = \"\\n---\\n\".join([normalize_text(r) for r in rejected])\n",
    "    \n",
    "    # ç¡®ä¿ prompt, chosen, rejected éƒ½ä¸ä¸ºç©º\n",
    "    if prompt is None or chosen is None or rejected is None:\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": str(prompt).strip(),\n",
    "        \"chosen\": str(chosen).strip(),\n",
    "        \"rejected\": str(rejected).strip(),\n",
    "    }\n",
    "\n",
    "# è½¬æ¢æ•°æ®æ ¼å¼\n",
    "print(\"\\n[RM] å¼€å§‹è½¬æ¢æ•°æ®æ ¼å¼...\")\n",
    "rm_ds = pref_ds.map(\n",
    "    _to_rm_format,\n",
    "    remove_columns=pref_ds.column_names,\n",
    "    desc=\"è½¬æ¢ä¸º RM æ ¼å¼\"\n",
    ")\n",
    "\n",
    "# è¿‡æ»¤æ‰ None å€¼ï¼ˆæ ¼å¼è½¬æ¢å¤±è´¥çš„æ•°æ®ï¼‰\n",
    "rm_ds = rm_ds.filter(lambda x: x[\"prompt\"] is not None and x[\"chosen\"] is not None and x[\"rejected\"] is not None)\n",
    "\n",
    "print(f\"[RM] æ ¼å¼è½¬æ¢å®Œæˆï¼Œæœ‰æ•ˆæ ·æœ¬æ•°ï¼š{len(rm_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73255c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RM] æ•°æ®åˆ‡åˆ†å®Œæˆï¼š\n",
      "  è®­ç»ƒé›†ï¼š59912 æ¡\n",
      "  éªŒè¯é›†ï¼š1223 æ¡\n",
      "\n",
      "[RM] è®­ç»ƒé›†ç¤ºä¾‹ï¼š\n",
      "  prompt: cara membuat animasi splash screen menggunakan flutter\n",
      "  chosen: user: cara membuat animasi splash screen menggunakan flutter\n",
      "---\n",
      "assistant: Untuk membuat animasi splash screen menggunakan Flutter, Anda harus mengik...\n",
      "  rejected: user: cara membuat animasi splash screen menggunakan flutter\n",
      "---\n",
      "assistant: Saya tidak punya privilajek yang lengkap untuk menyembarkan cara membuat a...\n"
     ]
    }
   ],
   "source": [
    "# æ•°æ®åˆ‡åˆ†ï¼š98% è®­ç»ƒï¼Œ2% éªŒè¯\n",
    "rm_split = rm_ds.train_test_split(test_size=0.02, seed=42)\n",
    "rm_train_ds = rm_split[\"train\"]\n",
    "rm_eval_ds = rm_split[\"test\"]\n",
    "\n",
    "print(f\"\\n[RM] æ•°æ®åˆ‡åˆ†å®Œæˆï¼š\")\n",
    "print(f\"  è®­ç»ƒé›†ï¼š{len(rm_train_ds)} æ¡\")\n",
    "print(f\"  éªŒè¯é›†ï¼š{len(rm_eval_ds)} æ¡\")\n",
    "\n",
    "# é¢„è§ˆè®­ç»ƒé›†æ ·æœ¬\n",
    "print(f\"\\n[RM] è®­ç»ƒé›†ç¤ºä¾‹ï¼š\")\n",
    "sample = rm_train_ds[0]\n",
    "for k, v in sample.items():\n",
    "    if isinstance(v, str) and len(v) > 150:\n",
    "        print(f\"  {k}: {v[:150]}...\")\n",
    "    else:\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58be0c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã€RM æ¨¡å‹åŠ è½½ä¸é…ç½®ã€‘\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from trl import RewardTrainer, RewardConfig\n",
    "import torch\n",
    "\n",
    "# æ¨¡å‹è¾“å…¥è¾“å‡ºé…ç½®\n",
    "rm_output_dir = \"outputs/rm_qlora\"\n",
    "max_seq_length = 2048\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_mps = torch.backends.mps.is_available()\n",
    "\n",
    "# é€‰æ‹©åŸºç¡€æ¨¡å‹ï¼šä¼˜å…ˆä½¿ç”¨ SFT æ¨¡å‹ï¼Œå¦åˆ™ä½¿ç”¨åŸºç¡€æ¨¡å‹\n",
    "# æ³¨æ„ï¼šRM è®­ç»ƒéœ€è¦å°†å› æœè¯­è¨€æ¨¡å‹è½¬æ¢ä¸ºåºåˆ—åˆ†ç±»æ¨¡å‹ï¼ˆæ·»åŠ å¥–åŠ±å¤´ï¼‰\n",
    "if 'adapter_dir' in globals() and os.path.exists(adapter_dir):\n",
    "    try:\n",
    "        # å°è¯•ä» SFT æ¨¡å‹åŠ è½½ï¼ˆéœ€è¦åˆå¹¶ LoRA æƒé‡ï¼‰\n",
    "        from peft import PeftModel\n",
    "        base_model_for_rm = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_or_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        sft_model = PeftModel.from_pretrained(base_model_for_rm, adapter_dir)\n",
    "        # åˆå¹¶ LoRA æƒé‡åˆ°åŸºç¡€æ¨¡å‹\n",
    "        merged_model = sft_model.merge_and_unload()\n",
    "        rm_base_model_dir = None  # æ ‡è®°ä½¿ç”¨åˆå¹¶åçš„æ¨¡å‹\n",
    "        print(\"[RM] ä½¿ç”¨ SFT æ¨¡å‹ä½œä¸ºåŸºç¡€ï¼ˆå·²åˆå¹¶ LoRA æƒé‡ï¼‰\")\n",
    "    except Exception as e:\n",
    "        print(f\"[RM] æ— æ³•åŠ è½½ SFT æ¨¡å‹ï¼š{e}ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹\")\n",
    "        rm_base_model_dir = base_model_or_dir\n",
    "else:\n",
    "    rm_base_model_dir = base_model_or_dir\n",
    "    print(\"[RM] ä½¿ç”¨åŸºç¡€æ¨¡å‹ï¼ˆæœªæ‰¾åˆ° SFT æ¨¡å‹ï¼‰\")\n",
    "\n",
    "# bitsandbytes QLoRA é…ç½®ï¼ˆå¯é€‰ï¼‰\n",
    "rm_quantization_config = None\n",
    "if use_cuda:\n",
    "    try:\n",
    "        rm_quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "        print(\"[RM] ä½¿ç”¨ bitsandbytes 4-bit é‡åŒ–åŠ è½½æ¨¡å‹\")\n",
    "    except Exception as e:\n",
    "        print(f\"[RM] æœªå¯ç”¨4bité‡åŒ–ï¼š{e}\")\n",
    "else:\n",
    "    print(\"[RM] å½“å‰ä¸ºé CUDA ç¯å¢ƒï¼Œä½¿ç”¨å¸¸è§„ç²¾åº¦åŠ è½½\")\n",
    "\n",
    "# Tokenizer åˆå§‹åŒ–ï¼ˆå¤ç”¨ä¹‹å‰çš„ tokenizerï¼‰\n",
    "if 'tokenizer' not in globals() or tokenizer is None:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        rm_base_model_dir if rm_base_model_dir else base_model_or_dir,\n",
    "        use_fast=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# åŠ è½½å¥–åŠ±æ¨¡å‹ï¼ˆå°†å› æœè¯­è¨€æ¨¡å‹è½¬æ¢ä¸ºåºåˆ—åˆ†ç±»æ¨¡å‹ï¼‰\n",
    "# AutoModelForSequenceClassification ä¼šè‡ªåŠ¨æ·»åŠ åˆ†ç±»å¤´ï¼ˆå¥–åŠ±å¤´ï¼‰\n",
    "if rm_base_model_dir:\n",
    "    rm_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        rm_base_model_dir,\n",
    "        trust_remote_code=True,\n",
    "        num_labels=1,  # å¥–åŠ±åˆ†æ•°æ˜¯æ ‡é‡\n",
    "        quantization_config=rm_quantization_config,\n",
    "        device_map=\"auto\" if use_cuda else None,\n",
    "    )\n",
    "else:\n",
    "    # å¦‚æœä½¿ç”¨åˆå¹¶åçš„ SFT æ¨¡å‹ï¼Œéœ€è¦ä»åˆå¹¶æ¨¡å‹åˆ›å»º RM æ¨¡å‹\n",
    "    # ç®€åŒ–å¤„ç†ï¼šç›´æ¥ä½¿ç”¨åŸºç¡€æ¨¡å‹ï¼ˆå®é™…ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„å¤„ç†ï¼‰\n",
    "    rm_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        base_model_or_dir,\n",
    "        trust_remote_code=True,\n",
    "        num_labels=1,\n",
    "        quantization_config=rm_quantization_config,\n",
    "        device_map=\"auto\" if use_cuda else None,\n",
    "    )\n",
    "\n",
    "print(\"[RM] å¥–åŠ±æ¨¡å‹åŠ è½½å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb3109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã€RM æ•°æ®æ ¼å¼åŒ–ï¼šæ„é€  prompt-response å¯¹ã€‘\n",
    "def format_rm_prompt(prompt, response):\n",
    "    \"\"\"å°† prompt å’Œ response æ ¼å¼åŒ–ä¸ºæ¨¡å‹è¾“å…¥æ–‡æœ¬ï¼ˆç¬¦åˆ Qwen æ¨¡æ¿ï¼‰\"\"\"\n",
    "    # æ„é€ ç¬¦åˆ Qwen æ¨¡æ¿çš„æ–‡æœ¬æ ¼å¼\n",
    "    text = f\"<|user|>\\n{prompt}\\n<|assistant|>\\n{response}\"\n",
    "    return text\n",
    "\n",
    "def tokenize_rm_dataset(examples):\n",
    "    \"\"\"å¯¹ RM æ•°æ®é›†è¿›è¡Œåˆ†è¯ï¼ˆå¤„ç† prompt+chosen å’Œ prompt+rejected å¯¹ï¼‰\"\"\"\n",
    "    # æ„é€  chosen å’Œ rejected çš„å®Œæ•´æ–‡æœ¬\n",
    "    chosen_texts = [format_rm_prompt(p, c) for p, c in zip(examples[\"prompt\"], examples[\"chosen\"])]\n",
    "    rejected_texts = [format_rm_prompt(p, r) for p, r in zip(examples[\"prompt\"], examples[\"rejected\"])]\n",
    "    \n",
    "    # åˆ†è¯ï¼ˆchosenï¼‰- ä¸ä½¿ç”¨ return_tensorsï¼Œå› ä¸º Dataset.map æœŸæœ›è¿”å›æ™®é€šåˆ—è¡¨\n",
    "    chosen_tokenized = tokenizer(\n",
    "        chosen_texts,\n",
    "        max_length=max_seq_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    \n",
    "    # åˆ†è¯ï¼ˆrejectedï¼‰- ä¸ä½¿ç”¨ return_tensors\n",
    "    rejected_tokenized = tokenizer(\n",
    "        rejected_texts,\n",
    "        max_length=max_seq_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    \n",
    "    # è¿”å›æ ¼å¼åŒ–çš„æ•°æ®ï¼ˆTRL RewardTrainer æœŸæœ›çš„æ ¼å¼ï¼‰\n",
    "    return {\n",
    "        \"input_ids_chosen\": chosen_tokenized[\"input_ids\"],\n",
    "        \"attention_mask_chosen\": chosen_tokenized[\"attention_mask\"],\n",
    "        \"input_ids_rejected\": rejected_tokenized[\"input_ids\"],\n",
    "        \"attention_mask_rejected\": rejected_tokenized[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "# å¯¹è®­ç»ƒé›†å’ŒéªŒè¯é›†è¿›è¡Œåˆ†è¯\n",
    "print(\"[RM] å¼€å§‹å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯...\")\n",
    "rm_train_tokenized = rm_train_ds.map(\n",
    "    tokenize_rm_dataset,\n",
    "    batched=True,\n",
    "    desc=\"åˆ†è¯è®­ç»ƒé›†\"\n",
    ")\n",
    "rm_eval_tokenized = rm_eval_ds.map(\n",
    "    tokenize_rm_dataset,\n",
    "    batched=True,\n",
    "    desc=\"åˆ†è¯éªŒè¯é›†\"\n",
    ")\n",
    "\n",
    "print(f\"[RM] åˆ†è¯å®Œæˆï¼š\")\n",
    "print(f\"  è®­ç»ƒé›†ï¼š{len(rm_train_tokenized)} æ¡\")\n",
    "print(f\"  éªŒè¯é›†ï¼š{len(rm_eval_tokenized)} æ¡\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97636ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã€RM LoRA é…ç½®ä¸è®­ç»ƒå‚æ•°ã€‘\n",
    "# LoRA é…ç½®ï¼ˆå¦‚æœä½¿ç”¨ LoRA å¾®è°ƒï¼‰\n",
    "rm_lora_config = LoraConfig(\n",
    "    r=16,                    # ç§©ï¼ˆrankï¼‰\n",
    "    lora_alpha=32,          # ç¼©æ”¾ç³»æ•°ï¼ˆé€šå¸¸ alpha=2*rï¼‰\n",
    "    lora_dropout=0.05,      # é€‚é…å™¨å±‚ dropout\n",
    "    bias=\"none\",            # ä¸è®­ç»ƒ bias\n",
    "    task_type=\"SEQ_CLS\",    # ä»»åŠ¡ç±»å‹ï¼šåºåˆ—åˆ†ç±»ï¼ˆå¥–åŠ±æ¨¡å‹ï¼‰\n",
    ")\n",
    "\n",
    "# è®­ç»ƒå‚æ•°ï¼šæ§åˆ¶è®­ç»ƒæµç¨‹ä¸ä¼˜åŒ–\n",
    "rm_args = RewardConfig(\n",
    "    # è¾“å‡ºä¸ä¿å­˜\n",
    "    output_dir=rm_output_dir,           # æ¨¡å‹/æ—¥å¿—è¾“å‡ºç›®å½•\n",
    "    save_steps=500,                     # æ¯ N æ­¥ä¿å­˜ä¸€æ¬¡ checkpoint\n",
    "    save_total_limit=2,                 # æœ€å¤šä¿ç•™ N ä¸ª checkpoint\n",
    "    save_safetensors=True,              # ä½¿ç”¨ safetensors æ ¼å¼ä¿å­˜\n",
    "    \n",
    "    # æ‰¹å¤§å°ä¸æ¢¯åº¦\n",
    "    per_device_train_batch_size=1,      # æ¯è®¾å¤‡è®­ç»ƒ batch å¤§å°\n",
    "    per_device_eval_batch_size=1,       # æ¯è®¾å¤‡éªŒè¯ batch å¤§å°\n",
    "    gradient_accumulation_steps=8,      # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆç­‰æ•ˆ batch = 1 * 8 = 8ï¼‰\n",
    "    \n",
    "    # è®­ç»ƒè½®æ¬¡ä¸å­¦ä¹ ç‡\n",
    "    num_train_epochs=1,                 # è®­ç»ƒè½®æ¬¡æ•°\n",
    "    learning_rate=1e-5,                 # åˆå§‹å­¦ä¹ ç‡ï¼ˆRM è®­ç»ƒå¸¸ç”¨ 1e-5 åˆ° 5e-5ï¼‰\n",
    "    lr_scheduler_type=\"cosine\",         # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆä½™å¼¦é€€ç«ï¼‰\n",
    "    warmup_ratio=0.03,                  # warmup æ¯”ä¾‹ï¼ˆå‰ 3% æ­¥æ•°çº¿æ€§å¢é•¿ LRï¼‰\n",
    "    \n",
    "    # è¯„ä¼°ä¸æ—¥å¿—\n",
    "    eval_strategy=\"steps\",              # è¯„ä¼°ç­–ç•¥ï¼ˆ\"steps\"/\"epoch\"/\"no\"ï¼‰\n",
    "    eval_steps=250,                     # æ¯ N æ­¥è¯„ä¼°ä¸€æ¬¡\n",
    "    logging_steps=50,                   # æ¯ N æ­¥æ‰“å°ä¸€æ¬¡æ—¥å¿—\n",
    "    report_to=[\"none\"],                 # ä¸å‘å¤–éƒ¨ä¸ŠæŠ¥ï¼ˆå¯æ”¹ä¸º [\"wandb\"] ç­‰ï¼‰\n",
    "    \n",
    "    # æ¨¡å‹æ£€æŸ¥ç‚¹\n",
    "    load_best_model_at_end=True,        # è®­ç»ƒç»“æŸåŠ è½½æœ€ä½³æ¨¡å‹\n",
    "    metric_for_best_model=\"eval_loss\",  # æœ€ä½³æ¨¡å‹æŒ‡æ ‡\n",
    "    greater_is_better=False,            # è¯¥æŒ‡æ ‡è¶Šå°è¶Šå¥½\n",
    "    \n",
    "    # æ€§èƒ½ä¼˜åŒ–\n",
    "    bf16=use_cuda,                      # CUDA ä¸Šç”¨ bfloat16\n",
    "    fp16=False,                         # ç¦ç”¨ FP16\n",
    "    gradient_checkpointing=True,        # æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆç‰ºç‰²æ—¶é—´æ¢æ˜¾å­˜ï¼‰\n",
    "    \n",
    "    # RM ç‰¹å®šå‚æ•°\n",
    "    max_length=max_seq_length,          # æœ€å¤§åºåˆ—é•¿åº¦\n",
    ")\n",
    "\n",
    "print(\"[RM] è®­ç»ƒå‚æ•°é…ç½®å®Œæˆ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447d7694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã€æ„é€  RM è®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒã€‘\n",
    "# å¦‚æœä½¿ç”¨ LoRAï¼Œåº”ç”¨ LoRA é…ç½®åˆ°æ¨¡å‹\n",
    "if rm_quantization_config is not None or use_cuda:\n",
    "    # å¦‚æœå·²ç»é‡åŒ–æˆ–ä½¿ç”¨ CUDAï¼Œåº”ç”¨ LoRA\n",
    "    try:\n",
    "        rm_model = get_peft_model(rm_model, rm_lora_config)\n",
    "        print(\"[RM] LoRA é…ç½®å·²åº”ç”¨åˆ°æ¨¡å‹\")\n",
    "    except Exception as e:\n",
    "        print(f\"[RM] åº”ç”¨ LoRA æ—¶å‡ºé”™ï¼š{e}ï¼Œç»§ç»­ä½¿ç”¨å…¨é‡å¾®è°ƒ\")\n",
    "\n",
    "# æ„é€  RewardTrainer\n",
    "rm_trainer = RewardTrainer(\n",
    "    model=rm_model,\n",
    "    args=rm_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=rm_train_tokenized,\n",
    "    eval_dataset=rm_eval_tokenized,\n",
    "    peft_config=rm_lora_config if rm_quantization_config is None and not use_cuda else None,\n",
    ")\n",
    "\n",
    "print(\"[RM] è®­ç»ƒå™¨æ„é€ å®Œæˆï¼Œå‡†å¤‡å¼€å§‹è®­ç»ƒ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20f807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¼€å§‹è®­ç»ƒ\n",
    "rm_trainer.train()\n",
    "\n",
    "# ä¿å­˜æ¨¡å‹\n",
    "rm_model_dir = os.path.join(rm_output_dir, \"reward_model\")\n",
    "rm_trainer.model.save_pretrained(rm_model_dir)\n",
    "tokenizer.save_pretrained(rm_model_dir)\n",
    "print(f\"[Done] RM + QLoRA è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜è‡³: {rm_model_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8fe69d",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ PPO å¼ºåŒ–ä¼˜åŒ–\n",
    "- **è¾“å…¥**ï¼šSFT æ¨¡å‹ä½œä¸ºåˆå§‹ç­–ç•¥ \\(\\pi_\\theta\\)ï¼Œå¥–åŠ±æ¨¡å‹ r ä½œä¸ºå¥–åŠ±ä¿¡å·\n",
    "- **ç›®æ ‡**ï¼šåœ¨ KL çº¦æŸä¸‹æœ€å¤§åŒ–æœŸæœ›å¥–åŠ±ï¼Œæå‡å¯¹é½åº¦ä¸æœ‰ç”¨æ€§\n",
    "- **è®­ç»ƒ**ï¼šPPOï¼ˆå‰ªåˆ‡ç­–ç•¥æ¢¯åº¦ï¼‰ï¼Œå¼•å…¥ KL æƒ©ç½šä»¥ä¿æŒä¸å‚è€ƒç­–ç•¥æ¥è¿‘\n",
    "- **è¾“å‡º**ï¼šPPO åçš„å¯¹é½æ¨¡å‹ï¼ˆæ›´ç¬¦åˆäººç±»åå¥½ï¼‰\n",
    "- **å®è·µè¦ç‚¹**ï¼šé«˜è´¨é‡åå¥½æ•°æ®ä¸ç¨³å®šçš„ KL æ§åˆ¶æ˜¯æˆåŠŸå…³é”®ï¼›ç›‘æ§é•¿åº¦åç½®ã€æ¨¡å¼åç¼©ä¸è¿‡æ‹Ÿåˆã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a52435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã€PPO è®­ç»ƒå‡†å¤‡ï¼šæ¨¡å‹åŠ è½½ã€‘\n",
    "from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "import torch\n",
    "\n",
    "# PPO è¾“å‡ºé…ç½®\n",
    "ppo_output_dir = \"outputs/ppo_model\"\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_mps = torch.backends.mps.is_available()\n",
    "\n",
    "print(\"[PPO] å¼€å§‹åŠ è½½æ¨¡å‹...\")\n",
    "\n",
    "# ========== 1. åŠ è½½ç­–ç•¥æ¨¡å‹ï¼ˆSFT æ¨¡å‹ï¼‰ä½œä¸ºåˆå§‹ç­–ç•¥ Ï€_Î¸ ==========\n",
    "# PPO éœ€è¦ç­–ç•¥æ¨¡å‹ï¼ˆè®­ç»ƒä¸­çš„æ¨¡å‹ï¼‰å’Œå‚è€ƒæ¨¡å‹ï¼ˆç”¨äº KL çº¦æŸï¼‰\n",
    "if 'adapter_dir' in globals() and os.path.exists(adapter_dir):\n",
    "    try:\n",
    "        # ä» SFT æ¨¡å‹åŠ è½½ï¼ˆå¸¦ LoRA æƒé‡ï¼‰\n",
    "        from peft import PeftModel\n",
    "        base_policy = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_or_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        policy_model = PeftModel.from_pretrained(base_policy, adapter_dir)\n",
    "        print(\"[PPO] ç­–ç•¥æ¨¡å‹ï¼šä½¿ç”¨ SFT æ¨¡å‹ï¼ˆå¸¦ LoRA æƒé‡ï¼‰\")\n",
    "        \n",
    "        # å‚è€ƒæ¨¡å‹ï¼šä½¿ç”¨åˆå¹¶åçš„åŸºç¡€æ¨¡å‹ï¼ˆç”¨äº KL çº¦æŸï¼‰\n",
    "        reference_model = PeftModel.from_pretrained(base_policy, adapter_dir)\n",
    "        # åˆ›å»ºå‚è€ƒæ¨¡å‹çš„å‰¯æœ¬ï¼ˆå†»ç»“å‚æ•°ï¼Œä¸æ›´æ–°ï¼‰\n",
    "        reference_model.merge_and_unload()\n",
    "        print(\"[PPO] å‚è€ƒæ¨¡å‹ï¼šä½¿ç”¨ SFT æ¨¡å‹ï¼ˆå·²åˆå¹¶ï¼Œç”¨äº KL çº¦æŸï¼‰\")\n",
    "    except Exception as e:\n",
    "        print(f\"[PPO] æ— æ³•åŠ è½½ SFT æ¨¡å‹ï¼š{e}ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹\")\n",
    "        policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_or_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        reference_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_or_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "else:\n",
    "    # å¦‚æœæœªæ‰¾åˆ° SFT æ¨¡å‹ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹\n",
    "    policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_or_dir,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    reference_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_or_dir,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    print(\"[PPO] ç­–ç•¥æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹ï¼šä½¿ç”¨åŸºç¡€æ¨¡å‹\")\n",
    "\n",
    "# å°†ç­–ç•¥æ¨¡å‹åŒ…è£…ä¸ºå¸¦å€¼å‡½æ•°çš„æ¨¡å‹ï¼ˆPPO éœ€è¦ï¼‰\n",
    "# AutoModelForCausalLMWithValueHead ä¼šåœ¨ç­–ç•¥æ¨¡å‹åŸºç¡€ä¸Šæ·»åŠ å€¼å‡½æ•°å¤´ï¼ˆç”¨äºä»·å€¼ä¼°è®¡ï¼‰\n",
    "# æ³¨æ„ï¼šå¦‚æœ policy_model æ˜¯ PeftModelï¼Œéœ€è¦å…ˆè·å–åŸºç¡€æ¨¡å‹è·¯å¾„\n",
    "try:\n",
    "    if hasattr(policy_model, 'merge_and_unload'):\n",
    "        # å¯¹äº PeftModelï¼Œåˆå¹¶ LoRA æƒé‡ååˆ›å»º ValueHead æ¨¡å‹\n",
    "        # æ³¨æ„ï¼šåˆå¹¶åçš„æ¨¡å‹ä¼šåœ¨å†…å­˜ä¸­ï¼Œéœ€è¦ä¿å­˜åˆ°ä¸´æ—¶è·¯å¾„æˆ–ç›´æ¥ä½¿ç”¨åŸºç¡€æ¨¡å‹\n",
    "        print(\"[PPO] æ£€æµ‹åˆ° PeftModelï¼Œä»åŸºç¡€æ¨¡å‹åˆ›å»º ValueHead æ¨¡å‹ï¼ˆå°†ä½¿ç”¨ SFT æƒé‡ï¼‰\")\n",
    "        # ç®€åŒ–å¤„ç†ï¼šç›´æ¥ä»åŸºç¡€æ¨¡å‹åˆ›å»ºï¼ŒSFT æƒé‡å°†åœ¨åç»­åŠ è½½\n",
    "        ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "            base_model_or_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "    else:\n",
    "        # å¦‚æœ policy_model æ˜¯æ™®é€šæ¨¡å‹ï¼Œç›´æ¥ä»åŸºç¡€æ¨¡å‹åˆ›å»º\n",
    "        ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "            base_model_or_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "    print(\"[PPO] ç­–ç•¥æ¨¡å‹ï¼ˆå¸¦å€¼å‡½æ•°å¤´ï¼‰åŠ è½½å®Œæˆ\")\n",
    "except Exception as e:\n",
    "    print(f\"[PPO] åˆ›å»º ValueHead æ¨¡å‹å¤±è´¥ï¼š{e}\")\n",
    "    print(\"[PPO] å°è¯•ç›´æ¥ä»åŸºç¡€æ¨¡å‹åˆ›å»º...\")\n",
    "    ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "        base_model_or_dir,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "# ========== 2. åŠ è½½å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ä½œä¸ºå¥–åŠ±ä¿¡å· ==========\n",
    "if 'rm_model_dir' in globals() and os.path.exists(rm_model_dir):\n",
    "    try:\n",
    "        reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            rm_model_dir,\n",
    "            trust_remote_code=True,\n",
    "            num_labels=1,  # å¥–åŠ±åˆ†æ•°æ˜¯æ ‡é‡\n",
    "        )\n",
    "        print(f\"[PPO] å¥–åŠ±æ¨¡å‹ï¼šä» {rm_model_dir} åŠ è½½æˆåŠŸ\")\n",
    "    except Exception as e:\n",
    "        print(f\"[PPO] æ— æ³•åŠ è½½å¥–åŠ±æ¨¡å‹ï¼š{e}\")\n",
    "        reward_model = None\n",
    "else:\n",
    "    print(\"[PPO] æœªæ‰¾åˆ°å¥–åŠ±æ¨¡å‹ï¼Œéœ€è¦åœ¨è®­ç»ƒå‰å…ˆå®Œæˆ RM è®­ç»ƒ\")\n",
    "    reward_model = None\n",
    "\n",
    "# Tokenizer å¤ç”¨ï¼ˆå¦‚æœå·²å®šä¹‰ï¼‰\n",
    "if 'tokenizer' not in globals() or tokenizer is None:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        base_model_or_dir,\n",
    "        use_fast=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"[PPO] æ¨¡å‹åŠ è½½å®Œæˆ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5960d724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã€PPO æ•°æ®å‡†å¤‡ï¼šæç¤ºæ•°æ®é›†ã€‘\n",
    "# PPO è®­ç»ƒéœ€è¦æç¤ºï¼ˆpromptsï¼‰æ•°æ®é›†ï¼Œæ¨¡å‹ä¼šæ ¹æ®è¿™äº›æç¤ºç”Ÿæˆå›ç­”\n",
    "# ç„¶åå¥–åŠ±æ¨¡å‹å¯¹ç”Ÿæˆçš„å›ç­”æ‰“åˆ†ï¼ŒPPO æ ¹æ®å¥–åŠ±ä¿¡å·ä¼˜åŒ–ç­–ç•¥\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# æ–¹å¼ 1ï¼šä» SFT æ•°æ®é›†ä¸­æå– promptsï¼ˆæ¨èï¼‰\n",
    "# ä½¿ç”¨ SFT è®­ç»ƒé›†ä¸­çš„ prompts ä½œä¸º PPO çš„è¾“å…¥\n",
    "if 'train_ds' in globals() and len(train_ds) > 0:\n",
    "    # ä» SFT è®­ç»ƒé›†ä¸­æå– prompts\n",
    "    ppo_prompts = [ex[\"prompt\"] for ex in train_ds.select(range(min(1000, len(train_ds))))]\n",
    "    print(f\"[PPO] ä» SFT è®­ç»ƒé›†æå– {len(ppo_prompts)} ä¸ª prompts\")\n",
    "else:\n",
    "    # æ–¹å¼ 2ï¼šåˆ›å»ºç®€å•çš„æç¤ºç¤ºä¾‹\n",
    "    ppo_prompts = [\n",
    "        \"è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ã€‚\",\n",
    "        \"å¦‚ä½•æé«˜ç¼–ç¨‹æŠ€èƒ½ï¼Ÿ\",\n",
    "        \"ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ\",\n",
    "        \"å¦‚ä½•å­¦ä¹  Pythonï¼Ÿ\",\n",
    "        \"ä»‹ç»ä¸€ä¸‹ç¥ç»ç½‘ç»œã€‚\",\n",
    "    ]\n",
    "    print(f\"[PPO] ä½¿ç”¨ç¤ºä¾‹ promptsï¼ˆå…± {len(ppo_prompts)} ä¸ªï¼‰\")\n",
    "\n",
    "# æ„é€  PPO æç¤ºæ•°æ®é›†\n",
    "ppo_dataset = Dataset.from_dict({\"query\": ppo_prompts})\n",
    "\n",
    "print(f\"[PPO] æç¤ºæ•°æ®é›†å‡†å¤‡å®Œæˆï¼š{len(ppo_dataset)} æ¡ prompts\")\n",
    "print(f\"[PPO] ç¤ºä¾‹ prompt: {ppo_dataset[0]['query']}\")\n",
    "\n",
    "# PPO è®­ç»ƒæµç¨‹è¯´æ˜ï¼š\n",
    "# 1. ç­–ç•¥æ¨¡å‹æ ¹æ® prompts ç”Ÿæˆå›ç­”ï¼ˆå¤šä¸ªå€™é€‰å›ç­”ï¼‰\n",
    "# 2. å¥–åŠ±æ¨¡å‹å¯¹æ¯ä¸ªç”Ÿæˆçš„å›ç­”æ‰“åˆ†\n",
    "# 3. PPO ç®—æ³•æ ¹æ®å¥–åŠ±ä¿¡å·ä¼˜åŒ–ç­–ç•¥ï¼ŒåŒæ—¶ä¿æŒä¸å‚è€ƒæ¨¡å‹çš„ KL æ•£åº¦çº¦æŸ\n",
    "# 4. é‡å¤ä¸Šè¿°è¿‡ç¨‹ï¼Œç›´åˆ°ç­–ç•¥æ”¶æ•›\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9851c08b",
   "metadata": {},
   "source": [
    "## ğŸ” RLHF ä¸‰é˜¶æ®µçš„æƒé‡å­˜å‚¨è¯¦è§£\n",
    "\n",
    "### 1ï¸âƒ£ SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰é˜¶æ®µ - LoRA æƒé‡å­˜å‚¨\n",
    "\n",
    "**å­˜å‚¨æ–¹å¼**ï¼šä»…ä¿å­˜ LoRA adapter æƒé‡ï¼ˆè½»é‡çº§ï¼‰\n",
    "\n",
    "```\n",
    "SFT è®­ç»ƒåçš„æ¨¡å‹ç»“æ„ï¼š\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   åŸºç¡€æ¨¡å‹ï¼ˆQwen2.5-1.5Bï¼‰              â”‚  â† ä¸ä¿å­˜ï¼Œä¿æŒä¸å˜\n",
    "â”‚   â”œâ”€â”€ Embeddings                     â”‚\n",
    "â”‚   â”œâ”€â”€ Transformer Layers (x24)       â”‚\n",
    "â”‚   â””â”€â”€ LM Head                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â–²\n",
    "           â”‚ LoRA é€‚é…å™¨ï¼ˆå°æƒé‡çŸ©é˜µï¼‰\n",
    "           â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   LoRA Adapters                     â”‚  â† ä¿å­˜è¿™äº›\n",
    "â”‚   â”œâ”€â”€ LoRA_A (rank=16)              â”‚    å¤§å°ï¼š~å‡ MB\n",
    "â”‚   â””â”€â”€ LoRA_B (rank=16)              â”‚    å‚æ•°é‡ï¼š~0.1% of åŸºç¡€æ¨¡å‹\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "ä¿å­˜è·¯å¾„ï¼šoutputs/sft_qlora/adapter/\n",
    "â”œâ”€â”€ adapter_config.json              â† LoRA é…ç½®\n",
    "â”œâ”€â”€ adapter_model.safetensors        â† LoRA æƒé‡ï¼ˆè½»é‡çº§ï¼‰\n",
    "â””â”€â”€ tokenizer.json                   â† Tokenizer\n",
    "```\n",
    "\n",
    "**ç‰¹ç‚¹**ï¼š\n",
    "- âœ… ä»…ä¿å­˜å°‘é‡å‚æ•°ï¼ˆLoRA æƒé‡ï¼Œé€šå¸¸åªæœ‰åŸæ¨¡å‹çš„ 0.1-1%ï¼‰\n",
    "- âœ… æ–‡ä»¶å°ï¼Œä¾¿äºå­˜å‚¨å’Œå…±äº«\n",
    "- âœ… éœ€è¦åŸºç¡€æ¨¡å‹æ‰èƒ½ä½¿ç”¨ï¼ˆæ¨ç†æ—¶éœ€è¦åˆå¹¶ï¼‰\n",
    "\n",
    "**ä»£ç ç¤ºä¾‹**ï¼š\n",
    "```python\n",
    "# SFT è®­ç»ƒåä¿å­˜\n",
    "adapter_dir = os.path.join(output_dir, \"adapter\")\n",
    "trainer.model.save_pretrained(adapter_dir)  # åªä¿å­˜ LoRA æƒé‡\n",
    "\n",
    "# ä½¿ç”¨æ—¶çš„åŠ è½½\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_or_dir)\n",
    "model = PeftModel.from_pretrained(base_model, adapter_dir)  # åŠ è½½ LoRA\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ RMï¼ˆå¥–åŠ±æ¨¡å‹ï¼‰é˜¶æ®µ - å¥–åŠ±å¤´å­˜å‚¨\n",
    "\n",
    "**å­˜å‚¨æ–¹å¼**ï¼šä¿å­˜å®Œæ•´çš„å¥–åŠ±æ¨¡å‹ï¼ˆåŸºç¡€æ¨¡å‹ + å¥–åŠ±å¤´ï¼‰\n",
    "\n",
    "```\n",
    "RM è®­ç»ƒåçš„æ¨¡å‹ç»“æ„ï¼š\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   åŸºç¡€æ¨¡å‹ï¼ˆå¯èƒ½æ˜¯ SFT åˆå¹¶åçš„ï¼‰       â”‚  â† åŒ…å«åœ¨å†…\n",
    "â”‚   â”œâ”€â”€ Embeddings                     â”‚\n",
    "â”‚   â”œâ”€â”€ Transformer Layers             â”‚\n",
    "â”‚   â””â”€â”€ Hidden States                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â–²\n",
    "           â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   å¥–åŠ±å¤´ï¼ˆReward Headï¼‰                â”‚  â† è¿™æ˜¯æ–°æ·»åŠ çš„\n",
    "â”‚   â”œâ”€â”€ Linear Layer (hidden_size)     â”‚    å…³é”®ç»„ä»¶ï¼\n",
    "â”‚   â””â”€â”€ Output: 1 (å¥–åŠ±åˆ†æ•°æ ‡é‡)        â”‚    å¤§å°ï¼š~å‡ MB\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "ä¿å­˜è·¯å¾„ï¼šoutputs/rm_qlora/reward_model/\n",
    "â”œâ”€â”€ config.json                       â† æ¨¡å‹é…ç½®ï¼ˆåŒ…å« num_labels=1ï¼‰\n",
    "â”œâ”€â”€ model.safetensors                 â† å®Œæ•´æ¨¡å‹æƒé‡ï¼ˆåŸºç¡€æ¨¡å‹ + å¥–åŠ±å¤´ï¼‰\n",
    "â”‚                                       æˆ– adapter_model.safetensorsï¼ˆå¦‚æœç”¨äº† LoRAï¼‰\n",
    "â””â”€â”€ tokenizer.json                    â† Tokenizer\n",
    "```\n",
    "\n",
    "**ç‰¹ç‚¹**ï¼š\n",
    "- âœ… å¥–åŠ±å¤´æ˜¯æ–°å¢çš„ç»„ä»¶ï¼ˆLinear å±‚ï¼Œè¾“å‡ºç»´åº¦ä¸º 1ï¼‰\n",
    "- âœ… å¦‚æœä½¿ç”¨ LoRAï¼Œå¯èƒ½åªä¿å­˜ LoRA æƒé‡ï¼›å¦‚æœå…¨é‡å¾®è°ƒï¼Œä¿å­˜å®Œæ•´æ¨¡å‹\n",
    "- âœ… å¥–åŠ±å¤´æ˜¯å…³é”®ï¼šå®ƒå­¦ä¼šäº†å¯¹å›ç­”æ‰“åˆ†ï¼ˆchosen > rejectedï¼‰\n",
    "\n",
    "**ä»£ç ç¤ºä¾‹**ï¼š\n",
    "```python\n",
    "# RM è®­ç»ƒåä¿å­˜\n",
    "rm_model_dir = os.path.join(rm_output_dir, \"reward_model\")\n",
    "rm_trainer.model.save_pretrained(rm_model_dir)  # ä¿å­˜å¥–åŠ±æ¨¡å‹ï¼ˆåŒ…å«å¥–åŠ±å¤´ï¼‰\n",
    "\n",
    "# ä½¿ç”¨æ—¶çš„åŠ è½½\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    rm_model_dir,\n",
    "    num_labels=1,  # å¥–åŠ±å¤´è¾“å‡ºç»´åº¦ä¸º 1\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ PPOï¼ˆå¼ºåŒ–ä¼˜åŒ–ï¼‰é˜¶æ®µ - ç­–ç•¥æ¨¡å‹ + å€¼å‡½æ•°å¤´å­˜å‚¨\n",
    "\n",
    "**å­˜å‚¨æ–¹å¼**ï¼šä¿å­˜ç­–ç•¥æ¨¡å‹æƒé‡ + å€¼å‡½æ•°å¤´æƒé‡\n",
    "\n",
    "```\n",
    "PPO è®­ç»ƒåçš„æ¨¡å‹ç»“æ„ï¼š\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   ç­–ç•¥æ¨¡å‹ï¼ˆPolicy Modelï¼‰             â”‚  â† å¯ä»¥æ˜¯åŸºç¡€æ¨¡å‹æˆ– SFT+LoRA\n",
    "â”‚   â”œâ”€â”€ Embeddings                     â”‚\n",
    "â”‚   â”œâ”€â”€ Transformer Layers             â”‚\n",
    "â”‚   â””â”€â”€ LM Head                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â–²\n",
    "           â”‚ ä¸¤ä¸ªå¤´ï¼šç­–ç•¥å¤´ + å€¼å‡½æ•°å¤´\n",
    "           â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   ç­–ç•¥å¤´ï¼ˆPolicy Headï¼‰               â”‚  â† åŸæœ‰çš„ï¼ˆç”¨äºç”Ÿæˆï¼‰\n",
    "â”‚   â””â”€â”€ LM Head (vocab_size)          â”‚\n",
    "â”‚                                      â”‚\n",
    "â”‚   å€¼å‡½æ•°å¤´ï¼ˆValue Headï¼‰               â”‚  â† æ–°æ·»åŠ çš„ï¼ˆç”¨äº PPOï¼‰\n",
    "â”‚   â”œâ”€â”€ Linear Layer 1 (hidden_size)  â”‚    å…³é”®ç»„ä»¶ï¼\n",
    "â”‚   â””â”€â”€ Output: 1 (çŠ¶æ€ä»·å€¼æ ‡é‡)       â”‚    å¤§å°ï¼š~å‡ MB\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "ä¿å­˜è·¯å¾„ï¼šoutputs/ppo_model/\n",
    "â”œâ”€â”€ config.json                       â† æ¨¡å‹é…ç½®\n",
    "â”œâ”€â”€ pytorch_model.bin                  â† ç­–ç•¥æ¨¡å‹æƒé‡\n",
    "â”‚   æˆ– adapter_model.safetensors      â† å¦‚æœç”¨äº† LoRAï¼ˆä»… LoRA æƒé‡ï¼‰\n",
    "â”œâ”€â”€ value_head/                        â† å€¼å‡½æ•°å¤´ï¼ˆå•ç‹¬ä¿å­˜ï¼‰\n",
    "â”‚   â”œâ”€â”€ config.json\n",
    "â”‚   â””â”€â”€ pytorch_model.bin\n",
    "â””â”€â”€ tokenizer.json                     â† Tokenizer\n",
    "```\n",
    "\n",
    "**ç‰¹ç‚¹**ï¼š\n",
    "- âœ… **å€¼å‡½æ•°å¤´ï¼ˆValue Headï¼‰**æ˜¯ PPO ç‰¹æœ‰çš„ç»„ä»¶\n",
    "  - ç”¨äºä¼°è®¡çŠ¶æ€ä»·å€¼ï¼ˆstate valueï¼‰ï¼Œè®¡ç®—ä¼˜åŠ¿å‡½æ•°ï¼ˆadvantageï¼‰\n",
    "  - è¾“å‡ºç»´åº¦ä¸º 1ï¼ˆæ ‡é‡ä»·å€¼ï¼‰\n",
    "- âœ… ç­–ç•¥æ¨¡å‹å¯ä»¥æ˜¯åŸºç¡€æ¨¡å‹æˆ–å¸¦ LoRA çš„æ¨¡å‹\n",
    "- âœ… å¦‚æœä½¿ç”¨ LoRAï¼Œå¯èƒ½åªä¿å­˜ LoRA æƒé‡ï¼›å€¼å‡½æ•°å¤´é€šå¸¸å…¨é‡ä¿å­˜\n",
    "\n",
    "**ä»£ç ç¤ºä¾‹**ï¼š\n",
    "```python\n",
    "# PPO æ¨¡å‹åˆ›å»º\n",
    "ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    base_model_or_dir,  # è‡ªåŠ¨æ·»åŠ å€¼å‡½æ•°å¤´\n",
    ")\n",
    "\n",
    "# PPO è®­ç»ƒåä¿å­˜\n",
    "ppo_trainer.save_model(ppo_output_dir)  # ä¿å­˜ç­–ç•¥æ¨¡å‹ + å€¼å‡½æ•°å¤´\n",
    "\n",
    "# ä½¿ç”¨æ—¶çš„åŠ è½½\n",
    "ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(ppo_output_dir)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š ä¸‰é˜¶æ®µæƒé‡å­˜å‚¨å¯¹æ¯”\n",
    "\n",
    "| é˜¶æ®µ | å­˜å‚¨å†…å®¹ | å­˜å‚¨å¤§å° | å…³é”®ç»„ä»¶ | èƒ½å¦ç‹¬ç«‹ä½¿ç”¨ |\n",
    "|------|---------|---------|---------|------------|\n",
    "| **SFT** | LoRA Adapter æƒé‡ | ~å‡ MBï¼ˆ0.1-1% å‚æ•°é‡ï¼‰ | LoRA_A, LoRA_B | âŒ éœ€è¦åŸºç¡€æ¨¡å‹ |\n",
    "| **RM** | åŸºç¡€æ¨¡å‹ + å¥–åŠ±å¤´ | ~å®Œæ•´æ¨¡å‹ï¼ˆæˆ– LoRAï¼‰ | å¥–åŠ±å¤´ï¼ˆReward Headï¼‰ | âœ… å¯ä»¥ç‹¬ç«‹ä½¿ç”¨ |\n",
    "| **PPO** | ç­–ç•¥æ¨¡å‹ + å€¼å‡½æ•°å¤´ | ~å®Œæ•´æ¨¡å‹ï¼ˆæˆ– LoRAï¼‰ | å€¼å‡½æ•°å¤´ï¼ˆValue Headï¼‰ | âœ… å¯ä»¥ç‹¬ç«‹ä½¿ç”¨ |\n",
    "\n",
    "## ğŸ”‘ å…³é”®åŒºåˆ«\n",
    "\n",
    "### SFT vs RM vs PPO çš„å­˜å‚¨å·®å¼‚\n",
    "\n",
    "1. **SFT**ï¼š\n",
    "   - åªä¿å­˜ LoRA æƒé‡ï¼ˆè½»é‡çº§ï¼‰\n",
    "   - ä¸ä¿®æ”¹åŸºç¡€æ¨¡å‹\n",
    "   - æ¨ç†æ—¶éœ€è¦åˆå¹¶\n",
    "\n",
    "2. **RM**ï¼š\n",
    "   - ä¿å­˜å¥–åŠ±å¤´ï¼ˆæ–°å¢çš„ Linear å±‚ï¼‰\n",
    "   - å¦‚æœå…¨é‡å¾®è°ƒï¼Œä¿å­˜å®Œæ•´æ¨¡å‹ï¼›å¦‚æœ LoRAï¼Œä¿å­˜ LoRA + å¥–åŠ±å¤´é…ç½®\n",
    "   - å¯ä»¥ç‹¬ç«‹ç”¨äºæ‰“åˆ†\n",
    "\n",
    "3. **PPO**ï¼š\n",
    "   - ä¿å­˜å€¼å‡½æ•°å¤´ï¼ˆæ–°å¢çš„ç»„ä»¶ï¼Œç”¨äº RLï¼‰\n",
    "   - ç­–ç•¥æ¨¡å‹å¯ä»¥ä¿æŒ LoRA æ ¼å¼æˆ–å…¨é‡ä¿å­˜\n",
    "   - å¯ä»¥ç‹¬ç«‹ç”¨äºç”Ÿæˆï¼ˆä½†å€¼å‡½æ•°å¤´åªåœ¨è®­ç»ƒæ—¶ä½¿ç”¨ï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64e31f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã€PPO è®­ç»ƒé…ç½®ã€‘\n",
    "# PPO è®­ç»ƒå‚æ•°ï¼šæ§åˆ¶å¼ºåŒ–å­¦ä¹ è®­ç»ƒæµç¨‹\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    # è¾“å‡ºä¸ä¿å­˜\n",
    "    output_dir=ppo_output_dir,              # æ¨¡å‹/æ—¥å¿—è¾“å‡ºç›®å½•\n",
    "    save_steps=500,                         # æ¯ N æ­¥ä¿å­˜ä¸€æ¬¡ checkpoint\n",
    "    save_total_limit=2,                     # æœ€å¤šä¿ç•™ N ä¸ª checkpoint\n",
    "    \n",
    "    # ç”Ÿæˆå‚æ•°ï¼ˆç­–ç•¥æ¨¡å‹ç”Ÿæˆå›ç­”æ—¶çš„å‚æ•°ï¼‰\n",
    "    mini_batch_size=1,                     # PPO mini-batch å¤§å°ï¼ˆæ¯ä¸ª prompt çš„å¤„ç†æ‰¹æ¬¡ï¼‰\n",
    "    batch_size=8,                          # PPO batch å¤§å°ï¼ˆæ”¶é›†ç»éªŒçš„æ‰¹æ¬¡ï¼‰\n",
    "    gradient_accumulation_steps=1,         # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
    "    \n",
    "    # PPO ç®—æ³•å‚æ•°\n",
    "    ppo_epochs=4,                           # PPO æ›´æ–°è½®æ¬¡ï¼ˆæ¯æ¬¡æ”¶é›†ç»éªŒåæ›´æ–°å¤šå°‘è½®ï¼‰\n",
    "    learning_rate=1e-6,                     # PPO å­¦ä¹ ç‡ï¼ˆé€šå¸¸è¾ƒå°ï¼Œ1e-6 åˆ° 1e-5ï¼‰\n",
    "    lr_scheduler_type=\"linear\",             # å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹\n",
    "    warmup_ratio=0.1,                       # warmup æ¯”ä¾‹\n",
    "    \n",
    "    # å¥–åŠ±ä¸ KL çº¦æŸ\n",
    "    init_kl_coef=0.1,                       # åˆå§‹ KL æƒ©ç½šç³»æ•°ï¼ˆå¹³è¡¡å¥–åŠ±ä¸ KL æ•£åº¦ï¼‰\n",
    "    target=6.0,                             # KL æ•£åº¦ç›®æ ‡å€¼ï¼ˆæ§åˆ¶ä¸å‚è€ƒæ¨¡å‹çš„åç¦»ç¨‹åº¦ï¼‰\n",
    "    horizon=10000,                          # PPO horizonï¼ˆå¥–åŠ±å½’ä¸€åŒ–å‚æ•°ï¼‰\n",
    "    gamma=1.0,                              # æŠ˜æ‰£å› å­ï¼ˆRL ä¸­çš„æœªæ¥å¥–åŠ±æŠ˜æ‰£ï¼‰\n",
    "    lam=0.95,                               # GAE lambda å‚æ•°ï¼ˆä¼˜åŠ¿ä¼°è®¡ï¼‰\n",
    "    \n",
    "    # ç”Ÿæˆå‚æ•°ï¼ˆæ¯ä¸ª prompt ç”Ÿæˆå¤šå°‘ä¸ªå€™é€‰å›ç­”ï¼‰\n",
    "    num_padding_at_beginning=1,             # å¡«å……ä½ç½®ï¼ˆæŸäº›æ¨¡å‹éœ€è¦ï¼‰\n",
    "    \n",
    "    # è®­ç»ƒæ§åˆ¶\n",
    "    max_grad_norm=1.0,                      # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰\n",
    "    report_to=[\"none\"],                     # ä¸å‘å¤–éƒ¨ä¸ŠæŠ¥ï¼ˆå¯æ”¹ä¸º [\"wandb\"] ç­‰ï¼‰\n",
    "    \n",
    "    # è¯„ä¼°ä¸æ—¥å¿—\n",
    "    log_with=\"none\",                        # æ—¥å¿—è®°å½•å·¥å…·ï¼ˆ\"wandb\", \"tensorboard\" ç­‰ï¼‰\n",
    "    logging_steps=10,                       # æ¯ N æ­¥æ‰“å°ä¸€æ¬¡æ—¥å¿—\n",
    "    \n",
    "    # æ€§èƒ½ä¼˜åŒ–\n",
    "    bf16=use_cuda,                          # CUDA ä¸Šç”¨ bfloat16\n",
    "    fp16=False,                             # ç¦ç”¨ FP16\n",
    "    \n",
    "    # åºåˆ—é•¿åº¦\n",
    "    max_length=max_seq_length,              # æœ€å¤§åºåˆ—é•¿åº¦\n",
    "    max_new_tokens=512,                    # æ¯æ¬¡ç”Ÿæˆçš„æœ€å¤§æ–° token æ•°\n",
    ")\n",
    "\n",
    "print(\"[PPO] è®­ç»ƒé…ç½®å®Œæˆ\")\n",
    "print(f\"[PPO] å…³é”®å‚æ•°ï¼š\")\n",
    "print(f\"  - PPO epochs: {ppo_config.ppo_epochs}ï¼ˆæ¯æ¬¡æ›´æ–° {ppo_config.ppo_epochs} è½®ï¼‰\")\n",
    "print(f\"  - Batch size: {ppo_config.batch_size}ï¼ˆæ”¶é›†ç»éªŒçš„æ‰¹æ¬¡å¤§å°ï¼‰\")\n",
    "print(f\"  - Mini batch size: {ppo_config.mini_batch_size}ï¼ˆPPO æ›´æ–°çš„å°æ‰¹æ¬¡ï¼‰\")\n",
    "print(f\"  - Learning rate: {ppo_config.learning_rate}ï¼ˆPPO å­¦ä¹ ç‡ï¼‰\")\n",
    "print(f\"  - KL coefficient: {ppo_config.init_kl_coef}ï¼ˆKL æƒ©ç½šç³»æ•°ï¼‰\")\n",
    "print(f\"  - Target KL: {ppo_config.target}ï¼ˆç›®æ ‡ KL æ•£åº¦ï¼‰\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1755d942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã€PPO è®­ç»ƒå™¨æ„é€ ä¸è®­ç»ƒæµç¨‹è¯´æ˜ã€‘\n",
    "from trl import PPOTrainer\n",
    "\n",
    "print(\"[PPO] æ„é€  PPO è®­ç»ƒå™¨...\")\n",
    "\n",
    "# æ³¨æ„ï¼šåœ¨æ„é€ è®­ç»ƒå™¨ä¹‹å‰ï¼Œéœ€è¦ç¡®ä¿å¥–åŠ±æ¨¡å‹å·²åŠ è½½\n",
    "if reward_model is None:\n",
    "    print(\"[PPO] âš ï¸  è­¦å‘Šï¼šå¥–åŠ±æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•è¿›è¡Œ PPO è®­ç»ƒ\")\n",
    "    print(\"[PPO] è¯·å…ˆå®Œæˆ RM è®­ç»ƒï¼Œç¡®ä¿ rm_model_dir æŒ‡å‘æœ‰æ•ˆçš„å¥–åŠ±æ¨¡å‹è·¯å¾„\")\n",
    "else:\n",
    "    # æ„é€  PPO è®­ç»ƒå™¨\n",
    "    ppo_trainer = PPOTrainer(\n",
    "        config=ppo_config,\n",
    "        model=ppo_model,                    # ç­–ç•¥æ¨¡å‹ï¼ˆå¸¦å€¼å‡½æ•°å¤´ï¼‰\n",
    "        ref_model=reference_model,          # å‚è€ƒæ¨¡å‹ï¼ˆç”¨äº KL çº¦æŸï¼‰\n",
    "        tokenizer=tokenizer,\n",
    "        dataset=ppo_dataset,                # æç¤ºæ•°æ®é›†\n",
    "    )\n",
    "    \n",
    "    print(\"[PPO] è®­ç»ƒå™¨æ„é€ å®Œæˆ\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"[PPO] PPO è®­ç»ƒæµç¨‹è¯´æ˜ï¼š\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\"\"\n",
    "    PPOï¼ˆProximal Policy Optimizationï¼‰çš„è®­ç»ƒæµç¨‹ï¼š\n",
    "    \n",
    "    1ï¸âƒ£ ã€æ”¶é›†ç»éªŒé˜¶æ®µã€‘ï¼ˆExperience Collectionï¼‰\n",
    "       - ç­–ç•¥æ¨¡å‹æ ¹æ® prompts ç”Ÿæˆå›ç­”\n",
    "       - å¥–åŠ±æ¨¡å‹å¯¹ç”Ÿæˆçš„å›ç­”æ‰“åˆ†ï¼ˆrewardï¼‰\n",
    "       - è®¡ç®—æ¯ä¸ªå›ç­”çš„ä¼˜åŠ¿ï¼ˆadvantageï¼‰å’Œä»·å€¼ï¼ˆvalueï¼‰\n",
    "       - æ”¶é›†ä¸€ä¸ª batch çš„ç»éªŒï¼ˆprompts, responses, rewards, advantagesï¼‰\n",
    "    \n",
    "    2ï¸âƒ£ ã€PPO æ›´æ–°é˜¶æ®µã€‘ï¼ˆPolicy Updateï¼‰\n",
    "       - å¯¹æ”¶é›†çš„ç»éªŒè¿›è¡Œå¤šè½® PPO æ›´æ–°ï¼ˆppo_epochs è½®ï¼‰\n",
    "       - è®¡ç®—ç­–ç•¥æŸå¤±ï¼ˆpolicy lossï¼‰ï¼šæœ€å¤§åŒ–æœŸæœ›å¥–åŠ±\n",
    "       - è®¡ç®—ä»·å€¼æŸå¤±ï¼ˆvalue lossï¼‰ï¼šä»·å€¼å‡½æ•°çš„å›å½’æŸå¤±\n",
    "       - è®¡ç®— KL æ•£åº¦æŸå¤±ï¼ˆKL penaltyï¼‰ï¼šä¿æŒä¸å‚è€ƒç­–ç•¥çš„æ¥è¿‘\n",
    "       - æ€»æŸå¤± = policy_loss - value_loss + kl_penalty\n",
    "       - ä½¿ç”¨æ¢¯åº¦ä¸Šå‡ä¼˜åŒ–ç­–ç•¥å‚æ•°\n",
    "    \n",
    "    3ï¸âƒ£ ã€KL çº¦æŸæ§åˆ¶ã€‘ï¼ˆKL Divergence Controlï¼‰\n",
    "       - åŠ¨æ€è°ƒæ•´ KL æƒ©ç½šç³»æ•°ï¼ˆkl_coefï¼‰\n",
    "       - å¦‚æœ KL æ•£åº¦è¶…è¿‡ç›®æ ‡å€¼ï¼Œå¢åŠ æƒ©ç½š\n",
    "       - å¦‚æœ KL æ•£åº¦ä½äºç›®æ ‡å€¼ï¼Œå‡å°‘æƒ©ç½š\n",
    "       - ç¡®ä¿ç­–ç•¥ä¸ä¼šåç¦»å‚è€ƒæ¨¡å‹å¤ªè¿œï¼ˆé¿å…æ¨¡å¼åç¼©ï¼‰\n",
    "    \n",
    "    4ï¸âƒ£ ã€é‡å¤è¿­ä»£ã€‘\n",
    "       - é‡å¤æ­¥éª¤ 1-3ï¼Œç›´åˆ°ç­–ç•¥æ”¶æ•›æˆ–è¾¾åˆ°æœ€å¤§æ­¥æ•°\n",
    "       - å®šæœŸä¿å­˜ checkpoint\n",
    "       - ç›‘æ§å¥–åŠ±ã€KL æ•£åº¦ã€ç”Ÿæˆè´¨é‡ç­‰æŒ‡æ ‡\n",
    "    \n",
    "    å…³é”®ä¼˜åŠ¿ï¼š\n",
    "    - âœ… ç¨³å®šçš„ç­–ç•¥æ›´æ–°ï¼ˆå‰ªåˆ‡ç­–ç•¥æ¢¯åº¦ï¼Œé¿å…å¤§å¹…æ³¢åŠ¨ï¼‰\n",
    "    - âœ… KL çº¦æŸé˜²æ­¢æ¨¡å¼åç¼©\n",
    "    - âœ… æ”¯æŒåœ¨çº¿å­¦ä¹ ï¼ˆè¾¹ç”Ÿæˆè¾¹ä¼˜åŒ–ï¼‰\n",
    "    - âœ… é€‚ç”¨äºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹\n",
    "    \n",
    "    æ³¨æ„äº‹é¡¹ï¼š\n",
    "    - âš ï¸  éœ€è¦é«˜è´¨é‡å¥–åŠ±æ¨¡å‹ï¼ˆRM çš„è´¨é‡ç›´æ¥å½±å“ PPO æ•ˆæœï¼‰\n",
    "    - âš ï¸  KL ç³»æ•°éœ€è¦ä»”ç»†è°ƒä¼˜ï¼ˆè¿‡å¤§å¯¼è‡´æ›´æ–°æ…¢ï¼Œè¿‡å°å¯¼è‡´åç¦»ï¼‰\n",
    "    - âš ï¸  ç›‘æ§é•¿åº¦åç½®ï¼ˆæ¨¡å‹å¯èƒ½å€¾å‘äºç”Ÿæˆæ›´é•¿çš„å›ç­”ä»¥è·å¾—æ›´é«˜å¥–åŠ±ï¼‰\n",
    "    - âš ï¸  é˜²æ­¢è¿‡æ‹Ÿåˆï¼ˆéœ€è¦å®šæœŸè¯„ä¼°ç”Ÿæˆè´¨é‡ï¼‰\n",
    "    \"\"\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # æ„é€ å¥–åŠ±å‡½æ•°ï¼ˆç”¨äº PPO è®­ç»ƒï¼‰\n",
    "    def reward_function(samples):\n",
    "        \"\"\"å¥–åŠ±å‡½æ•°ï¼šä½¿ç”¨å¥–åŠ±æ¨¡å‹å¯¹ç”Ÿæˆçš„å›ç­”æ‰“åˆ†\"\"\"\n",
    "        # å¯¹æ¯ä¸ªç”Ÿæˆçš„å›ç­”ä½¿ç”¨å¥–åŠ±æ¨¡å‹æ‰“åˆ†\n",
    "        inputs = tokenizer(\n",
    "            samples,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "        ).to(reward_model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # å¥–åŠ±æ¨¡å‹è¿”å› logitsï¼ˆæ ‡é‡å¥–åŠ±åˆ†æ•°ï¼‰\n",
    "            rewards = reward_model(**inputs).logits.squeeze(-1)\n",
    "        \n",
    "        return rewards\n",
    "    \n",
    "    print(\"\\n[PPO] å¥–åŠ±å‡½æ•°å·²å®šä¹‰ï¼ˆä½¿ç”¨å¥–åŠ±æ¨¡å‹æ‰“åˆ†ï¼‰\")\n",
    "    print(\"[PPO] è®­ç»ƒå™¨å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒ\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"[PPO] è®­ç»ƒç¤ºä¾‹ä»£ç ï¼ˆæ³¨é‡Šæ‰ï¼Œä¸å®é™…æ‰§è¡Œï¼‰ï¼š\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    example_code = '''\n",
    "    # å¼€å§‹ PPO è®­ç»ƒå¾ªç¯\n",
    "    generation_kwargs = {\n",
    "        \"max_new_tokens\": 512,              # ç”Ÿæˆçš„æœ€å¤§æ–° token æ•°\n",
    "        \"temperature\": 1.0,                # ç”Ÿæˆæ¸©åº¦ï¼ˆæ§åˆ¶éšæœºæ€§ï¼‰\n",
    "        \"do_sample\": True,                  # æ˜¯å¦é‡‡æ ·\n",
    "        \"top_p\": 0.95,                      # nucleus sampling\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    }\n",
    "    \n",
    "    # PPO è®­ç»ƒå¾ªç¯\n",
    "    for epoch in range(1):  # å¯ä»¥è®¾ç½®å¤šä¸ª epoch\n",
    "        for batch in ppo_trainer.dataloader:\n",
    "            # 1. ç­–ç•¥æ¨¡å‹ç”Ÿæˆå›ç­”\n",
    "            query_tensors = batch[\"input_ids\"]\n",
    "            response_tensors = ppo_trainer.generate(\n",
    "                query_tensors,\n",
    "                return_prompt=False,\n",
    "                length_sampler=None,\n",
    "                batch_size=ppo_config.batch_size,\n",
    "                **generation_kwargs,\n",
    "            )\n",
    "            \n",
    "            # 2. æå–ç”Ÿæˆçš„å›ç­”æ–‡æœ¬\n",
    "            batch[\"response\"] = [\n",
    "                tokenizer.decode(r.squeeze(), skip_special_tokens=True)\n",
    "                for r in response_tensors\n",
    "            ]\n",
    "            \n",
    "            # 3. è®¡ç®—å¥–åŠ±ï¼ˆä½¿ç”¨å¥–åŠ±æ¨¡å‹ï¼‰\n",
    "            texts = [\n",
    "                q + r for q, r in zip(batch[\"query\"], batch[\"response\"])\n",
    "            ]\n",
    "            rewards = reward_function(texts)\n",
    "            \n",
    "            # 4. PPO æ›´æ–°\n",
    "            stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "            ppo_trainer.log_stats(stats, batch, rewards)\n",
    "    \n",
    "    # ä¿å­˜æœ€ç»ˆæ¨¡å‹\n",
    "    ppo_trainer.save_model(ppo_output_dir)\n",
    "    tokenizer.save_pretrained(ppo_output_dir)\n",
    "    print(f\"[Done] PPO è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜è‡³: {ppo_output_dir}\")\n",
    "    '''\n",
    "    \n",
    "    print(example_code)\n",
    "    print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336dc170",
   "metadata": {},
   "source": [
    "## DPOï¼ˆDirect Preference Optimizationï¼‰\n",
    "- **å®šä½**ï¼šä½œä¸ºï¼ˆRM+PPOï¼‰çš„å¸¸è§æ›¿ä»£æ–¹æ¡ˆï¼Œç”¨åå¥½å¯¹ç›´æ¥ä¼˜åŒ–ç­–ç•¥ã€‚\n",
    "- **æ ¸å¿ƒ**ï¼šåŸºäº \\((x, y_{pos}, y_{neg})\\) æé«˜ \\(y_{pos}\\) æ¦‚ç‡ã€é™ä½ \\(y_{neg}\\)ï¼Œå¹¶ä»¥å‚è€ƒç­–ç•¥ \\(\\pi_{ref}\\) çš„å¯¹æ•°æ¦‚ç‡å·®ä½œéšå¼ KL çº¦æŸã€‚\n",
    "- **ç›´è§‚ç›®æ ‡**ï¼šæœ€å°åŒ– \\(-\\log \\sigma\\big(\\beta[(\\log \\pi_\\theta(y_{pos}|x) - \\log \\pi_\\theta(y_{neg}|x)) - (\\log \\pi_{ref}(y_{pos}|x) - \\log \\pi_{ref}(y_{neg}|x))]\\big)\\)\n",
    "- **ä¼˜ç‚¹**ï¼šæµç¨‹ç®€å•ã€æ— å¥–åŠ±æ¨¡å‹ä¸ RL å›è·¯ã€ç¨³å®šæ˜“å¤ç°ã€ååé«˜ã€‚\n",
    "- **å±€é™**ï¼šä¾èµ–é«˜è´¨é‡åå¥½æ•°æ®ï¼›æç«¯åˆ†å¸ƒè¿ç§»ä¸‹å¯æ§æ€§è¾ƒå¼±ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcb38e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã€DPO æ•°æ®å‡†å¤‡ï¼šå¤ç”¨åå¥½æ•°æ®é›†ã€‘\n",
    "# DPO ä½¿ç”¨ä¸ RM ç›¸åŒçš„åå¥½æ•°æ®æ ¼å¼ï¼ˆprompt, chosen, rejectedï¼‰\n",
    "# å¯ä»¥ç›´æ¥å¤ç”¨ä¹‹å‰åŠ è½½çš„ RM æ•°æ®é›†\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "print(\"[DPO] å‡†å¤‡åå¥½æ•°æ®...\")\n",
    "\n",
    "# æ–¹å¼ 1ï¼šç›´æ¥å¤ç”¨ RM è®­ç»ƒæ•°æ®ï¼ˆæ¨èï¼‰\n",
    "if 'rm_train_ds' in globals() and len(rm_train_ds) > 0:\n",
    "    # å¤ç”¨ RM çš„åå¥½æ•°æ®é›†ï¼ˆå·²ç»æ˜¯ prompt, chosen, rejected æ ¼å¼ï¼‰\n",
    "    dpo_train_ds = rm_train_ds.select(range(min(5000, len(rm_train_ds))))  # å¯ä»¥é€‰æ‹©éƒ¨åˆ†æ•°æ®\n",
    "    dpo_eval_ds = rm_eval_ds.select(range(min(200, len(rm_eval_ds))))\n",
    "    print(f\"[DPO] å¤ç”¨ RM è®­ç»ƒæ•°æ®ï¼šè®­ç»ƒé›† {len(dpo_train_ds)} æ¡ï¼ŒéªŒè¯é›† {len(dpo_eval_ds)} æ¡\")\n",
    "else:\n",
    "    # æ–¹å¼ 2ï¼šé‡æ–°åŠ è½½åå¥½æ•°æ®ï¼ˆå¦‚æœ RM æ•°æ®ä¸å¯ç”¨ï¼‰\n",
    "    print(\"[DPO] RM æ•°æ®ä¸å¯ç”¨ï¼Œå°è¯•é‡æ–°åŠ è½½åå¥½æ•°æ®...\")\n",
    "    if 'rm_ds' in globals():\n",
    "        dpo_split = rm_ds.train_test_split(test_size=0.02, seed=42)\n",
    "        dpo_train_ds = dpo_split[\"train\"].select(range(min(5000, len(dpo_split[\"train\"]))))\n",
    "        dpo_eval_ds = dpo_split[\"test\"].select(range(min(200, len(dpo_split[\"test\"]))))\n",
    "        print(f\"[DPO] ä» rm_ds åŠ è½½ï¼šè®­ç»ƒé›† {len(dpo_train_ds)} æ¡ï¼ŒéªŒè¯é›† {len(dpo_eval_ds)} æ¡\")\n",
    "    else:\n",
    "        # æ–¹å¼ 3ï¼šä½¿ç”¨ç¤ºä¾‹æ•°æ®ï¼ˆä»…ç”¨äºæ¼”ç¤ºï¼‰\n",
    "        print(\"[DPO] âš ï¸  è­¦å‘Šï¼šä½¿ç”¨ç¤ºä¾‹æ•°æ®ï¼Œå®é™…è®­ç»ƒåº”ä½¿ç”¨çœŸå®åå¥½æ•°æ®\")\n",
    "        dpo_examples = [\n",
    "            {\n",
    "                \"prompt\": \"è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ã€‚\",\n",
    "                \"chosen\": \"æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ è§„å¾‹ï¼Œæ— éœ€æ˜ç¡®ç¼–ç¨‹å°±èƒ½åšå‡ºé¢„æµ‹æˆ–å†³ç­–ã€‚\",\n",
    "                \"rejected\": \"æœºå™¨å­¦ä¹ å°±æ˜¯ä¸€ç§ç¼–ç¨‹æ–¹å¼ã€‚\"\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": \"å¦‚ä½•å­¦ä¹  Pythonï¼Ÿ\",\n",
    "                \"chosen\": \"å­¦ä¹  Python å¯ä»¥ä»åŸºç¡€è¯­æ³•å¼€å§‹ï¼Œç„¶åé€æ­¥å­¦ä¹ æ•°æ®ç»“æ„ã€é¢å‘å¯¹è±¡ç¼–ç¨‹ï¼Œå¹¶é€šè¿‡å®é™…é¡¹ç›®ç»ƒä¹ æå‡ã€‚\",\n",
    "                \"rejected\": \"Python å¾ˆç®€å•ï¼Œéšä¾¿å­¦å­¦å°±è¡Œã€‚\"\n",
    "            },\n",
    "        ]\n",
    "        dpo_train_ds = Dataset.from_list(dpo_examples)\n",
    "        dpo_eval_ds = Dataset.from_list(dpo_examples[:1])\n",
    "\n",
    "print(f\"\\n[DPO] æ•°æ®å‡†å¤‡å®Œæˆï¼š\")\n",
    "print(f\"  è®­ç»ƒé›†ï¼š{len(dpo_train_ds)} æ¡\")\n",
    "print(f\"  éªŒè¯é›†ï¼š{len(dpo_eval_ds)} æ¡\")\n",
    "print(f\"\\n[DPO] ç¤ºä¾‹æ•°æ®ï¼š\")\n",
    "sample = dpo_train_ds[0]\n",
    "for k, v in sample.items():\n",
    "    if isinstance(v, str) and len(v) > 100:\n",
    "        print(f\"  {k}: {v[:100]}...\")\n",
    "    else:\n",
    "        print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6a0c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã€DPO æ¨¡å‹åŠ è½½ï¼šç­–ç•¥æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹ã€‘\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "import torch\n",
    "\n",
    "# DPO è¾“å‡ºé…ç½®\n",
    "dpo_output_dir = \"outputs/dpo_qlora\"\n",
    "max_seq_length = 2048\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_mps = torch.backends.mps.is_available()\n",
    "\n",
    "print(\"[DPO] å¼€å§‹åŠ è½½æ¨¡å‹...\")\n",
    "\n",
    "# ========== 1. åŠ è½½ç­–ç•¥æ¨¡å‹ï¼ˆé€šå¸¸æ˜¯ SFT æ¨¡å‹ï¼‰==========\n",
    "# DPO éœ€è¦ç­–ç•¥æ¨¡å‹ï¼ˆè®­ç»ƒä¸­çš„æ¨¡å‹ï¼‰å’Œå‚è€ƒæ¨¡å‹ï¼ˆç”¨äº KL çº¦æŸï¼Œå†»ç»“å‚æ•°ï¼‰\n",
    "if 'adapter_dir' in globals() and os.path.exists(adapter_dir):\n",
    "    try:\n",
    "        # ä» SFT æ¨¡å‹åŠ è½½ï¼ˆå¸¦ LoRA æƒé‡ï¼‰\n",
    "        base_policy = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_or_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        dpo_policy_model = PeftModel.from_pretrained(base_policy, adapter_dir)\n",
    "        print(\"[DPO] ç­–ç•¥æ¨¡å‹ï¼šä½¿ç”¨ SFT æ¨¡å‹ï¼ˆå¸¦ LoRA æƒé‡ï¼‰\")\n",
    "    except Exception as e:\n",
    "        print(f\"[DPO] æ— æ³•åŠ è½½ SFT æ¨¡å‹ï¼š{e}ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹\")\n",
    "        dpo_policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_or_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "else:\n",
    "    # å¦‚æœæœªæ‰¾åˆ° SFT æ¨¡å‹ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹\n",
    "    dpo_policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_or_dir,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    print(\"[DPO] ç­–ç•¥æ¨¡å‹ï¼šä½¿ç”¨åŸºç¡€æ¨¡å‹ï¼ˆæœªæ‰¾åˆ° SFT æ¨¡å‹ï¼‰\")\n",
    "\n",
    "# ========== 2. åŠ è½½å‚è€ƒæ¨¡å‹ï¼ˆç”¨äº KL çº¦æŸï¼Œå†»ç»“å‚æ•°ï¼‰==========\n",
    "# å‚è€ƒæ¨¡å‹é€šå¸¸æ˜¯ SFT æ¨¡å‹çš„å‰¯æœ¬ï¼Œä½†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å†»ç»“å‚æ•°\n",
    "if 'adapter_dir' in globals() and os.path.exists(adapter_dir):\n",
    "    try:\n",
    "        # åŠ è½½å‚è€ƒæ¨¡å‹ï¼ˆä¸ç­–ç•¥æ¨¡å‹ç›¸åŒï¼Œä½†å†»ç»“å‚æ•°ï¼‰\n",
    "        base_ref = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_or_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        # å¯¹äº LoRA æ¨¡å‹ï¼Œå¯ä»¥åŠ è½½ç›¸åŒçš„ adapterï¼Œç„¶ååœ¨è®­ç»ƒå™¨ä¸­å†»ç»“\n",
    "        dpo_ref_model = PeftModel.from_pretrained(base_ref, adapter_dir)\n",
    "        # å†»ç»“å‚è€ƒæ¨¡å‹çš„å‚æ•°ï¼ˆDPOTrainer ä¼šè‡ªåŠ¨å¤„ç†ï¼‰\n",
    "        print(\"[DPO] å‚è€ƒæ¨¡å‹ï¼šä½¿ç”¨ SFT æ¨¡å‹ï¼ˆå¸¦ LoRA æƒé‡ï¼Œè®­ç»ƒæ—¶å†»ç»“ï¼‰\")\n",
    "    except Exception as e:\n",
    "        print(f\"[DPO] æ— æ³•åŠ è½½ SFT æ¨¡å‹ä½œä¸ºå‚è€ƒï¼š{e}ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹\")\n",
    "        dpo_ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_or_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "else:\n",
    "    # å¦‚æœæœªæ‰¾åˆ° SFT æ¨¡å‹ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹\n",
    "    dpo_ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_or_dir,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    print(\"[DPO] å‚è€ƒæ¨¡å‹ï¼šä½¿ç”¨åŸºç¡€æ¨¡å‹ï¼ˆæœªæ‰¾åˆ° SFT æ¨¡å‹ï¼‰\")\n",
    "\n",
    "# Tokenizer åˆå§‹åŒ–ï¼ˆå¦‚æœä¹‹å‰æœªå®šä¹‰ï¼‰\n",
    "if 'tokenizer' not in globals() or tokenizer is None:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        base_model_or_dir,\n",
    "        use_fast=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"[DPO] æ¨¡å‹åŠ è½½å®Œæˆ\")\n",
    "print(f\"[DPO] ç­–ç•¥æ¨¡å‹ï¼š{type(dpo_policy_model).__name__}\")\n",
    "print(f\"[DPO] å‚è€ƒæ¨¡å‹ï¼š{type(dpo_ref_model).__name__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef44db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã€DPO LoRA é…ç½®ä¸è®­ç»ƒå‚æ•°ã€‘\n",
    "# DPO å¯ä»¥ä½¿ç”¨ LoRA å¾®è°ƒï¼Œä¹Ÿå¯ä»¥å…¨é‡å¾®è°ƒ\n",
    "\n",
    "# LoRA é…ç½®ï¼ˆå¦‚æœéœ€è¦ä½¿ç”¨ LoRAï¼‰\n",
    "dpo_lora_config = None\n",
    "if True:  # é»˜è®¤ä½¿ç”¨ LoRAï¼ˆæ›´èŠ‚çœæ˜¾å­˜ï¼‰\n",
    "    dpo_lora_config = LoraConfig(\n",
    "        r=16,                    # ç§©ï¼ˆrankï¼‰\n",
    "        lora_alpha=32,          # ç¼©æ”¾ç³»æ•°ï¼ˆé€šå¸¸ alpha=2*rï¼‰\n",
    "        lora_dropout=0.05,      # é€‚é…å™¨å±‚ dropout\n",
    "        bias=\"none\",            # ä¸è®­ç»ƒ bias\n",
    "        task_type=\"CAUSAL_LM\",  # ä»»åŠ¡ç±»å‹ï¼šå› æœè¯­è¨€æ¨¡å‹\n",
    "    )\n",
    "    print(\"[DPO] ä½¿ç”¨ LoRA é…ç½®ï¼ˆr=16, alpha=32ï¼‰\")\n",
    "else:\n",
    "    print(\"[DPO] ä½¿ç”¨å…¨é‡å¾®è°ƒï¼ˆä¸ä½¿ç”¨ LoRAï¼‰\")\n",
    "\n",
    "# bitsandbytes QLoRA é…ç½®ï¼ˆå¯é€‰ï¼‰\n",
    "dpo_quantization_config = None\n",
    "if use_cuda:\n",
    "    try:\n",
    "        dpo_quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "        print(\"[DPO] ä½¿ç”¨ bitsandbytes 4-bit é‡åŒ–åŠ è½½æ¨¡å‹\")\n",
    "    except Exception as e:\n",
    "        print(f\"[DPO] æœªå¯ç”¨4bité‡åŒ–ï¼š{e}\")\n",
    "else:\n",
    "    print(\"[DPO] å½“å‰ä¸ºé CUDA ç¯å¢ƒï¼Œä½¿ç”¨å¸¸è§„ç²¾åº¦åŠ è½½\")\n",
    "\n",
    "# DPO è®­ç»ƒå‚æ•°ï¼šæ§åˆ¶è®­ç»ƒæµç¨‹ä¸ä¼˜åŒ–\n",
    "dpo_args = TrainingArguments(\n",
    "    # è¾“å‡ºä¸ä¿å­˜\n",
    "    output_dir=dpo_output_dir,              # æ¨¡å‹/æ—¥å¿—è¾“å‡ºç›®å½•\n",
    "    save_steps=500,                          # æ¯ N æ­¥ä¿å­˜ä¸€æ¬¡ checkpoint\n",
    "    save_total_limit=2,                      # æœ€å¤šä¿ç•™ N ä¸ª checkpoint\n",
    "    save_safetensors=True,                   # ä½¿ç”¨ safetensors æ ¼å¼ä¿å­˜\n",
    "    \n",
    "    # æ‰¹å¤§å°ä¸æ¢¯åº¦\n",
    "    per_device_train_batch_size=2,          # æ¯è®¾å¤‡è®­ç»ƒ batch å¤§å°ï¼ˆDPO éœ€è¦å¤„ç† chosen/rejected å¯¹ï¼‰\n",
    "    per_device_eval_batch_size=2,           # æ¯è®¾å¤‡éªŒè¯ batch å¤§å°\n",
    "    gradient_accumulation_steps=4,          # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆç­‰æ•ˆ batch = 2 * 4 = 8ï¼‰\n",
    "    \n",
    "    # è®­ç»ƒè½®æ¬¡ä¸å­¦ä¹ ç‡\n",
    "    num_train_epochs=1,                      # è®­ç»ƒè½®æ¬¡æ•°\n",
    "    learning_rate=1e-5,                     # åˆå§‹å­¦ä¹ ç‡ï¼ˆDPO å¸¸ç”¨ 1e-5 åˆ° 5e-5ï¼‰\n",
    "    lr_scheduler_type=\"cosine\",             # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆä½™å¼¦é€€ç«ï¼‰\n",
    "    warmup_ratio=0.1,                       # warmup æ¯”ä¾‹ï¼ˆå‰ 10% æ­¥æ•°çº¿æ€§å¢é•¿ LRï¼‰\n",
    "    \n",
    "    # è¯„ä¼°ä¸æ—¥å¿—\n",
    "    eval_strategy=\"steps\",                  # è¯„ä¼°ç­–ç•¥ï¼ˆ\"steps\"/\"epoch\"/\"no\"ï¼‰\n",
    "    eval_steps=250,                         # æ¯ N æ­¥è¯„ä¼°ä¸€æ¬¡\n",
    "    logging_steps=50,                       # æ¯ N æ­¥æ‰“å°ä¸€æ¬¡æ—¥å¿—\n",
    "    report_to=[\"none\"],                     # ä¸å‘å¤–éƒ¨ä¸ŠæŠ¥ï¼ˆå¯æ”¹ä¸º [\"wandb\"] ç­‰ï¼‰\n",
    "    \n",
    "    # æ¨¡å‹æ£€æŸ¥ç‚¹\n",
    "    load_best_model_at_end=True,            # è®­ç»ƒç»“æŸåŠ è½½æœ€ä½³æ¨¡å‹\n",
    "    metric_for_best_model=\"eval_loss\",     # æœ€ä½³æ¨¡å‹æŒ‡æ ‡\n",
    "    greater_is_better=False,                # è¯¥æŒ‡æ ‡è¶Šå°è¶Šå¥½\n",
    "    \n",
    "    # æ€§èƒ½ä¼˜åŒ–\n",
    "    bf16=use_cuda,                          # CUDA ä¸Šç”¨ bfloat16\n",
    "    fp16=False,                             # ç¦ç”¨ FP16\n",
    "    gradient_checkpointing=True,           # æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆç‰ºç‰²æ—¶é—´æ¢æ˜¾å­˜ï¼‰\n",
    ")\n",
    "\n",
    "# DPO ç‰¹å®šé…ç½®ï¼ˆbeta æ¸©åº¦å‚æ•°ï¼‰\n",
    "dpo_config = DPOConfig(\n",
    "    beta=0.1,                               # æ¸©åº¦å‚æ•°ï¼ˆæ§åˆ¶ KL çº¦æŸå¼ºåº¦ï¼Œå¸¸ç”¨ 0.1-0.5ï¼‰\n",
    "    loss_type=\"sigmoid\",                    # æŸå¤±ç±»å‹ï¼ˆ\"sigmoid\" æˆ– \"hinge\"ï¼‰\n",
    "    label_smoothing=0.0,                    # æ ‡ç­¾å¹³æ»‘ï¼ˆ0.0 è¡¨ç¤ºä¸å¹³æ»‘ï¼‰\n",
    "    reference_free=False,                   # æ˜¯å¦ä¸ä½¿ç”¨å‚è€ƒæ¨¡å‹ï¼ˆFalse è¡¨ç¤ºä½¿ç”¨ï¼‰\n",
    ")\n",
    "\n",
    "print(\"[DPO] è®­ç»ƒå‚æ•°é…ç½®å®Œæˆ\")\n",
    "print(f\"[DPO] å…³é”®å‚æ•°ï¼š\")\n",
    "print(f\"  - Learning rate: {dpo_args.learning_rate}ï¼ˆDPO å­¦ä¹ ç‡ï¼‰\")\n",
    "print(f\"  - Beta (Î²): {dpo_config.beta}ï¼ˆKL çº¦æŸå¼ºåº¦ï¼Œè¶Šå¤§è¶Šæ¥è¿‘å‚è€ƒæ¨¡å‹ï¼‰\")\n",
    "print(f\"  - Batch size: {dpo_args.per_device_train_batch_size}ï¼ˆæ¯è®¾å¤‡æ‰¹æ¬¡å¤§å°ï¼‰\")\n",
    "print(f\"  - Gradient accumulation: {dpo_args.gradient_accumulation_steps}ï¼ˆæ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼‰\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ab23bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã€DPO è®­ç»ƒå™¨æ„é€ ä¸è®­ç»ƒã€‘\n",
    "from trl import DPOTrainer\n",
    "\n",
    "print(\"[DPO] æ„é€  DPO è®­ç»ƒå™¨...\")\n",
    "\n",
    "# å¦‚æœç­–ç•¥æ¨¡å‹ä½¿ç”¨äº†é‡åŒ–ï¼Œéœ€è¦åº”ç”¨ LoRAï¼ˆå¦‚æœä½¿ç”¨ï¼‰\n",
    "if dpo_quantization_config is not None or use_cuda:\n",
    "    # å¦‚æœå·²ç»é‡åŒ–æˆ–ä½¿ç”¨ CUDAï¼Œå°è¯•åº”ç”¨ LoRA\n",
    "    try:\n",
    "        if dpo_lora_config is not None:\n",
    "            from peft import get_peft_model\n",
    "            # æ³¨æ„ï¼šå¦‚æœç­–ç•¥æ¨¡å‹å·²ç»æ˜¯ PeftModelï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†\n",
    "            if not isinstance(dpo_policy_model, PeftModel):\n",
    "                dpo_policy_model = get_peft_model(dpo_policy_model, dpo_lora_config)\n",
    "                print(\"[DPO] LoRA é…ç½®å·²åº”ç”¨åˆ°ç­–ç•¥æ¨¡å‹\")\n",
    "    except Exception as e:\n",
    "        print(f\"[DPO] åº”ç”¨ LoRA æ—¶å‡ºé”™ï¼š{e}ï¼Œç»§ç»­ä½¿ç”¨å½“å‰é…ç½®\")\n",
    "\n",
    "# æ„é€  DPO è®­ç»ƒå™¨\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=dpo_policy_model,                  # ç­–ç•¥æ¨¡å‹ï¼ˆè¦ä¼˜åŒ–çš„æ¨¡å‹ï¼‰\n",
    "    ref_model=dpo_ref_model,                 # å‚è€ƒæ¨¡å‹ï¼ˆç”¨äº KL çº¦æŸï¼Œå†»ç»“å‚æ•°ï¼‰\n",
    "    args=dpo_args,                           # è®­ç»ƒå‚æ•°\n",
    "    beta=dpo_config.beta,                    # æ¸©åº¦å‚æ•°ï¼ˆKL çº¦æŸå¼ºåº¦ï¼‰\n",
    "    train_dataset=dpo_train_ds,             # è®­ç»ƒæ•°æ®é›†ï¼ˆprompt, chosen, rejectedï¼‰\n",
    "    eval_dataset=dpo_eval_ds,                # éªŒè¯æ•°æ®é›†\n",
    "    tokenizer=tokenizer,                     # Tokenizer\n",
    "    peft_config=dpo_lora_config if dpo_quantization_config is None and not use_cuda else None,  # LoRA é…ç½®\n",
    "    max_length=max_seq_length,              # æœ€å¤§åºåˆ—é•¿åº¦\n",
    "    max_target_length=max_seq_length,        # æœ€å¤§ç›®æ ‡é•¿åº¦\n",
    "    loss_type=dpo_config.loss_type,          # æŸå¤±ç±»å‹ï¼ˆ\"sigmoid\" æˆ– \"hinge\"ï¼‰\n",
    "    label_smoothing=dpo_config.label_smoothing,  # æ ‡ç­¾å¹³æ»‘\n",
    ")\n",
    "\n",
    "print(\"[DPO] è®­ç»ƒå™¨æ„é€ å®Œæˆ\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"[DPO] DPO è®­ç»ƒæµç¨‹è¯´æ˜ï¼š\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "DPOï¼ˆDirect Preference Optimizationï¼‰çš„è®­ç»ƒæµç¨‹ï¼š\n",
    "\n",
    "1ï¸âƒ£ ã€æ•°æ®å‡†å¤‡ã€‘\n",
    "   - å‡†å¤‡åå¥½æ•°æ®ï¼ˆprompt, chosen, rejectedï¼‰\n",
    "   - chosenï¼šæ›´å¥½çš„å›ç­”\n",
    "   - rejectedï¼šæ›´å·®çš„å›ç­”\n",
    "\n",
    "2ï¸âƒ£ ã€æ¨¡å‹å‡†å¤‡ã€‘\n",
    "   - ç­–ç•¥æ¨¡å‹ï¼ˆÏ€_Î¸ï¼‰ï¼šè¦ä¼˜åŒ–çš„æ¨¡å‹ï¼ˆé€šå¸¸æ˜¯ SFT æ¨¡å‹ï¼‰\n",
    "   - å‚è€ƒæ¨¡å‹ï¼ˆÏ€_refï¼‰ï¼šç”¨äº KL çº¦æŸçš„åŸºå‡†æ¨¡å‹ï¼ˆå†»ç»“å‚æ•°ï¼‰\n",
    "\n",
    "3ï¸âƒ£ ã€DPO è®­ç»ƒã€‘\n",
    "   - è®¡ç®—ç­–ç•¥æ¨¡å‹å¯¹ chosen/rejected çš„å¯¹æ•°æ¦‚ç‡\n",
    "   - è®¡ç®—å‚è€ƒæ¨¡å‹å¯¹ chosen/rejected çš„å¯¹æ•°æ¦‚ç‡\n",
    "   - è®¡ç®— DPO æŸå¤±ï¼š\n",
    "     L = -log Ïƒ(Î²[(log Ï€_Î¸(chosen|x) - log Ï€_Î¸(rejected|x)) - \n",
    "                  (log Ï€_ref(chosen|x) - log Ï€_ref(rejected|x))])\n",
    "   - ä¼˜åŒ–ç­–ç•¥æ¨¡å‹ï¼Œæœ€å¤§åŒ– chosen æ¦‚ç‡ï¼Œæœ€å°åŒ– rejected æ¦‚ç‡\n",
    "   - é€šè¿‡éšå¼ KL çº¦æŸé˜²æ­¢åç¦»å‚è€ƒæ¨¡å‹\n",
    "\n",
    "4ï¸âƒ£ ã€ä¿å­˜æ¨¡å‹ã€‘\n",
    "   - ä¿å­˜ä¼˜åŒ–åçš„ç­–ç•¥æ¨¡å‹ï¼ˆå¯ä»¥æ˜¯ LoRA æƒé‡ï¼‰\n",
    "\n",
    "å…³é”®ä¼˜åŠ¿ï¼š\n",
    "- âœ… æ— éœ€è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰\n",
    "- âœ… æ— éœ€å¼ºåŒ–å­¦ä¹ ï¼ˆPPOï¼‰\n",
    "- âœ… è®­ç»ƒç®€å•ç¨³å®šï¼ˆç›‘ç£å­¦ä¹ ï¼‰\n",
    "- âœ… è®­ç»ƒé€Ÿåº¦å¿«ï¼ˆç›´æ¥ä¼˜åŒ–ï¼‰\n",
    "- âœ… æ˜“å¤ç°ï¼ˆç¡®å®šæ€§è®­ç»ƒï¼‰\n",
    "\n",
    "å…³é”®å‚æ•°ï¼š\n",
    "- Î² (beta)ï¼šæ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶ KL çº¦æŸå¼ºåº¦ï¼ˆå¸¸ç”¨ 0.1-0.5ï¼‰\n",
    "  - Î² è¶Šå¤§ â†’ æ›´æ¥è¿‘å‚è€ƒæ¨¡å‹ï¼ˆä¿å®ˆï¼‰\n",
    "  - Î² è¶Šå° â†’ æ›´åç¦»å‚è€ƒæ¨¡å‹ï¼ˆæ¿€è¿›ï¼‰\n",
    "\"\"\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# å¼€å§‹è®­ç»ƒï¼ˆæ³¨é‡Šæ‰ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©æ˜¯å¦æ‰§è¡Œï¼‰\n",
    "print(\"\\n[DPO] å‡†å¤‡å¼€å§‹è®­ç»ƒ...\")\n",
    "print(\"[DPO] è¦å¼€å§‹è®­ç»ƒï¼Œè¯·å–æ¶ˆä¸‹é¢ä»£ç çš„æ³¨é‡Š\")\n",
    "\n",
    "# è®­ç»ƒä»£ç ï¼ˆæ³¨é‡Šæ‰ï¼Œä¸å®é™…æ‰§è¡Œï¼‰\n",
    "# dpo_trainer.train()\n",
    "\n",
    "# ä¿å­˜æ¨¡å‹ï¼ˆæ³¨é‡Šæ‰ï¼‰\n",
    "# dpo_model_dir = os.path.join(dpo_output_dir, \"dpo_model\")\n",
    "# dpo_trainer.model.save_pretrained(dpo_model_dir)\n",
    "# tokenizer.save_pretrained(dpo_model_dir)\n",
    "# print(f\"[Done] DPO è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜è‡³: {dpo_model_dir}\")\n",
    "\n",
    "print(\"\\n[DPO] è®­ç»ƒä»£ç å·²å‡†å¤‡ï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86705062",
   "metadata": {},
   "source": [
    "## ğŸ“Š DPO vs RM+PPO å®Œæ•´å¯¹æ¯”\n",
    "\n",
    "### ğŸ”„ æµç¨‹å¯¹æ¯”\n",
    "\n",
    "```\n",
    "ä¼ ç»Ÿ RLHFï¼ˆRM + PPOï¼‰æµç¨‹ï¼š\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  1ï¸âƒ£ SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰                        â”‚\n",
    "â”‚     â†“                                    â”‚\n",
    "â”‚  2ï¸âƒ£ RMï¼ˆå¥–åŠ±æ¨¡å‹è®­ç»ƒï¼‰                     â”‚\n",
    "â”‚     â”œâ”€â”€ è®­ç»ƒå¥–åŠ±å¤´                        â”‚\n",
    "â”‚     â””â”€â”€ å­¦ä¼šæ‰“åˆ†ï¼ˆchosen > rejectedï¼‰      â”‚\n",
    "â”‚     â†“                                    â”‚\n",
    "â”‚  3ï¸âƒ£ PPOï¼ˆå¼ºåŒ–ä¼˜åŒ–ï¼‰                        â”‚\n",
    "â”‚     â”œâ”€â”€ ç­–ç•¥æ¨¡å‹ç”Ÿæˆå›ç­”                   â”‚\n",
    "â”‚     â”œâ”€â”€ å¥–åŠ±æ¨¡å‹æ‰“åˆ†                       â”‚\n",
    "â”‚     â”œâ”€â”€ PPO ä¼˜åŒ–ç­–ç•¥                      â”‚\n",
    "â”‚     â””â”€â”€ å€¼å‡½æ•°å¤´ï¼ˆç”¨äºä¼˜åŠ¿ä¼°è®¡ï¼‰            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "DPO æµç¨‹ï¼š\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  1ï¸âƒ£ SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰                        â”‚\n",
    "â”‚     â†“                                    â”‚\n",
    "â”‚  2ï¸âƒ£ DPOï¼ˆç›´æ¥åå¥½ä¼˜åŒ–ï¼‰                    â”‚\n",
    "â”‚     â”œâ”€â”€ ç›´æ¥ç”¨åå¥½æ•°æ®è®­ç»ƒ                â”‚\n",
    "â”‚     â”œâ”€â”€ æœ€å¤§åŒ– chosen æ¦‚ç‡               â”‚\n",
    "â”‚     â”œâ”€â”€ æœ€å°åŒ– rejected æ¦‚ç‡              â”‚\n",
    "â”‚     â””â”€â”€ éšå¼ KL çº¦æŸï¼ˆé€šè¿‡å‚è€ƒæ¨¡å‹ï¼‰       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ğŸ¯ å…³é”®åŒºåˆ«\n",
    "\n",
    "| ç‰¹æ€§ | **RM + PPO** | **DPO** |\n",
    "|------|------------|---------|\n",
    "| **è®­ç»ƒé˜¶æ®µ** | 3 é˜¶æ®µï¼ˆSFT â†’ RM â†’ PPOï¼‰ | 2 é˜¶æ®µï¼ˆSFT â†’ DPOï¼‰ |\n",
    "| **éœ€è¦å¥–åŠ±æ¨¡å‹** | âœ… æ˜¯ | âŒ å¦ |\n",
    "| **éœ€è¦å¼ºåŒ–å­¦ä¹ ** | âœ… æ˜¯ï¼ˆPPOï¼‰ | âŒ å¦ |\n",
    "| **éœ€è¦å€¼å‡½æ•°å¤´** | âœ… æ˜¯ï¼ˆç”¨äº PPOï¼‰ | âŒ å¦ |\n",
    "| **KL çº¦æŸæ–¹å¼** | æ˜¾å¼ KL æƒ©ç½š | éšå¼ KL çº¦æŸï¼ˆé€šè¿‡å‚è€ƒæ¨¡å‹ï¼‰ |\n",
    "| **è®­ç»ƒå¤æ‚åº¦** | é«˜ï¼ˆRL å›è·¯ï¼‰ | ä½ï¼ˆç›‘ç£å­¦ä¹ ï¼‰ |\n",
    "| **è®­ç»ƒç¨³å®šæ€§** | ä¸­ç­‰ï¼ˆRL ä¸ç¨³å®šï¼‰ | é«˜ï¼ˆç›‘ç£å­¦ä¹ ï¼‰ |\n",
    "| **è®­ç»ƒé€Ÿåº¦** | æ…¢ï¼ˆç”Ÿæˆ+æ‰“åˆ†å¾ªç¯ï¼‰ | å¿«ï¼ˆç›´æ¥ä¼˜åŒ–ï¼‰ |\n",
    "| **æ•°æ®éœ€æ±‚** | Promptsï¼ˆç”Ÿæˆæ—¶ç”¨ï¼‰ | åå¥½å¯¹ï¼ˆchosen/rejectedï¼‰ |\n",
    "| **èµ„æºéœ€æ±‚** | é«˜ï¼ˆéœ€è¦ RMï¼‰ | ä½ï¼ˆæ— éœ€ RMï¼‰ |\n",
    "| **é€‚ç”¨åœºæ™¯** | éœ€è¦çµæ´»å¥–åŠ±æ§åˆ¶ | å›ºå®šåå¥½æ•°æ® |\n",
    "\n",
    "### ğŸ“¦ æƒé‡å­˜å‚¨å¯¹æ¯”\n",
    "\n",
    "| é˜¶æ®µ | **RM + PPO** | **DPO** |\n",
    "|------|------------|---------|\n",
    "| **SFT** | LoRA æƒé‡ | LoRA æƒé‡ |\n",
    "| **RM** | åŸºç¡€æ¨¡å‹ + å¥–åŠ±å¤´ | âŒ ä¸éœ€è¦ |\n",
    "| **PPO** | ç­–ç•¥æ¨¡å‹ + å€¼å‡½æ•°å¤´ | âŒ ä¸éœ€è¦ |\n",
    "| **DPO** | âŒ ä¸éœ€è¦ | ç­–ç•¥æ¨¡å‹ï¼ˆå¯ä»¥æ˜¯ LoRAï¼‰ |\n",
    "\n",
    "### âœ… ä½•æ—¶ä½¿ç”¨ DPO\n",
    "\n",
    "**é€‚åˆä½¿ç”¨ DPO çš„åœºæ™¯**ï¼š\n",
    "- âœ… æœ‰é«˜è´¨é‡åå¥½æ•°æ®ï¼ˆchosen/rejected å¯¹ï¼‰\n",
    "- âœ… ä¸éœ€è¦åŠ¨æ€å¥–åŠ±è°ƒæ•´\n",
    "- âœ… è¿½æ±‚è®­ç»ƒç®€å•æ€§å’Œç¨³å®šæ€§\n",
    "- âœ… èµ„æºå—é™ï¼ˆä¸æƒ³è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼‰\n",
    "- âœ… éœ€è¦å¿«é€Ÿè¿­ä»£\n",
    "\n",
    "**é€‚åˆä½¿ç”¨ RM+PPO çš„åœºæ™¯**ï¼š\n",
    "- âœ… éœ€è¦åŠ¨æ€å¥–åŠ±è°ƒæ•´ï¼ˆåœ¨çº¿å­¦ä¹ ï¼‰\n",
    "- âœ… éœ€è¦çµæ´»çš„å¥–åŠ±æ§åˆ¶\n",
    "- âœ… åå¥½æ•°æ®è¾ƒå°‘ï¼Œä½†æœ‰å¾ˆå¤š prompts\n",
    "- âœ… éœ€è¦ç²¾ç»†çš„ RL æ§åˆ¶\n",
    "\n",
    "### ğŸ”‘ DPO çš„å…³é”®å‚æ•°\n",
    "\n",
    "1. **beta (Î²)**ï¼šæ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶ KL çº¦æŸå¼ºåº¦\n",
    "   - **èŒƒå›´**ï¼š0.1 - 0.5ï¼ˆå¸¸ç”¨ï¼‰\n",
    "   - **Î² è¶Šå¤§**ï¼šæ›´ä¿å®ˆï¼Œæ›´æ¥è¿‘å‚è€ƒæ¨¡å‹\n",
    "   - **Î² è¶Šå°**ï¼šæ›´æ¿€è¿›ï¼Œæ›´å®¹æ˜“åç¦»å‚è€ƒæ¨¡å‹\n",
    "   - **æ¨èå€¼**ï¼š0.1ï¼ˆé»˜è®¤ï¼‰ï¼Œæ ¹æ®å®é™…æ•ˆæœè°ƒæ•´\n",
    "\n",
    "2. **å‚è€ƒæ¨¡å‹**ï¼šç”¨äº KL çº¦æŸçš„åŸºå‡†æ¨¡å‹\n",
    "   - **é€šå¸¸æ˜¯**ï¼šSFT æ¨¡å‹ï¼ˆå†»ç»“å‚æ•°ï¼‰\n",
    "   - **ä½œç”¨**ï¼šé˜²æ­¢ç­–ç•¥æ¨¡å‹åç¦»å¤ªè¿œ\n",
    "   - **å…³é”®**ï¼šå‚è€ƒæ¨¡å‹å¿…é¡»ä¸ç­–ç•¥æ¨¡å‹åˆå§‹åŒ–ç›¸åŒ\n",
    "\n",
    "3. **æŸå¤±ç±»å‹**ï¼š\n",
    "   - **sigmoid**ï¼šæ ‡å‡†çš„ sigmoid æŸå¤±ï¼ˆæ¨èï¼‰\n",
    "   - **hinge**ï¼šhinge æŸå¤±ï¼ˆåœ¨æŸäº›åœºæ™¯ä¸‹æ›´ç¨³å®šï¼‰\n",
    "\n",
    "### ğŸ’¡ DPO å®è·µå»ºè®®\n",
    "\n",
    "1. **æ•°æ®è´¨é‡**ï¼šåå¥½æ•°æ®è´¨é‡ç›´æ¥å½±å“ DPO æ•ˆæœ\n",
    "2. **beta è°ƒä¼˜**ï¼šæ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ beta å‚æ•°\n",
    "3. **å‚è€ƒæ¨¡å‹**ï¼šç¡®ä¿å‚è€ƒæ¨¡å‹ä¸ç­–ç•¥æ¨¡å‹åˆå§‹åŒ–ä¸€è‡´\n",
    "4. **è¯„ä¼°æŒ‡æ ‡**ï¼šç›‘æ§ chosen/rejected çš„æ¦‚ç‡å·®å¼‚\n",
    "5. **é˜²æ­¢è¿‡æ‹Ÿåˆ**ï¼šä½¿ç”¨éªŒè¯é›†å®šæœŸè¯„ä¼°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b952911c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlhf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
