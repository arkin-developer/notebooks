{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5a11e90",
   "metadata": {},
   "source": [
    "## RLHFÔºàReinforcement Learning from Human FeedbackÔºâ\n",
    "\n",
    "- **ÁõÆÊ†á**ÔºöËÆ©Ê®°ÂûãÊõ¥Á¨¶Âêà‰∫∫Á±ªÊÑèÂõæ„ÄÅÊõ¥ÂÆâÂÖ®„ÄÅÊõ¥ÊúâÁî®\n",
    "- **Ê†∏ÂøÉÊÄùÊÉ≥**Ôºö\n",
    "  - Áî®ÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÊïô‰ºöÊ®°ÂûãÂü∫Êú¨ÁöÑÊåá‰ª§Ë∑üÈöè\n",
    "  - Áî®ÂÅèÂ•ΩÊï∞ÊçÆËÆ≠ÁªÉÂ•ñÂä±Ê®°ÂûãÔºàRMÔºâÔºåÂ≠¶‰ºöÊâìÂàÜ‚ÄúÊõ¥Â•Ω/Êõ¥Â∑Æ‚ÄùÁöÑÂõûÁ≠î\n",
    "  - Áî®Âº∫ÂåñÂ≠¶‰π†ÔºàPPOÔºâÂú®Â•ñÂä±‰ø°Âè∑‰∏ã‰ºòÂåñÁ≠ñÁï•ÔºåÊùÉË°°Ë¥®Èáè„ÄÅÁ®≥ÂÆöÊÄß‰∏éÂ§öÊ†∑ÊÄß\n",
    "- **ÂÖ≥ÈîÆÁªÑ‰ª∂**ÔºöÊåá‰ª§Êï∞ÊçÆ„ÄÅÂÅèÂ•ΩÊï∞ÊçÆÔºàA/B ÂØπÊØîÔºâ„ÄÅÂ•ñÂä±Ê®°Âûã„ÄÅÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ï„ÄÅKL Á∫¶Êùü/ÂèÇËÄÉÁ≠ñÁï•\n",
    "- **ÂÖ∏Âûã‰∫ßÁâ©**Ôºö\n",
    "  - SFT Ê®°ÂûãÔºà‰ºöÂÅö‰∫ãÔºâ\n",
    "  - RM Â•ñÂä±Ê®°ÂûãÔºà‰ºöÊâìÂàÜÔºâ\n",
    "  - PPO ÂêéÁöÑÂØπÈΩêÊ®°ÂûãÔºàÂÅöÂæóÊõ¥Â•ΩÔºâ\n",
    "  - DPOÔºàÂèñ‰ª£ RM+PPO ÁöÑÁõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñÔºâ\n",
    "\n",
    "### RLHF ÁöÑ‰∏âÈò∂ÊÆµÊµÅÁ®ãÔºàÂ∑•Á®ãÂåñËßÜËßíÔºâ\n",
    "\n",
    "| Èò∂ÊÆµ | ÂêçÁß∞ | ‰ΩúÁî® | ÊäÄÊúØ |\n",
    "|---|---|---|---|\n",
    "| 1Ô∏è‚É£ | SFTÔºàÁõëÁù£ÂæÆË∞ÉÔºâ | ÊïôÊ®°ÂûãÊâßË°åÊåá‰ª§ | CrossEntropyLoss |\n",
    "| 2Ô∏è‚É£ | Â•ñÂä±Ê®°ÂûãÔºàRMÔºâËÆ≠ÁªÉ | Â≠¶‰ºö‚Äú‰ªÄ‰πàÊ†∑ÁöÑÂõûÁ≠îÊõ¥Â•Ω‚Äù | Pairwise ranking (A > B) |\n",
    "| 3Ô∏è‚É£ | PPO Âº∫Âåñ‰ºòÂåñ | Áî®Â•ñÂä±‰ø°Âè∑‰ºòÂåñÁîüÊàêÁ≠ñÁï• | PPO ÁÆóÊ≥ïÔºàPolicy GradientÔºâ |\n",
    "\n",
    "### ÂÆûÈ™åËÆæÁΩÆÔºöÊ®°Âûã‰∏éÊï∞ÊçÆÈõÜÈÄâÊã©\n",
    "- **Ê®°Âûã**ÔºöQwen2.5-1.5B-InstructÔºà‰∏≠ÊñáÊåá‰ª§ËÉΩÂäõÂº∫ÔºåÂ∞èÂèÇÊï∞„ÄÅÊòì‰∫é LoRA/QLoRAÔºâ\n",
    "- **SFT Êï∞ÊçÆ**ÔºöBelleGroup/train_0.5M_CNÔºà‰∏≠ÊñáÊåá‰ª§-ÂõûÁ≠îÂØπÔºå‰ΩìÈáèÈÄÇ‰∏≠ÔºåÂèØÈááÊ†∑Ôºâ\n",
    "- **ÂÅèÂ•ΩÊï∞ÊçÆÔºàÁî®‰∫é DPO/RMÔºâ**Ôºöargilla/ultrafeedback-binarized-preferencesÔºàÊàêÂØπÂÅèÂ•ΩÔºåÊòìÁõ¥Êé•Áî®‰∫é DPOÔºâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3104c69a",
   "metadata": {},
   "source": [
    "## ÁéØÂ¢ÉÂÆâË£Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b3c68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÂÆâË£ÖÈúÄË¶ÅÁöÑÂ∫ì\n",
    "%env TOKENIZERS_PARALLELISM=false\n",
    "%pip install -q torch torchvision torchaudio\n",
    "%pip install -q \"transformers>=4.44.0\" \"datasets>=2.18.0\" \"accelerate>=0.33.0\" \\\n",
    "                \"peft>=0.12.0\" \"trl>=0.9.6\" \"sentencepiece>=0.1.99\" \"safetensors>=0.4.5\" \\\n",
    "                \"huggingface_hub>=0.24.0\" \"modelscope>=1.14.0\" \"protobuf>=4.25.0\" \\\n",
    "                \"numpy>=1.24.0\" \"scipy>=1.10.0\" \"tiktoken>=0.7.0\" \n",
    "\n",
    "# ‰ªÖÂú® CUDA ÂèØÁî®Êó∂ÂÆâË£Ö bitsandbytesÔºàÂèØÈÄâÔºâ\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    %pip install -q \"bitsandbytes>=0.43.0\"\n",
    "\n",
    "# ÊâìÂç∞ÂÖ≥ÈîÆÁâàÊú¨Ôºå‰æø‰∫éÊéíÊü•\n",
    "import importlib.metadata as im\n",
    "v = lambda n: (im.version(n) if n in {d.metadata['Name'] for d in im.distributions()} else 'N/A')\n",
    "print(\"[Versions]\",\n",
    "      \"torch=\", v(\"torch\"),\n",
    "      \"transformers=\", v(\"transformers\"),\n",
    "      \"datasets=\", v(\"datasets\"),\n",
    "      \"accelerate=\", v(\"accelerate\"),\n",
    "      \"peft=\", v(\"peft\"),\n",
    "      \"trl=\", v(\"trl\"),\n",
    "      \"modelscope=\", v(\"modelscope\"),\n",
    "      \"sentencepiece=\", v(\"sentencepiece\"),\n",
    "      \"bitsandbytes=\", v(\"bitsandbytes\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a578b03c",
   "metadata": {},
   "source": [
    "## Ê®°Âûã‰∏ãËΩΩ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a203c0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‰∏ãËΩΩQwen/Qwen2.5-1.5B-Instruct\n",
    "import torch\n",
    "from modelscope.hub.snapshot_download import snapshot_download\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"  # ModelScope ‰∏äÁöÑÊ®°ÂûãÊ†áËØÜÔºàÂÖ¨ÂÖ±ÂèØÁõ¥Êé•‰∏ãËΩΩÔºâ\n",
    "\n",
    "# ÈÄâÊã©ËÆæÂ§á\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# ÈÄöËøá ModelScope ‰∏ãËΩΩÂà∞Êú¨Âú∞ÁºìÂ≠òÔºåÁÑ∂ÂêéÁî® Transformers ‰ªéÊú¨Âú∞ÁõÆÂΩïÂä†ËΩΩ\n",
    "model_dir = snapshot_download(model_id)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir, trust_remote_code=True)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"[Device] {device}\")\n",
    "\n",
    "# ÁÆÄÂçïËá™Ê£Ä\n",
    "txt = \"‰Ω†Â•ΩÔºåÁÆÄË¶Å‰ªãÁªç‰∏Ä‰∏ã‰Ω†Ëá™Â∑±„ÄÇ\"\n",
    "inputs = tokenizer(txt, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "    out = model.generate(**inputs, max_new_tokens=64, do_sample=False)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8784880",
   "metadata": {},
   "source": [
    "## Êï∞ÊçÆÈõÜ‰∏ãËΩΩ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb0edcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‰∏ãËΩΩsftÊï∞ÊçÆÈõÜÂíåÂÅèÂ•ΩÊï∞ÊçÆÈõÜ\n",
    "from modelscope.hub.snapshot_download import snapshot_download\n",
    "\n",
    "# ÊåáÂÆö‰∏§‰∏™Êï∞ÊçÆÈõÜÂêçÁß∞ÔºàÂèØÊåâÈúÄ‰øÆÊîπÔºâ\n",
    "sft_id = \"AI-ModelScope/train_0.5M_CN\"  # ‰∏≠ÊñáÊï∞ÊçÆÈõÜ\n",
    "pref_id_primary = \"HuggingFaceH4/ultrafeedback_binarized\" # Â§öËØ≠ÔºåËã±Êñá‰∏∫‰∏ªÔºåÂèØÁ≠õÂá∫‰∏≠ÊñáÂ≠êÈõÜ\n",
    " \n",
    "# ‰ªÖ‰ΩøÁî® ModelScope ‰∏ãËΩΩÂà∞Êú¨Âú∞ÁºìÂ≠òÔºà‰∏çÂÅöÂõûÈÄÄÔºâ\n",
    "sft_dir = snapshot_download(sft_id, repo_type=\"dataset\")\n",
    "pref_dir = snapshot_download(pref_id_primary, repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6915e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# È¢ÑËßà SFT ÂíåÂÅèÂ•ΩÊï∞ÊçÆÈõÜ\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd, os, json, glob\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "# ========== ÈÄöÁî®Â∑•ÂÖ∑ÂáΩÊï∞ ==========\n",
    "def normalize_text(x):\n",
    "    \"\"\"Â∞ÜÂµåÂ•óÁªìÊûÑÊ†áÂáÜÂåñ‰∏∫ÂèØËØªÂ≠óÁ¨¶‰∏≤\"\"\"\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, str):\n",
    "        return x\n",
    "    if isinstance(x, dict):\n",
    "        return x.get(\"content\") or x.get(\"text\") or x.get(\"value\") or str(x)\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        parts = []\n",
    "        for it in x:\n",
    "            if isinstance(it, str):\n",
    "                parts.append(it)\n",
    "            elif isinstance(it, dict):\n",
    "                role = it.get(\"role\")\n",
    "                content = it.get(\"content\") or it.get(\"text\") or it.get(\"value\")\n",
    "                if content:\n",
    "                    parts.append((f\"{role}: \" if role else \"\") + str(content))\n",
    "        return \"\\n---\\n\".join(parts)\n",
    "    return str(x)\n",
    "# ========== SFT Êï∞ÊçÆÂä†ËΩΩ ==========\n",
    "print(\"üìò Âä†ËΩΩÂπ∂È¢ÑËßà SFT Êï∞ÊçÆ\")\n",
    "sft_preview = load_dataset(sft_dir, split=\"train[:2]\")\n",
    "sft_rows = []\n",
    "for ex in sft_preview:\n",
    "    sft_rows.append({\n",
    "        \"instruction\": normalize_text(ex.get(\"instruction\")),\n",
    "        \"input\": normalize_text(ex.get(\"input\")),\n",
    "        \"output\": normalize_text(ex.get(\"output\")),\n",
    "        \"instr_len\": len(str(ex.get(\"instruction\", \"\"))),\n",
    "        \"input_len\": len(str(ex.get(\"input\", \"\"))),\n",
    "        \"output_len\": len(str(ex.get(\"output\", \"\"))),\n",
    "    })\n",
    "print(f\"[SFT] È¢ÑËßà {len(sft_rows)} Êù° / split=train[:2]\")\n",
    "display(pd.DataFrame(sft_rows))\n",
    "\n",
    "# ========== ÂÅèÂ•ΩÊï∞ÊçÆÂä†ËΩΩÔºàPrimaryÔºâ ==========\n",
    "def load_pref_data(ds_dir, name=\"Primary\"):\n",
    "    \"\"\"Â∞ùËØïÂ§öÁßçÊñπÂºèÂä†ËΩΩÂÅèÂ•ΩÊï∞ÊçÆ\"\"\"\n",
    "    for split in [\"train_prefs[:2]\", \"train[:2]\"]:\n",
    "        try:\n",
    "            ds = load_dataset(ds_dir, split=split)\n",
    "            return ds, f\"{name} ({split.split('[')[0]})\"\n",
    "        except:\n",
    "            continue\n",
    "    # Â∞ùËØï‰ªé data ÁõÆÂΩïÂä†ËΩΩ\n",
    "    data_dir = os.path.join(ds_dir, \"data\")\n",
    "    if not os.path.exists(data_dir):\n",
    "        return None, None\n",
    "    parquet = glob.glob(os.path.join(data_dir, \"*.parquet\"))\n",
    "    if parquet:\n",
    "        ds = Dataset.from_parquet(parquet[0]).select(range(2))\n",
    "        return ds, f\"{name} (Parquet)\"\n",
    "    jsonl = glob.glob(os.path.join(data_dir, \"*.jsonl\"))\n",
    "    if jsonl:\n",
    "        data = []\n",
    "        with open(jsonl[0], \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= 2: break\n",
    "                try: data.append(json.loads(line))\n",
    "                except: pass\n",
    "        return Dataset.from_list(data), f\"{name} (JSONL)\" if data else (None, None)\n",
    "    return None, None\n",
    "\n",
    "def format_pref_data(pref_ds, name=\"Primary\"):\n",
    "    if pref_ds is None:\n",
    "        print(f\"[Preference {name}] ‚ùå Êó†Ê≥ïÂä†ËΩΩ\")\n",
    "        return\n",
    "    rows = []\n",
    "    for ex in pref_ds:\n",
    "        row = {\n",
    "            \"prompt\": normalize_text(ex.get(\"prompt\") or ex.get(\"instruction\") or ex.get(\"input\")),\n",
    "            \"y_pos\": normalize_text(ex.get(\"chosen\") or ex.get(\"better_response\") or ex.get(\"pos\")),\n",
    "            \"y_neg\": normalize_text(ex.get(\"rejected\") or ex.get(\"worse_response\") or ex.get(\"neg\")),\n",
    "        }\n",
    "        row[\"prompt_len\"] = len(str(row[\"prompt\"] or \"\"))\n",
    "        row[\"y_pos_len\"] = len(str(row[\"y_pos\"] or \"\"))\n",
    "        row[\"y_neg_len\"] = len(str(row[\"y_neg\"] or \"\"))\n",
    "        rows.append(row)\n",
    "    print(f\"[Preference {name}] È¢ÑËßà {len(rows)} Êù°\")\n",
    "    display(pd.DataFrame(rows))\n",
    "\n",
    "print(\"üìó Âä†ËΩΩÂπ∂È¢ÑËßà Preference Êï∞ÊçÆ\")\n",
    "pref_preview, pref_source = load_pref_data(pref_dir, \"Primary\")\n",
    "format_pref_data(pref_preview, \"Primary\")\n",
    "\n",
    "print(f\"\\n[Preference Primary] Êï∞ÊçÆÈõÜ: {pref_dir.split('/')[-1]}\")\n",
    "if pref_preview and len(pref_preview) > 0:\n",
    "    fields = list(pref_preview[0].keys())\n",
    "    has_chosen = any(k in fields for k in [\"chosen\", \"better_response\"])\n",
    "    has_rejected = any(k in fields for k in [\"rejected\", \"worse_response\"])\n",
    "    print(f\"  - Â≠óÊÆµ: {fields}\")\n",
    "    print(f\"  - ÊîØÊåÅ: {'‚úÖ RM/DPO' if has_chosen and has_rejected else '‚ö†Ô∏è  ÈÉ®ÂàÜÊîØÊåÅ'}\")\n",
    "else:\n",
    "    print(\"  - Áä∂ÊÄÅ: ‚ùå Êú™Âä†ËΩΩ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c63d2f",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ SFTÔºàÁõëÁù£ÂæÆË∞ÉÔºâ\n",
    "- **ËæìÂÖ•**ÔºöÊåá‰ª§-ÂõûÁ≠îÂØπÔºàÈ´òË¥®Èáè„ÄÅ‰∫∫Á±ª‰π¶ÂÜô/Á≠õÈÄâÔºâ\n",
    "- **ÁõÆÊ†á**ÔºöËÆ©Ê®°ÂûãÂü∫Êú¨Â≠¶‰ºö‚ÄúÊåâÊåá‰ª§‰ΩúÁ≠î‚Äù\n",
    "- **ËÆ≠ÁªÉ**ÔºöÊúÄÂ∞èÂåñ‰∫§ÂèâÁÜµÊçüÂ§±ÔºàÂèÇËÄÉÂ∏∏Áî®Êåá‰ª§Êï∞ÊçÆÈõÜÔºâ\n",
    "- **ËæìÂá∫**ÔºöSFT Ê®°ÂûãÔºà‰Ωú‰∏∫ÂêéÁª≠ RM/PPO ÁöÑÂèÇËÄÉÁ≠ñÁï•Ôºâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd883faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# „ÄêÂä†ËΩΩsftÊï∞ÊçÆÈõÜ„Äë dataset\n",
    "from datasets import Dataset\n",
    "import os\n",
    "import json\n",
    "\n",
    "sft_path = os.path.expanduser(sft_dir)\n",
    "\n",
    "# Ê£ÄÊü•ÁõÆÂΩïÂÜÖÂÆπ\n",
    "print(\"[Êñá‰ª∂ÂàóË°®]\", os.listdir(sft_path))\n",
    "\n",
    "# Â∞ùËØïÊâæÂà∞ JSON Êàñ JSONL Êñá‰ª∂\n",
    "json_files = [f for f in os.listdir(sft_path) if f.endswith(\".json\") or f.endswith(\".jsonl\")]\n",
    "if not json_files:\n",
    "    raise RuntimeError(f\"Âú® {sft_path} Êú™ÊâæÂà∞ JSON/JSONL Êñá‰ª∂ÔºåËØ∑ÊâãÂä®Êü•ÁúãÊñá‰ª∂ÁªìÊûÑ„ÄÇ\")\n",
    "\n",
    "json_path = os.path.join(sft_path, json_files[0])\n",
    "print(f\"[Âä†ËΩΩÊñá‰ª∂] {json_path}\")\n",
    "\n",
    "# Âä†ËΩΩ JSON Êï∞ÊçÆ\n",
    "data = []\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            data.append(json.loads(line))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# ËΩ¨Êç¢Êàê HuggingFace Dataset\n",
    "sft_ds = Dataset.from_list(data)\n",
    "print(f\"[SFT] ÊàêÂäüÂä†ËΩΩ {len(sft_ds)} Êù°Ê†∑Êú¨„ÄÇ\")\n",
    "print(\"[Á§∫‰æãÊ†∑Êú¨]\", sft_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026ca34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# „ÄêÊï∞ÊçÆÈõÜÂàáÂàÜ„ÄëÊåâÊØî‰æãÂàíÂàÜÔºåÊØîÂ¶Ç 98% ËÆ≠ÁªÉ„ÄÅ2% È™åËØÅ\n",
    "sft_split = sft_ds.train_test_split(test_size=0.02, seed=42)\n",
    "\n",
    "# ÈáçÊûÑÊï∞ÊçÆÊ†ºÂºèÔºöÊûÑÈÄ† prompt-completion ÁªìÊûÑÔºàÁ¨¶Âêà TRL 0.24 Ê†áÂáÜÔºâ\n",
    "def _to_sft(example):\n",
    "    instr = example.get(\"instruction\", \"\")\n",
    "    inp = example.get(\"input\", \"\")\n",
    "    output = example.get(\"output\", None)\n",
    "    # ‰ΩøÁî®Á©∫Ê†ºÁªìÂ∞æÔºåÈÅøÂÖçÂàÜËØçËæπÁïåÈóÆÈ¢ò\n",
    "    prompt = (instr + (\"\\n\" + inp if inp else \"\")).strip() + \" \"\n",
    "    return {\"prompt\": prompt, \"completion\": output}\n",
    "\n",
    "# ÂàáÂàÜÂπ∂Ê†ºÂºèÂåñ\n",
    "train_ds = sft_split[\"train\"].map(_to_sft, remove_columns=sft_split[\"train\"].column_names)\n",
    "eval_ds = sft_split[\"test\"].map(_to_sft, remove_columns=sft_split[\"test\"].column_names)\n",
    "\n",
    "print(\"ËÆ≠ÁªÉÈõÜÊ†∑Êú¨Êï∞:\", len(train_ds))\n",
    "print(\"È™åËØÅÈõÜÊ†∑Êú¨Êï∞:\", len(eval_ds))\n",
    "print(\"[Á§∫‰æã]\", train_ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9549d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# „ÄêLoRA ÈÖçÁΩÆ‰∏éËÆ≠ÁªÉÂèÇÊï∞„Äë\n",
    "from peft import LoraConfig\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Ê®°ÂûãËæìÂÖ•ËæìÂá∫ÈÖçÁΩÆ\n",
    "base_model_or_dir = model_dir  # Â§çÁî® ModelScope ‰∏ãËΩΩÁöÑÊ®°ÂûãÁõÆÂΩï\n",
    "output_dir = \"outputs/sft_qlora\"\n",
    "max_seq_length = 2048\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_mps = torch.backends.mps.is_available()\n",
    "\n",
    "# bitsandbytes QLoRA ÈÖçÁΩÆ \n",
    "quantization_config = None\n",
    "if use_cuda:\n",
    "    try:\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "        print(\"[QLoRA] ‰ΩøÁî® bitsandbytes 4-bit ÈáèÂåñÂä†ËΩΩÊ®°Âûã\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Warn] Êú™ÂêØÁî®4bitÈáèÂåñÔºö{e}\")\n",
    "else:\n",
    "    print(\"[Info] ÂΩìÂâç‰∏∫Èùû CUDA ÁéØÂ¢ÉÔºå‰ΩøÁî®Â∏∏ËßÑÁ≤æÂ∫¶Âä†ËΩΩ\")\n",
    "\n",
    "# Tokenizer ÂàùÂßãÂåñ\n",
    "if tokenizer is None:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_or_dir, use_fast=True, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528cc6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA ÈÖçÁΩÆÔºö‰ΩéÁß©ÈÄÇÈÖçÂô®ÂèÇÊï∞\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                    # Áß©ÔºàrankÔºâÔºåÊéßÂà∂ÈÄÇÈÖçÂô®Áü©ÈòµÁöÑÁª¥Â∫¶ÔºåË∂äÂ§ßÂÆπÈáèË∂äÂ§ß‰ΩÜÂèÇÊï∞ÈáèÊõ¥Â§öÔºàÂ∏∏Áî® 8/16/32Ôºâ\n",
    "    lora_alpha=32,          # Áº©ÊîæÁ≥ªÊï∞Ôºå‰∏é r ÊàêÊØî‰æã‰ΩøÁî®ÔºàÈÄöÂ∏∏ alpha=2*rÔºâ\n",
    "    lora_dropout=0.05,      # ÈÄÇÈÖçÂô®Â±Ç dropoutÔºàÈò≤Ê≠¢ËøáÊãüÂêàÔºåÂ∏∏Áî® 0.05-0.1Ôºâ\n",
    "    bias=\"none\",            # ÊòØÂê¶ËÆ≠ÁªÉ biasÔºà\"none\"/\"all\"/\"lora_only\"Ôºâ\n",
    "    task_type=\"CAUSAL_LM\",   # ‰ªªÂä°Á±ªÂûãÔºöÂõ†ÊûúËØ≠Ë®ÄÊ®°ÂûãÔºàÁîüÊàê‰ªªÂä°Ôºâ\n",
    ")\n",
    "\n",
    "# ËÆ≠ÁªÉÂèÇÊï∞ÔºöÊéßÂà∂ËÆ≠ÁªÉÊµÅÁ®ã‰∏é‰ºòÂåñ\n",
    "args = TrainingArguments(\n",
    "    # ËæìÂá∫‰∏é‰øùÂ≠ò\n",
    "    output_dir=output_dir,              # Ê®°Âûã/Êó•ÂøóËæìÂá∫ÁõÆÂΩï\n",
    "    save_steps=200,                     # ÊØè N Ê≠•‰øùÂ≠ò‰∏ÄÊ¨° checkpoint\n",
    "    save_total_limit=2,                 # ÊúÄÂ§ö‰øùÁïô N ‰∏™ checkpointÔºàÈÅøÂÖçÂç†Á£ÅÁõòÔºâ\n",
    "    save_safetensors=True,              # ‰ΩøÁî® safetensors Ê†ºÂºè‰øùÂ≠òÔºàÊõ¥Âø´Êõ¥ÂÆâÂÖ®Ôºâ\n",
    "    \n",
    "    # ÊâπÂ§ßÂ∞è‰∏éÊ¢ØÂ∫¶\n",
    "    per_device_train_batch_size=1,      # ÊØèËÆæÂ§áËÆ≠ÁªÉ batch Â§ßÂ∞èÔºàÊòæÂ≠òÂèóÈôêÊó∂ÂÖàÁî® 1Ôºâ\n",
    "    per_device_eval_batch_size=1,       # ÊØèËÆæÂ§áÈ™åËØÅ batch Â§ßÂ∞è\n",
    "    gradient_accumulation_steps=8,      # Ê¢ØÂ∫¶Á¥ØÁßØÊ≠•Êï∞ÔºàÁ≠âÊïà batch = 1 * 8 = 8Ôºâ\n",
    "    \n",
    "    # ËÆ≠ÁªÉËΩÆÊ¨°‰∏éÂ≠¶‰π†Áéá\n",
    "    num_train_epochs=1,                 # ËÆ≠ÁªÉËΩÆÊ¨°Êï∞\n",
    "    learning_rate=2e-4,                 # ÂàùÂßãÂ≠¶‰π†ÁéáÔºàLoRA Â∏∏Áî® 1e-4 Âà∞ 5e-4Ôºâ\n",
    "    lr_scheduler_type=\"cosine\",         # Â≠¶‰π†ÁéáË∞ÉÂ∫¶Âô®Ôºà‰ΩôÂº¶ÈÄÄÁÅ´Ôºâ\n",
    "    warmup_ratio=0.03,                  # warmup ÊØî‰æãÔºàÂâç 3% Ê≠•Êï∞Á∫øÊÄßÂ¢ûÈïø LRÔºâ\n",
    "    \n",
    "    # ËØÑ‰º∞‰∏éÊó•Âøó\n",
    "    eval_strategy=\"steps\",              # ËØÑ‰º∞Á≠ñÁï•Ôºà\"steps\"/\"epoch\"/\"no\"Ôºâ\n",
    "    eval_steps=100,                     # ÊØè N Ê≠•ËØÑ‰º∞‰∏ÄÊ¨°\n",
    "    logging_steps=10,                   # ÊØè N Ê≠•ÊâìÂç∞‰∏ÄÊ¨°Êó•Âøó\n",
    "    report_to=[\"none\"],                 # ‰∏çÂêëÂ§ñÈÉ®‰∏äÊä•ÔºàÂèØÊîπ‰∏∫ [\"wandb\"] Á≠âÔºâ\n",
    "    \n",
    "    # Ê®°ÂûãÊ£ÄÊü•ÁÇπ\n",
    "    load_best_model_at_end=True,        # ËÆ≠ÁªÉÁªìÊùüÂä†ËΩΩÊúÄ‰Ω≥Ê®°Âûã\n",
    "    metric_for_best_model=\"eval_loss\",  # ÊúÄ‰Ω≥Ê®°ÂûãÊåáÊ†á\n",
    "    greater_is_better=False,            # ËØ•ÊåáÊ†áË∂äÂ∞èË∂äÂ•Ω\n",
    "    \n",
    "    # ÊÄßËÉΩ‰ºòÂåñ\n",
    "    bf16=use_cuda,                      # CUDA ‰∏äÁî® bfloat16ÔºàÁÆóÂäõ‰∏éÁ®≥ÂÆöÊÄßÂπ≥Ë°°Ôºâ\n",
    "    fp16=False,                         # Á¶ÅÁî® FP16ÔºàÈÅøÂÖçÂÜ≤Á™ÅÔºâ\n",
    "    gradient_checkpointing=True,        # Ê¢ØÂ∫¶Ê£ÄÊü•ÁÇπÔºàÁâ∫Áâ≤Êó∂Èó¥Êç¢ÊòæÂ≠òÔºâ\n",
    ")\n",
    "\n",
    "print(\"ËÆ≠ÁªÉÂèÇÊï∞ÈÖçÁΩÆÂÆåÊàê\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5244aa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÂÆö‰πâÊ†ºÂºèÂåñÂáΩÊï∞ÔºöÊûÑÈÄ†ËÆ≠ÁªÉÊñáÊú¨Ê®°Êùø\n",
    "def formatting_func(example):\n",
    "    \"\"\"Â∞Ü prompt-response ÂØπËΩ¨Êç¢‰∏∫Ê®°ÂûãËæìÂÖ•ÊñáÊú¨ÔºàÂçïÊù°Â§ÑÁêÜÔºâ\"\"\"\n",
    "    prompt = str(example[\"prompt\"]).strip()\n",
    "    resp = str(example[\"response\"]).strip()\n",
    "    # ÊãºÊé•Ê†ºÂºèÔºöÁ¨¶Âêà Qwen ÁöÑÊ®°Êùø\n",
    "    text = f\"<|user|>\\n{prompt}\\n<|assistant|>\\n{resp}{tokenizer.eos_token}\"\n",
    "    return text\n",
    "\n",
    "print(\"[Ê†ºÂºèÂåñÂáΩÊï∞Â∑≤ÂÆö‰πâ]\")\n",
    "print(\"[Á§∫‰æã]\", formatting_func({\"prompt\": \"‰Ω†Â•Ω\", \"response\": \"‰Ω†Â•ΩÔºÅÊúâ‰ªÄ‰πàÂèØ‰ª•Â∏ÆÂä©ÊÇ®ÁöÑÂêóÔºü\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f75cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ËÆ≠ÁªÉÂô®ÁöÑÊûÑÈÄ†\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Âä†ËΩΩÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºàQLoRA Ê®°Âºè‰∏ãËá™Âä®Â∫îÁî®ÈáèÂåñÔºâ\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_or_dir,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\" if use_cuda else None,\n",
    ")\n",
    "\n",
    "# ÊûÑÈÄ† SFT ËÆ≠ÁªÉÂô®\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    peft_config=lora_config,\n",
    "    args=args,\n",
    "    # ‰∏ç‰ΩøÁî® formatting_funcÔºåÁõ¥Êé•Áî® prompt-completion ÁªìÊûÑ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eba8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÂºÄÂßãËÆ≠ÁªÉ\n",
    "trainer.train()\n",
    "\n",
    "# === ‰øùÂ≠òÈÄÇÈÖçÂô®ÔºàLoRA ÊùÉÈáçÔºâ===\n",
    "adapter_dir = os.path.join(output_dir, \"adapter\")\n",
    "trainer.model.save_pretrained(adapter_dir)\n",
    "print(f\"[Done] SFT + QLoRA ËÆ≠ÁªÉÂÆåÊàêÔºåLoRA ÊùÉÈáçÂ∑≤‰øùÂ≠òËá≥: {adapter_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcc3230",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Â•ñÂä±Ê®°ÂûãÔºàRMÔºâËÆ≠ÁªÉ\n",
    "- **ËæìÂÖ•**ÔºöÂêå‰∏ÄÊåá‰ª§‰∏ãÊàêÂØπÂõûÁ≠îÔºàA„ÄÅBÔºâÔºå‰ª•ÂèäÂÅèÂ•ΩÊ†áÁ≠æÔºàA > BÔºâ\n",
    "- **ÁõÆÊ†á**ÔºöÂ≠¶‰π†‚ÄúÂÅèÂ•ΩËØÑÂàÜÂáΩÊï∞‚Äù r(x, y)\n",
    "- **ËÆ≠ÁªÉ**ÔºöPairwise rankingÔºàÂ¶Ç Bradley‚ÄìTerry/Logistic lossÔºâ\n",
    "- **ËæìÂá∫**ÔºöËÉΩÂØπ‰ªªÊÑèÂõûÁ≠îÊâìÂàÜÁöÑÂ•ñÂä±Ê®°Âûã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e6eb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Âä†ËΩΩÂÅèÂ•ΩÊï∞ÊçÆÈõÜ\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "if 'pref_dir' in globals():\n",
    "    for split in [\"train_prefs\", \"train\"]:\n",
    "        try:\n",
    "            pref_ds = load_dataset(pref_dir, split=split)\n",
    "            print(f\"[RM] ‰ªéÊú¨Âú∞ÁõÆÂΩïÂä†ËΩΩÊàêÂäüÔºà{split}ÔºâÔºåÂÖ± {len(pref_ds)} Êù°Ê†∑Êú¨\")\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "    # Â¶ÇÊûúËøòÊòØÂ§±Ë¥•ÔºåÂ∞ùËØï‰ªé data ÁõÆÂΩïÂä†ËΩΩ Parquet/JSONL\n",
    "    if pref_ds is None:\n",
    "        data_dir = os.path.join(pref_dir, \"data\") if os.path.exists(os.path.join(pref_dir, \"data\")) else pref_dir\n",
    "        parquet_files = glob.glob(os.path.join(data_dir, \"*.parquet\"))\n",
    "        if parquet_files:\n",
    "            pref_ds = Dataset.from_parquet(parquet_files[0])\n",
    "            print(f\"[RM] ‰ªé Parquet Âä†ËΩΩÊàêÂäüÔºåÂÖ± {len(pref_ds)} Êù°Ê†∑Êú¨\")\n",
    "        else:\n",
    "            jsonl_files = glob.glob(os.path.join(data_dir, \"*.jsonl\"))\n",
    "            if jsonl_files:\n",
    "                data = []\n",
    "                with open(jsonl_files[0], \"r\", encoding=\"utf-8\") as f:\n",
    "                    for line in f:\n",
    "                        try:\n",
    "                            data.append(json.loads(line))\n",
    "                        except:\n",
    "                            pass\n",
    "                pref_ds = Dataset.from_list(data)\n",
    "                print(f\"[RM] ‰ªé JSONL Âä†ËΩΩÊàêÂäüÔºåÂÖ± {len(pref_ds)} Êù°Ê†∑Êú¨\")\n",
    "if pref_ds is None:\n",
    "    raise RuntimeError(\"Êó†Ê≥ïÂä†ËΩΩÂÅèÂ•ΩÊï∞ÊçÆÈõÜÔºåËØ∑Ê£ÄÊü•Êï∞ÊçÆË∑ØÂæÑ\")\n",
    "\n",
    "# È¢ÑËßàÊï∞ÊçÆÁªìÊûÑ\n",
    "print(f\"\\n[RM] Êï∞ÊçÆÈõÜÂ≠óÊÆµÔºö{pref_ds[0].keys()}\")\n",
    "print(f\"[RM] Á§∫‰æãÊ†∑Êú¨Ôºö\")\n",
    "example = pref_ds[0]\n",
    "for k, v in example.items():\n",
    "    if isinstance(v, str) and len(v) > 100:\n",
    "        print(f\"  {k}: {v[:100]}...\")\n",
    "    else:\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66ae9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ËΩ¨Êç¢‰∏∫ RM ËÆ≠ÁªÉÊ†ºÂºèÔºàprompt, chosen, rejectedÔºâ\n",
    "def normalize_text(x):\n",
    "    \"\"\"Â∞ÜÂµåÂ•óÁªìÊûÑÊ†áÂáÜÂåñ‰∏∫ÂèØËØªÂ≠óÁ¨¶‰∏≤\"\"\"\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, str):\n",
    "        return x\n",
    "    if isinstance(x, dict):\n",
    "        return x.get(\"content\") or x.get(\"text\") or x.get(\"value\") or str(x)\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        parts = []\n",
    "        for it in x:\n",
    "            if isinstance(it, str):\n",
    "                parts.append(it)\n",
    "            elif isinstance(it, dict):\n",
    "                role = it.get(\"role\")\n",
    "                content = it.get(\"content\") or it.get(\"text\") or it.get(\"value\")\n",
    "                if content:\n",
    "                    parts.append((f\"{role}: \" if role else \"\") + str(content))\n",
    "        return \"\\n---\\n\".join(parts)\n",
    "    return str(x)\n",
    "\n",
    "def _to_rm_format(example):\n",
    "    \"\"\"Â∞ÜÂÅèÂ•ΩÊï∞ÊçÆËΩ¨Êç¢‰∏∫ RM ËÆ≠ÁªÉÊ†ºÂºèÔºàprompt, chosen, rejectedÔºâ\"\"\"\n",
    "    # Â∞ùËØïÂ§öÁßçÂ≠óÊÆµÂêç\n",
    "    prompt = normalize_text(\n",
    "        example.get(\"prompt\") or \n",
    "        example.get(\"instruction\") or \n",
    "        example.get(\"input\") or\n",
    "        example.get(\"messages\")\n",
    "    )\n",
    "    \n",
    "    chosen = normalize_text(\n",
    "        example.get(\"chosen\") or \n",
    "        example.get(\"better_response\") or \n",
    "        example.get(\"positive\") or\n",
    "        example.get(\"response_j\")\n",
    "    )\n",
    "    \n",
    "    rejected = normalize_text(\n",
    "        example.get(\"rejected\") or \n",
    "        example.get(\"worse_response\") or \n",
    "        example.get(\"negative\") or\n",
    "        example.get(\"response_k\")\n",
    "    )\n",
    "    \n",
    "    # Â§ÑÁêÜ messages Ê†ºÂºèÔºàÂØπËØùÊ†ºÂºèÔºâ\n",
    "    if prompt is None and isinstance(example.get(\"messages\"), list):\n",
    "        msgs = example.get(\"messages\", [])\n",
    "        # ÊèêÂèñÊúÄÂêé‰∏Ä‰∏™ user Ê∂àÊÅØ‰Ωú‰∏∫ promptÔºå‰πãÂâçÁöÑ‰Ωú‰∏∫ context\n",
    "        user_msgs = [m.get(\"content\", \"\") for m in msgs if m.get(\"role\") == \"user\"]\n",
    "        if user_msgs:\n",
    "            prompt = user_msgs[-1]\n",
    "    \n",
    "    # Â§ÑÁêÜ chosen/rejected Â¶ÇÊûúÊòØÂàóË°®Ê†ºÂºè\n",
    "    if isinstance(chosen, list):\n",
    "        chosen = \"\\n---\\n\".join([normalize_text(c) for c in chosen])\n",
    "    if isinstance(rejected, list):\n",
    "        rejected = \"\\n---\\n\".join([normalize_text(r) for r in rejected])\n",
    "    \n",
    "    # Á°Æ‰øù prompt, chosen, rejected ÈÉΩ‰∏ç‰∏∫Á©∫\n",
    "    if prompt is None or chosen is None or rejected is None:\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": str(prompt).strip(),\n",
    "        \"chosen\": str(chosen).strip(),\n",
    "        \"rejected\": str(rejected).strip(),\n",
    "    }\n",
    "\n",
    "# ËΩ¨Êç¢Êï∞ÊçÆÊ†ºÂºè\n",
    "print(\"\\n[RM] ÂºÄÂßãËΩ¨Êç¢Êï∞ÊçÆÊ†ºÂºè...\")\n",
    "rm_ds = pref_ds.map(\n",
    "    _to_rm_format,\n",
    "    remove_columns=pref_ds.column_names,\n",
    "    desc=\"ËΩ¨Êç¢‰∏∫ RM Ê†ºÂºè\"\n",
    ")\n",
    "\n",
    "# ËøáÊª§Êéâ None ÂÄºÔºàÊ†ºÂºèËΩ¨Êç¢Â§±Ë¥•ÁöÑÊï∞ÊçÆÔºâ\n",
    "rm_ds = rm_ds.filter(lambda x: x[\"prompt\"] is not None and x[\"chosen\"] is not None and x[\"rejected\"] is not None)\n",
    "\n",
    "print(f\"[RM] Ê†ºÂºèËΩ¨Êç¢ÂÆåÊàêÔºåÊúâÊïàÊ†∑Êú¨Êï∞Ôºö{len(rm_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73255c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Êï∞ÊçÆÂàáÂàÜÔºö98% ËÆ≠ÁªÉÔºå2% È™åËØÅ\n",
    "rm_split = rm_ds.train_test_split(test_size=0.02, seed=42)\n",
    "rm_train_ds = rm_split[\"train\"]\n",
    "rm_eval_ds = rm_split[\"test\"]\n",
    "\n",
    "print(f\"\\n[RM] Êï∞ÊçÆÂàáÂàÜÂÆåÊàêÔºö\")\n",
    "print(f\"  ËÆ≠ÁªÉÈõÜÔºö{len(rm_train_ds)} Êù°\")\n",
    "print(f\"  È™åËØÅÈõÜÔºö{len(rm_eval_ds)} Êù°\")\n",
    "\n",
    "# È¢ÑËßàËÆ≠ÁªÉÈõÜÊ†∑Êú¨\n",
    "print(f\"\\n[RM] ËÆ≠ÁªÉÈõÜÁ§∫‰æãÔºö\")\n",
    "sample = rm_train_ds[0]\n",
    "for k, v in sample.items():\n",
    "    if isinstance(v, str) and len(v) > 150:\n",
    "        print(f\"  {k}: {v[:150]}...\")\n",
    "    else:\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58be0c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# „ÄêRM Ê®°ÂûãÂä†ËΩΩ‰∏éÈÖçÁΩÆ„Äë\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from trl import RewardTrainer, RewardConfig\n",
    "import torch\n",
    "\n",
    "# Ê®°ÂûãËæìÂÖ•ËæìÂá∫ÈÖçÁΩÆ\n",
    "rm_output_dir = \"outputs/rm_qlora\"\n",
    "max_seq_length = 2048\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_mps = torch.backends.mps.is_available()\n",
    "\n",
    "# ÈÄâÊã©Âü∫Á°ÄÊ®°ÂûãÔºö‰ºòÂÖà‰ΩøÁî® SFT Ê®°ÂûãÔºåÂê¶Âàô‰ΩøÁî®Âü∫Á°ÄÊ®°Âûã\n",
    "# Ê≥®ÊÑèÔºöRM ËÆ≠ÁªÉÈúÄË¶ÅÂ∞ÜÂõ†ÊûúËØ≠Ë®ÄÊ®°ÂûãËΩ¨Êç¢‰∏∫Â∫èÂàóÂàÜÁ±ªÊ®°ÂûãÔºàÊ∑ªÂä†Â•ñÂä±Â§¥Ôºâ\n",
    "if 'adapter_dir' in globals() and os.path.exists(adapter_dir):\n",
    "    try:\n",
    "        # Â∞ùËØï‰ªé SFT Ê®°ÂûãÂä†ËΩΩÔºàÈúÄË¶ÅÂêàÂπ∂ LoRA ÊùÉÈáçÔºâ\n",
    "        from peft import PeftModel\n",
    "        base_model_for_rm = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_or_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        sft_model = PeftModel.from_pretrained(base_model_for_rm, adapter_dir)\n",
    "        # ÂêàÂπ∂ LoRA ÊùÉÈáçÂà∞Âü∫Á°ÄÊ®°Âûã\n",
    "        merged_model = sft_model.merge_and_unload()\n",
    "        rm_base_model_dir = None  # Ê†áËÆ∞‰ΩøÁî®ÂêàÂπ∂ÂêéÁöÑÊ®°Âûã\n",
    "        print(\"[RM] ‰ΩøÁî® SFT Ê®°Âûã‰Ωú‰∏∫Âü∫Á°ÄÔºàÂ∑≤ÂêàÂπ∂ LoRA ÊùÉÈáçÔºâ\")\n",
    "    except Exception as e:\n",
    "        print(f\"[RM] Êó†Ê≥ïÂä†ËΩΩ SFT Ê®°ÂûãÔºö{e}Ôºå‰ΩøÁî®Âü∫Á°ÄÊ®°Âûã\")\n",
    "        rm_base_model_dir = base_model_or_dir\n",
    "else:\n",
    "    rm_base_model_dir = base_model_or_dir\n",
    "    print(\"[RM] ‰ΩøÁî®Âü∫Á°ÄÊ®°ÂûãÔºàÊú™ÊâæÂà∞ SFT Ê®°ÂûãÔºâ\")\n",
    "\n",
    "# bitsandbytes QLoRA ÈÖçÁΩÆÔºàÂèØÈÄâÔºâ\n",
    "rm_quantization_config = None\n",
    "if use_cuda:\n",
    "    try:\n",
    "        rm_quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "        print(\"[RM] ‰ΩøÁî® bitsandbytes 4-bit ÈáèÂåñÂä†ËΩΩÊ®°Âûã\")\n",
    "    except Exception as e:\n",
    "        print(f\"[RM] Êú™ÂêØÁî®4bitÈáèÂåñÔºö{e}\")\n",
    "else:\n",
    "    print(\"[RM] ÂΩìÂâç‰∏∫Èùû CUDA ÁéØÂ¢ÉÔºå‰ΩøÁî®Â∏∏ËßÑÁ≤æÂ∫¶Âä†ËΩΩ\")\n",
    "\n",
    "# Tokenizer ÂàùÂßãÂåñÔºàÂ§çÁî®‰πãÂâçÁöÑ tokenizerÔºâ\n",
    "if 'tokenizer' not in globals() or tokenizer is None:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        rm_base_model_dir if rm_base_model_dir else base_model_or_dir,\n",
    "        use_fast=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Âä†ËΩΩÂ•ñÂä±Ê®°ÂûãÔºàÂ∞ÜÂõ†ÊûúËØ≠Ë®ÄÊ®°ÂûãËΩ¨Êç¢‰∏∫Â∫èÂàóÂàÜÁ±ªÊ®°ÂûãÔºâ\n",
    "# AutoModelForSequenceClassification ‰ºöËá™Âä®Ê∑ªÂä†ÂàÜÁ±ªÂ§¥ÔºàÂ•ñÂä±Â§¥Ôºâ\n",
    "if rm_base_model_dir:\n",
    "    rm_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        rm_base_model_dir,\n",
    "        trust_remote_code=True,\n",
    "        num_labels=1,  # Â•ñÂä±ÂàÜÊï∞ÊòØÊ†áÈáè\n",
    "        quantization_config=rm_quantization_config,\n",
    "        device_map=\"auto\" if use_cuda else None,\n",
    "    )\n",
    "else:\n",
    "    # Â¶ÇÊûú‰ΩøÁî®ÂêàÂπ∂ÂêéÁöÑ SFT Ê®°ÂûãÔºåÈúÄË¶Å‰ªéÂêàÂπ∂Ê®°ÂûãÂàõÂª∫ RM Ê®°Âûã\n",
    "    # ÁÆÄÂåñÂ§ÑÁêÜÔºöÁõ¥Êé•‰ΩøÁî®Âü∫Á°ÄÊ®°ÂûãÔºàÂÆûÈôÖ‰∏≠ÂèØËÉΩÈúÄË¶ÅÊõ¥Â§çÊùÇÁöÑÂ§ÑÁêÜÔºâ\n",
    "    rm_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        base_model_or_dir,\n",
    "        trust_remote_code=True,\n",
    "        num_labels=1,\n",
    "        quantization_config=rm_quantization_config,\n",
    "        device_map=\"auto\" if use_cuda else None,\n",
    "    )\n",
    "\n",
    "print(\"[RM] Â•ñÂä±Ê®°ÂûãÂä†ËΩΩÂÆåÊàê\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb3109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# „ÄêRM Êï∞ÊçÆÊ†ºÂºèÂåñÔºöÊûÑÈÄ† prompt-response ÂØπ„Äë\n",
    "def format_rm_prompt(prompt, response):\n",
    "    \"\"\"Â∞Ü prompt Âíå response Ê†ºÂºèÂåñ‰∏∫Ê®°ÂûãËæìÂÖ•ÊñáÊú¨ÔºàÁ¨¶Âêà Qwen Ê®°ÊùøÔºâ\"\"\"\n",
    "    # ÊûÑÈÄ†Á¨¶Âêà Qwen Ê®°ÊùøÁöÑÊñáÊú¨Ê†ºÂºè\n",
    "    text = f\"<|user|>\\n{prompt}\\n<|assistant|>\\n{response}\"\n",
    "    return text\n",
    "\n",
    "def tokenize_rm_dataset(examples):\n",
    "    \"\"\"ÂØπ RM Êï∞ÊçÆÈõÜËøõË°åÂàÜËØçÔºàÂ§ÑÁêÜ prompt+chosen Âíå prompt+rejected ÂØπÔºâ\"\"\"\n",
    "    # ÊûÑÈÄ† chosen Âíå rejected ÁöÑÂÆåÊï¥ÊñáÊú¨\n",
    "    chosen_texts = [format_rm_prompt(p, c) for p, c in zip(examples[\"prompt\"], examples[\"chosen\"])]\n",
    "    rejected_texts = [format_rm_prompt(p, r) for p, r in zip(examples[\"prompt\"], examples[\"rejected\"])]\n",
    "    \n",
    "    # ÂàÜËØçÔºàchosenÔºâ- ‰∏ç‰ΩøÁî® return_tensorsÔºåÂõ†‰∏∫ Dataset.map ÊúüÊúõËøîÂõûÊôÆÈÄöÂàóË°®\n",
    "    chosen_tokenized = tokenizer(\n",
    "        chosen_texts,\n",
    "        max_length=max_seq_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    \n",
    "    # ÂàÜËØçÔºàrejectedÔºâ- ‰∏ç‰ΩøÁî® return_tensors\n",
    "    rejected_tokenized = tokenizer(\n",
    "        rejected_texts,\n",
    "        max_length=max_seq_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    \n",
    "    # ËøîÂõûÊ†ºÂºèÂåñÁöÑÊï∞ÊçÆÔºàTRL RewardTrainer ÊúüÊúõÁöÑÊ†ºÂºèÔºâ\n",
    "    return {\n",
    "        \"input_ids_chosen\": chosen_tokenized[\"input_ids\"],\n",
    "        \"attention_mask_chosen\": chosen_tokenized[\"attention_mask\"],\n",
    "        \"input_ids_rejected\": rejected_tokenized[\"input_ids\"],\n",
    "        \"attention_mask_rejected\": rejected_tokenized[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "# ÂØπËÆ≠ÁªÉÈõÜÂíåÈ™åËØÅÈõÜËøõË°åÂàÜËØç\n",
    "print(\"[RM] ÂºÄÂßãÂØπÊï∞ÊçÆÈõÜËøõË°åÂàÜËØç...\")\n",
    "rm_train_tokenized = rm_train_ds.map(\n",
    "    tokenize_rm_dataset,\n",
    "    batched=True,\n",
    "    desc=\"ÂàÜËØçËÆ≠ÁªÉÈõÜ\"\n",
    ")\n",
    "rm_eval_tokenized = rm_eval_ds.map(\n",
    "    tokenize_rm_dataset,\n",
    "    batched=True,\n",
    "    desc=\"ÂàÜËØçÈ™åËØÅÈõÜ\"\n",
    ")\n",
    "\n",
    "print(f\"[RM] ÂàÜËØçÂÆåÊàêÔºö\")\n",
    "print(f\"  ËÆ≠ÁªÉÈõÜÔºö{len(rm_train_tokenized)} Êù°\")\n",
    "print(f\"  È™åËØÅÈõÜÔºö{len(rm_eval_tokenized)} Êù°\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97636ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# „ÄêRM LoRA ÈÖçÁΩÆ‰∏éËÆ≠ÁªÉÂèÇÊï∞„Äë\n",
    "# LoRA ÈÖçÁΩÆÔºàÂ¶ÇÊûú‰ΩøÁî® LoRA ÂæÆË∞ÉÔºâ\n",
    "rm_lora_config = LoraConfig(\n",
    "    r=16,                    # Áß©ÔºàrankÔºâ\n",
    "    lora_alpha=32,          # Áº©ÊîæÁ≥ªÊï∞ÔºàÈÄöÂ∏∏ alpha=2*rÔºâ\n",
    "    lora_dropout=0.05,      # ÈÄÇÈÖçÂô®Â±Ç dropout\n",
    "    bias=\"none\",            # ‰∏çËÆ≠ÁªÉ bias\n",
    "    task_type=\"SEQ_CLS\",    # ‰ªªÂä°Á±ªÂûãÔºöÂ∫èÂàóÂàÜÁ±ªÔºàÂ•ñÂä±Ê®°ÂûãÔºâ\n",
    ")\n",
    "\n",
    "# ËÆ≠ÁªÉÂèÇÊï∞ÔºöÊéßÂà∂ËÆ≠ÁªÉÊµÅÁ®ã‰∏é‰ºòÂåñ\n",
    "rm_args = RewardConfig(\n",
    "    # ËæìÂá∫‰∏é‰øùÂ≠ò\n",
    "    output_dir=rm_output_dir,           # Ê®°Âûã/Êó•ÂøóËæìÂá∫ÁõÆÂΩï\n",
    "    save_steps=500,                     # ÊØè N Ê≠•‰øùÂ≠ò‰∏ÄÊ¨° checkpoint\n",
    "    save_total_limit=2,                 # ÊúÄÂ§ö‰øùÁïô N ‰∏™ checkpoint\n",
    "    save_safetensors=True,              # ‰ΩøÁî® safetensors Ê†ºÂºè‰øùÂ≠ò\n",
    "    \n",
    "    # ÊâπÂ§ßÂ∞è‰∏éÊ¢ØÂ∫¶\n",
    "    per_device_train_batch_size=1,      # ÊØèËÆæÂ§áËÆ≠ÁªÉ batch Â§ßÂ∞è\n",
    "    per_device_eval_batch_size=1,       # ÊØèËÆæÂ§áÈ™åËØÅ batch Â§ßÂ∞è\n",
    "    gradient_accumulation_steps=8,      # Ê¢ØÂ∫¶Á¥ØÁßØÊ≠•Êï∞ÔºàÁ≠âÊïà batch = 1 * 8 = 8Ôºâ\n",
    "    \n",
    "    # ËÆ≠ÁªÉËΩÆÊ¨°‰∏éÂ≠¶‰π†Áéá\n",
    "    num_train_epochs=1,                 # ËÆ≠ÁªÉËΩÆÊ¨°Êï∞\n",
    "    learning_rate=1e-5,                 # ÂàùÂßãÂ≠¶‰π†ÁéáÔºàRM ËÆ≠ÁªÉÂ∏∏Áî® 1e-5 Âà∞ 5e-5Ôºâ\n",
    "    lr_scheduler_type=\"cosine\",         # Â≠¶‰π†ÁéáË∞ÉÂ∫¶Âô®Ôºà‰ΩôÂº¶ÈÄÄÁÅ´Ôºâ\n",
    "    warmup_ratio=0.03,                  # warmup ÊØî‰æãÔºàÂâç 3% Ê≠•Êï∞Á∫øÊÄßÂ¢ûÈïø LRÔºâ\n",
    "    \n",
    "    # ËØÑ‰º∞‰∏éÊó•Âøó\n",
    "    eval_strategy=\"steps\",              # ËØÑ‰º∞Á≠ñÁï•Ôºà\"steps\"/\"epoch\"/\"no\"Ôºâ\n",
    "    eval_steps=250,                     # ÊØè N Ê≠•ËØÑ‰º∞‰∏ÄÊ¨°\n",
    "    logging_steps=50,                   # ÊØè N Ê≠•ÊâìÂç∞‰∏ÄÊ¨°Êó•Âøó\n",
    "    report_to=[\"none\"],                 # ‰∏çÂêëÂ§ñÈÉ®‰∏äÊä•ÔºàÂèØÊîπ‰∏∫ [\"wandb\"] Á≠âÔºâ\n",
    "    \n",
    "    # Ê®°ÂûãÊ£ÄÊü•ÁÇπ\n",
    "    load_best_model_at_end=True,        # ËÆ≠ÁªÉÁªìÊùüÂä†ËΩΩÊúÄ‰Ω≥Ê®°Âûã\n",
    "    metric_for_best_model=\"eval_loss\",  # ÊúÄ‰Ω≥Ê®°ÂûãÊåáÊ†á\n",
    "    greater_is_better=False,            # ËØ•ÊåáÊ†áË∂äÂ∞èË∂äÂ•Ω\n",
    "    \n",
    "    # ÊÄßËÉΩ‰ºòÂåñ\n",
    "    bf16=use_cuda,                      # CUDA ‰∏äÁî® bfloat16\n",
    "    fp16=False,                         # Á¶ÅÁî® FP16\n",
    "    gradient_checkpointing=True,        # Ê¢ØÂ∫¶Ê£ÄÊü•ÁÇπÔºàÁâ∫Áâ≤Êó∂Èó¥Êç¢ÊòæÂ≠òÔºâ\n",
    "    \n",
    "    # RM ÁâπÂÆöÂèÇÊï∞\n",
    "    max_length=max_seq_length,          # ÊúÄÂ§ßÂ∫èÂàóÈïøÂ∫¶\n",
    ")\n",
    "\n",
    "print(\"[RM] ËÆ≠ÁªÉÂèÇÊï∞ÈÖçÁΩÆÂÆåÊàê\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447d7694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# „ÄêÊûÑÈÄ† RM ËÆ≠ÁªÉÂô®Âπ∂ÂºÄÂßãËÆ≠ÁªÉ„Äë\n",
    "# Â¶ÇÊûú‰ΩøÁî® LoRAÔºåÂ∫îÁî® LoRA ÈÖçÁΩÆÂà∞Ê®°Âûã\n",
    "if rm_quantization_config is not None or use_cuda:\n",
    "    # Â¶ÇÊûúÂ∑≤ÁªèÈáèÂåñÊàñ‰ΩøÁî® CUDAÔºåÂ∫îÁî® LoRA\n",
    "    try:\n",
    "        rm_model = get_peft_model(rm_model, rm_lora_config)\n",
    "        print(\"[RM] LoRA ÈÖçÁΩÆÂ∑≤Â∫îÁî®Âà∞Ê®°Âûã\")\n",
    "    except Exception as e:\n",
    "        print(f\"[RM] Â∫îÁî® LoRA Êó∂Âá∫ÈîôÔºö{e}ÔºåÁªßÁª≠‰ΩøÁî®ÂÖ®ÈáèÂæÆË∞É\")\n",
    "\n",
    "# ÊûÑÈÄ† RewardTrainer\n",
    "rm_trainer = RewardTrainer(\n",
    "    model=rm_model,\n",
    "    args=rm_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=rm_train_tokenized,\n",
    "    eval_dataset=rm_eval_tokenized,\n",
    "    peft_config=rm_lora_config if rm_quantization_config is None and not use_cuda else None,\n",
    ")\n",
    "\n",
    "print(\"[RM] ËÆ≠ÁªÉÂô®ÊûÑÈÄ†ÂÆåÊàêÔºåÂáÜÂ§áÂºÄÂßãËÆ≠ÁªÉ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20f807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÂºÄÂßãËÆ≠ÁªÉ\n",
    "rm_trainer.train()\n",
    "\n",
    "# ‰øùÂ≠òÊ®°Âûã\n",
    "rm_model_dir = os.path.join(rm_output_dir, \"reward_model\")\n",
    "rm_trainer.model.save_pretrained(rm_model_dir)\n",
    "tokenizer.save_pretrained(rm_model_dir)\n",
    "print(f\"[Done] RM + QLoRA ËÆ≠ÁªÉÂÆåÊàêÔºåÊ®°ÂûãÂ∑≤‰øùÂ≠òËá≥: {rm_model_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8fe69d",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ PPO Âº∫Âåñ‰ºòÂåñ\n",
    "- **ËæìÂÖ•**ÔºöSFT Ê®°Âûã‰Ωú‰∏∫ÂàùÂßãÁ≠ñÁï• \\(\\pi_\\theta\\)ÔºåÂ•ñÂä±Ê®°Âûã r ‰Ωú‰∏∫Â•ñÂä±‰ø°Âè∑\n",
    "- **ÁõÆÊ†á**ÔºöÂú® KL Á∫¶Êùü‰∏ãÊúÄÂ§ßÂåñÊúüÊúõÂ•ñÂä±ÔºåÊèêÂçáÂØπÈΩêÂ∫¶‰∏éÊúâÁî®ÊÄß\n",
    "- **ËÆ≠ÁªÉ**ÔºöPPOÔºàÂâ™ÂàáÁ≠ñÁï•Ê¢ØÂ∫¶ÔºâÔºåÂºïÂÖ• KL ÊÉ©ÁΩö‰ª•‰øùÊåÅ‰∏éÂèÇËÄÉÁ≠ñÁï•Êé•Ëøë\n",
    "- **ËæìÂá∫**ÔºöPPO ÂêéÁöÑÂØπÈΩêÊ®°ÂûãÔºàÊõ¥Á¨¶Âêà‰∫∫Á±ªÂÅèÂ•ΩÔºâ\n",
    "- **ÂÆûË∑µË¶ÅÁÇπ**ÔºöÈ´òË¥®ÈáèÂÅèÂ•ΩÊï∞ÊçÆ‰∏éÁ®≥ÂÆöÁöÑ KL ÊéßÂà∂ÊòØÊàêÂäüÂÖ≥ÈîÆÔºõÁõëÊéßÈïøÂ∫¶ÂÅèÁΩÆ„ÄÅÊ®°ÂºèÂùçÁº©‰∏éËøáÊãüÂêà„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a52435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# „ÄêPPO ËÆ≠ÁªÉÂáÜÂ§áÔºöÊ®°ÂûãÂä†ËΩΩ„Äë\n",
    "from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "import torch\n",
    "\n",
    "# PPO ËæìÂá∫ÈÖçÁΩÆ\n",
    "ppo_output_dir = \"outputs/ppo_model\"\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_mps = torch.backends.mps.is_available()\n",
    "\n",
    "print(\"[PPO] ÂºÄÂßãÂä†ËΩΩÊ®°Âûã...\")\n",
    "\n",
    "# ========== 1. Âä†ËΩΩÁ≠ñÁï•Ê®°ÂûãÔºàSFT Ê®°ÂûãÔºâ‰Ωú‰∏∫ÂàùÂßãÁ≠ñÁï• œÄ_Œ∏ ==========\n",
    "# PPO ÈúÄË¶ÅÁ≠ñÁï•Ê®°ÂûãÔºàËÆ≠ÁªÉ‰∏≠ÁöÑÊ®°ÂûãÔºâÂíåÂèÇËÄÉÊ®°ÂûãÔºàÁî®‰∫é KL Á∫¶ÊùüÔºâ\n",
    "if 'adapter_dir' in globals() and os.path.exists(adapter_dir):\n",
    "    try:\n",
    "        # ‰ªé SFT Ê®°ÂûãÂä†ËΩΩÔºàÂ∏¶ LoRA ÊùÉÈáçÔºâ\n",
    "        from peft import PeftModel\n",
    "        base_policy = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_or_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        policy_model = PeftModel.from_pretrained(base_policy, adapter_dir)\n",
    "        print(\"[PPO] Á≠ñÁï•Ê®°ÂûãÔºö‰ΩøÁî® SFT Ê®°ÂûãÔºàÂ∏¶ LoRA ÊùÉÈáçÔºâ\")\n",
    "        \n",
    "        # ÂèÇËÄÉÊ®°ÂûãÔºö‰ΩøÁî®ÂêàÂπ∂ÂêéÁöÑÂü∫Á°ÄÊ®°ÂûãÔºàÁî®‰∫é KL Á∫¶ÊùüÔºâ\n",
    "        reference_model = PeftModel.from_pretrained(base_policy, adapter_dir)\n",
    "        # ÂàõÂª∫ÂèÇËÄÉÊ®°ÂûãÁöÑÂâØÊú¨ÔºàÂÜªÁªìÂèÇÊï∞Ôºå‰∏çÊõ¥Êñ∞Ôºâ\n",
    "        reference_model.merge_and_unload()\n",
    "        print(\"[PPO] ÂèÇËÄÉÊ®°ÂûãÔºö‰ΩøÁî® SFT Ê®°ÂûãÔºàÂ∑≤ÂêàÂπ∂ÔºåÁî®‰∫é KL Á∫¶ÊùüÔºâ\")\n",
    "    except Exception as e:\n",
    "        print(f\"[PPO] Êó†Ê≥ïÂä†ËΩΩ SFT Ê®°ÂûãÔºö{e}Ôºå‰ΩøÁî®Âü∫Á°ÄÊ®°Âûã\")\n",
    "        policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_or_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        reference_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_or_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "else:\n",
    "    # Â¶ÇÊûúÊú™ÊâæÂà∞ SFT Ê®°ÂûãÔºå‰ΩøÁî®Âü∫Á°ÄÊ®°Âûã\n",
    "    policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_or_dir,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    reference_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_or_dir,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    print(\"[PPO] Á≠ñÁï•Ê®°ÂûãÂíåÂèÇËÄÉÊ®°ÂûãÔºö‰ΩøÁî®Âü∫Á°ÄÊ®°Âûã\")\n",
    "\n",
    "# Â∞ÜÁ≠ñÁï•Ê®°ÂûãÂåÖË£Ö‰∏∫Â∏¶ÂÄºÂáΩÊï∞ÁöÑÊ®°ÂûãÔºàPPO ÈúÄË¶ÅÔºâ\n",
    "# AutoModelForCausalLMWithValueHead ‰ºöÂú®Á≠ñÁï•Ê®°ÂûãÂü∫Á°Ä‰∏äÊ∑ªÂä†ÂÄºÂáΩÊï∞Â§¥ÔºàÁî®‰∫é‰ª∑ÂÄº‰º∞ËÆ°Ôºâ\n",
    "# Ê≥®ÊÑèÔºöÂ¶ÇÊûú policy_model ÊòØ PeftModelÔºåÈúÄË¶ÅÂÖàËé∑ÂèñÂü∫Á°ÄÊ®°ÂûãË∑ØÂæÑ\n",
    "try:\n",
    "    if hasattr(policy_model, 'merge_and_unload'):\n",
    "        # ÂØπ‰∫é PeftModelÔºåÂêàÂπ∂ LoRA ÊùÉÈáçÂêéÂàõÂª∫ ValueHead Ê®°Âûã\n",
    "        # Ê≥®ÊÑèÔºöÂêàÂπ∂ÂêéÁöÑÊ®°Âûã‰ºöÂú®ÂÜÖÂ≠ò‰∏≠ÔºåÈúÄË¶Å‰øùÂ≠òÂà∞‰∏¥Êó∂Ë∑ØÂæÑÊàñÁõ¥Êé•‰ΩøÁî®Âü∫Á°ÄÊ®°Âûã\n",
    "        print(\"[PPO] Ê£ÄÊµãÂà∞ PeftModelÔºå‰ªéÂü∫Á°ÄÊ®°ÂûãÂàõÂª∫ ValueHead Ê®°ÂûãÔºàÂ∞Ü‰ΩøÁî® SFT ÊùÉÈáçÔºâ\")\n",
    "        # ÁÆÄÂåñÂ§ÑÁêÜÔºöÁõ¥Êé•‰ªéÂü∫Á°ÄÊ®°ÂûãÂàõÂª∫ÔºåSFT ÊùÉÈáçÂ∞ÜÂú®ÂêéÁª≠Âä†ËΩΩ\n",
    "        ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "            base_model_or_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "    else:\n",
    "        # Â¶ÇÊûú policy_model ÊòØÊôÆÈÄöÊ®°ÂûãÔºåÁõ¥Êé•‰ªéÂü∫Á°ÄÊ®°ÂûãÂàõÂª∫\n",
    "        ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "            base_model_or_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "    print(\"[PPO] Á≠ñÁï•Ê®°ÂûãÔºàÂ∏¶ÂÄºÂáΩÊï∞Â§¥ÔºâÂä†ËΩΩÂÆåÊàê\")\n",
    "except Exception as e:\n",
    "    print(f\"[PPO] ÂàõÂª∫ ValueHead Ê®°ÂûãÂ§±Ë¥•Ôºö{e}\")\n",
    "    print(\"[PPO] Â∞ùËØïÁõ¥Êé•‰ªéÂü∫Á°ÄÊ®°ÂûãÂàõÂª∫...\")\n",
    "    ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "        base_model_or_dir,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "# ========== 2. Âä†ËΩΩÂ•ñÂä±Ê®°ÂûãÔºàRMÔºâ‰Ωú‰∏∫Â•ñÂä±‰ø°Âè∑ ==========\n",
    "if 'rm_model_dir' in globals() and os.path.exists(rm_model_dir):\n",
    "    try:\n",
    "        reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            rm_model_dir,\n",
    "            trust_remote_code=True,\n",
    "            num_labels=1,  # Â•ñÂä±ÂàÜÊï∞ÊòØÊ†áÈáè\n",
    "        )\n",
    "        print(f\"[PPO] Â•ñÂä±Ê®°ÂûãÔºö‰ªé {rm_model_dir} Âä†ËΩΩÊàêÂäü\")\n",
    "    except Exception as e:\n",
    "        print(f\"[PPO] Êó†Ê≥ïÂä†ËΩΩÂ•ñÂä±Ê®°ÂûãÔºö{e}\")\n",
    "        reward_model = None\n",
    "else:\n",
    "    print(\"[PPO] Êú™ÊâæÂà∞Â•ñÂä±Ê®°ÂûãÔºåÈúÄË¶ÅÂú®ËÆ≠ÁªÉÂâçÂÖàÂÆåÊàê RM ËÆ≠ÁªÉ\")\n",
    "    reward_model = None\n",
    "\n",
    "# Tokenizer Â§çÁî®ÔºàÂ¶ÇÊûúÂ∑≤ÂÆö‰πâÔºâ\n",
    "if 'tokenizer' not in globals() or tokenizer is None:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        base_model_or_dir,\n",
    "        use_fast=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"[PPO] Ê®°ÂûãÂä†ËΩΩÂÆåÊàê\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5960d724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# „ÄêPPO Êï∞ÊçÆÂáÜÂ§áÔºöÊèêÁ§∫Êï∞ÊçÆÈõÜ„Äë\n",
    "# PPO ËÆ≠ÁªÉÈúÄË¶ÅÊèêÁ§∫ÔºàpromptsÔºâÊï∞ÊçÆÈõÜÔºåÊ®°Âûã‰ºöÊ†πÊçÆËøô‰∫õÊèêÁ§∫ÁîüÊàêÂõûÁ≠î\n",
    "# ÁÑ∂ÂêéÂ•ñÂä±Ê®°ÂûãÂØπÁîüÊàêÁöÑÂõûÁ≠îÊâìÂàÜÔºåPPO Ê†πÊçÆÂ•ñÂä±‰ø°Âè∑‰ºòÂåñÁ≠ñÁï•\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# ÊñπÂºè 1Ôºö‰ªé SFT Êï∞ÊçÆÈõÜ‰∏≠ÊèêÂèñ promptsÔºàÊé®ËçêÔºâ\n",
    "# ‰ΩøÁî® SFT ËÆ≠ÁªÉÈõÜ‰∏≠ÁöÑ prompts ‰Ωú‰∏∫ PPO ÁöÑËæìÂÖ•\n",
    "if 'train_ds' in globals() and len(train_ds) > 0:\n",
    "    # ‰ªé SFT ËÆ≠ÁªÉÈõÜ‰∏≠ÊèêÂèñ prompts\n",
    "    ppo_prompts = [ex[\"prompt\"] for ex in train_ds.select(range(min(1000, len(train_ds))))]\n",
    "    print(f\"[PPO] ‰ªé SFT ËÆ≠ÁªÉÈõÜÊèêÂèñ {len(ppo_prompts)} ‰∏™ prompts\")\n",
    "else:\n",
    "    # ÊñπÂºè 2ÔºöÂàõÂª∫ÁÆÄÂçïÁöÑÊèêÁ§∫Á§∫‰æã\n",
    "    ppo_prompts = [\n",
    "        \"Ëß£Èáä‰∏Ä‰∏ã‰ªÄ‰πàÊòØÊú∫Âô®Â≠¶‰π†„ÄÇ\",\n",
    "        \"Â¶Ç‰ΩïÊèêÈ´òÁºñÁ®ãÊäÄËÉΩÔºü\",\n",
    "        \"‰ªÄ‰πàÊòØÊ∑±Â∫¶Â≠¶‰π†Ôºü\",\n",
    "        \"Â¶Ç‰ΩïÂ≠¶‰π† PythonÔºü\",\n",
    "        \"‰ªãÁªç‰∏Ä‰∏ãÁ•ûÁªèÁΩëÁªú„ÄÇ\",\n",
    "    ]\n",
    "    print(f\"[PPO] ‰ΩøÁî®Á§∫‰æã promptsÔºàÂÖ± {len(ppo_prompts)} ‰∏™Ôºâ\")\n",
    "\n",
    "# ÊûÑÈÄ† PPO ÊèêÁ§∫Êï∞ÊçÆÈõÜ\n",
    "ppo_dataset = Dataset.from_dict({\"query\": ppo_prompts})\n",
    "\n",
    "print(f\"[PPO] ÊèêÁ§∫Êï∞ÊçÆÈõÜÂáÜÂ§áÂÆåÊàêÔºö{len(ppo_dataset)} Êù° prompts\")\n",
    "print(f\"[PPO] Á§∫‰æã prompt: {ppo_dataset[0]['query']}\")\n",
    "\n",
    "# PPO ËÆ≠ÁªÉÊµÅÁ®ãËØ¥ÊòéÔºö\n",
    "# 1. Á≠ñÁï•Ê®°ÂûãÊ†πÊçÆ prompts ÁîüÊàêÂõûÁ≠îÔºàÂ§ö‰∏™ÂÄôÈÄâÂõûÁ≠îÔºâ\n",
    "# 2. Â•ñÂä±Ê®°ÂûãÂØπÊØè‰∏™ÁîüÊàêÁöÑÂõûÁ≠îÊâìÂàÜ\n",
    "# 3. PPO ÁÆóÊ≥ïÊ†πÊçÆÂ•ñÂä±‰ø°Âè∑‰ºòÂåñÁ≠ñÁï•ÔºåÂêåÊó∂‰øùÊåÅ‰∏éÂèÇËÄÉÊ®°ÂûãÁöÑ KL Êï£Â∫¶Á∫¶Êùü\n",
    "# 4. ÈáçÂ§ç‰∏äËø∞ËøáÁ®ãÔºåÁõ¥Âà∞Á≠ñÁï•Êî∂Êïõ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9851c08b",
   "metadata": {},
   "source": [
    "## üîç RLHF ‰∏âÈò∂ÊÆµÁöÑÊùÉÈáçÂ≠òÂÇ®ËØ¶Ëß£\n",
    "\n",
    "### 1Ô∏è‚É£ SFTÔºàÁõëÁù£ÂæÆË∞ÉÔºâÈò∂ÊÆµ - LoRA ÊùÉÈáçÂ≠òÂÇ®\n",
    "\n",
    "**Â≠òÂÇ®ÊñπÂºè**Ôºö‰ªÖ‰øùÂ≠ò LoRA adapter ÊùÉÈáçÔºàËΩªÈáèÁ∫ßÔºâ\n",
    "\n",
    "```\n",
    "SFT ËÆ≠ÁªÉÂêéÁöÑÊ®°ÂûãÁªìÊûÑÔºö\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   Âü∫Á°ÄÊ®°ÂûãÔºàQwen2.5-1.5BÔºâ              ‚îÇ  ‚Üê ‰∏ç‰øùÂ≠òÔºå‰øùÊåÅ‰∏çÂèò\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Embeddings                     ‚îÇ\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Transformer Layers (x24)       ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ LM Head                         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "           ‚ñ≤\n",
    "           ‚îÇ LoRA ÈÄÇÈÖçÂô®ÔºàÂ∞èÊùÉÈáçÁü©ÈòµÔºâ\n",
    "           ‚îÇ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   LoRA Adapters                     ‚îÇ  ‚Üê ‰øùÂ≠òËøô‰∫õ\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ LoRA_A (rank=16)              ‚îÇ    Â§ßÂ∞èÔºö~Âá†MB\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ LoRA_B (rank=16)              ‚îÇ    ÂèÇÊï∞ÈáèÔºö~0.1% of Âü∫Á°ÄÊ®°Âûã\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "‰øùÂ≠òË∑ØÂæÑÔºöoutputs/sft_qlora/adapter/\n",
    "‚îú‚îÄ‚îÄ adapter_config.json              ‚Üê LoRA ÈÖçÁΩÆ\n",
    "‚îú‚îÄ‚îÄ adapter_model.safetensors        ‚Üê LoRA ÊùÉÈáçÔºàËΩªÈáèÁ∫ßÔºâ\n",
    "‚îî‚îÄ‚îÄ tokenizer.json                   ‚Üê Tokenizer\n",
    "```\n",
    "\n",
    "**ÁâπÁÇπ**Ôºö\n",
    "- ‚úÖ ‰ªÖ‰øùÂ≠òÂ∞ëÈáèÂèÇÊï∞ÔºàLoRA ÊùÉÈáçÔºåÈÄöÂ∏∏Âè™ÊúâÂéüÊ®°ÂûãÁöÑ 0.1-1%Ôºâ\n",
    "- ‚úÖ Êñá‰ª∂Â∞èÔºå‰æø‰∫éÂ≠òÂÇ®ÂíåÂÖ±‰∫´\n",
    "- ‚úÖ ÈúÄË¶ÅÂü∫Á°ÄÊ®°ÂûãÊâçËÉΩ‰ΩøÁî®ÔºàÊé®ÁêÜÊó∂ÈúÄË¶ÅÂêàÂπ∂Ôºâ\n",
    "\n",
    "**‰ª£Á†ÅÁ§∫‰æã**Ôºö\n",
    "```python\n",
    "# SFT ËÆ≠ÁªÉÂêé‰øùÂ≠ò\n",
    "adapter_dir = os.path.join(output_dir, \"adapter\")\n",
    "trainer.model.save_pretrained(adapter_dir)  # Âè™‰øùÂ≠ò LoRA ÊùÉÈáç\n",
    "\n",
    "# ‰ΩøÁî®Êó∂ÁöÑÂä†ËΩΩ\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_or_dir)\n",
    "model = PeftModel.from_pretrained(base_model, adapter_dir)  # Âä†ËΩΩ LoRA\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ RMÔºàÂ•ñÂä±Ê®°ÂûãÔºâÈò∂ÊÆµ - Â•ñÂä±Â§¥Â≠òÂÇ®\n",
    "\n",
    "**Â≠òÂÇ®ÊñπÂºè**Ôºö‰øùÂ≠òÂÆåÊï¥ÁöÑÂ•ñÂä±Ê®°ÂûãÔºàÂü∫Á°ÄÊ®°Âûã + Â•ñÂä±Â§¥Ôºâ\n",
    "\n",
    "```\n",
    "RM ËÆ≠ÁªÉÂêéÁöÑÊ®°ÂûãÁªìÊûÑÔºö\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   Âü∫Á°ÄÊ®°ÂûãÔºàÂèØËÉΩÊòØ SFT ÂêàÂπ∂ÂêéÁöÑÔºâ       ‚îÇ  ‚Üê ÂåÖÂê´Âú®ÂÜÖ\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Embeddings                     ‚îÇ\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Transformer Layers             ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Hidden States                  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "           ‚ñ≤\n",
    "           ‚îÇ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   Â•ñÂä±Â§¥ÔºàReward HeadÔºâ                ‚îÇ  ‚Üê ËøôÊòØÊñ∞Ê∑ªÂä†ÁöÑ\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Linear Layer (hidden_size)     ‚îÇ    ÂÖ≥ÈîÆÁªÑ‰ª∂ÔºÅ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Output: 1 (Â•ñÂä±ÂàÜÊï∞Ê†áÈáè)        ‚îÇ    Â§ßÂ∞èÔºö~Âá†MB\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "‰øùÂ≠òË∑ØÂæÑÔºöoutputs/rm_qlora/reward_model/\n",
    "‚îú‚îÄ‚îÄ config.json                       ‚Üê Ê®°ÂûãÈÖçÁΩÆÔºàÂåÖÂê´ num_labels=1Ôºâ\n",
    "‚îú‚îÄ‚îÄ model.safetensors                 ‚Üê ÂÆåÊï¥Ê®°ÂûãÊùÉÈáçÔºàÂü∫Á°ÄÊ®°Âûã + Â•ñÂä±Â§¥Ôºâ\n",
    "‚îÇ                                       Êàñ adapter_model.safetensorsÔºàÂ¶ÇÊûúÁî®‰∫Ü LoRAÔºâ\n",
    "‚îî‚îÄ‚îÄ tokenizer.json                    ‚Üê Tokenizer\n",
    "```\n",
    "\n",
    "**ÁâπÁÇπ**Ôºö\n",
    "- ‚úÖ Â•ñÂä±Â§¥ÊòØÊñ∞Â¢ûÁöÑÁªÑ‰ª∂ÔºàLinear Â±ÇÔºåËæìÂá∫Áª¥Â∫¶‰∏∫ 1Ôºâ\n",
    "- ‚úÖ Â¶ÇÊûú‰ΩøÁî® LoRAÔºåÂèØËÉΩÂè™‰øùÂ≠ò LoRA ÊùÉÈáçÔºõÂ¶ÇÊûúÂÖ®ÈáèÂæÆË∞ÉÔºå‰øùÂ≠òÂÆåÊï¥Ê®°Âûã\n",
    "- ‚úÖ Â•ñÂä±Â§¥ÊòØÂÖ≥ÈîÆÔºöÂÆÉÂ≠¶‰ºö‰∫ÜÂØπÂõûÁ≠îÊâìÂàÜÔºàchosen > rejectedÔºâ\n",
    "\n",
    "**‰ª£Á†ÅÁ§∫‰æã**Ôºö\n",
    "```python\n",
    "# RM ËÆ≠ÁªÉÂêé‰øùÂ≠ò\n",
    "rm_model_dir = os.path.join(rm_output_dir, \"reward_model\")\n",
    "rm_trainer.model.save_pretrained(rm_model_dir)  # ‰øùÂ≠òÂ•ñÂä±Ê®°ÂûãÔºàÂåÖÂê´Â•ñÂä±Â§¥Ôºâ\n",
    "\n",
    "# ‰ΩøÁî®Êó∂ÁöÑÂä†ËΩΩ\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    rm_model_dir,\n",
    "    num_labels=1,  # Â•ñÂä±Â§¥ËæìÂá∫Áª¥Â∫¶‰∏∫ 1\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ PPOÔºàÂº∫Âåñ‰ºòÂåñÔºâÈò∂ÊÆµ - Á≠ñÁï•Ê®°Âûã + ÂÄºÂáΩÊï∞Â§¥Â≠òÂÇ®\n",
    "\n",
    "**Â≠òÂÇ®ÊñπÂºè**Ôºö‰øùÂ≠òÁ≠ñÁï•Ê®°ÂûãÊùÉÈáç + ÂÄºÂáΩÊï∞Â§¥ÊùÉÈáç\n",
    "\n",
    "```\n",
    "PPO ËÆ≠ÁªÉÂêéÁöÑÊ®°ÂûãÁªìÊûÑÔºö\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   Á≠ñÁï•Ê®°ÂûãÔºàPolicy ModelÔºâ             ‚îÇ  ‚Üê ÂèØ‰ª•ÊòØÂü∫Á°ÄÊ®°ÂûãÊàñ SFT+LoRA\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Embeddings                     ‚îÇ\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Transformer Layers             ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ LM Head                         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "           ‚ñ≤\n",
    "           ‚îÇ ‰∏§‰∏™Â§¥ÔºöÁ≠ñÁï•Â§¥ + ÂÄºÂáΩÊï∞Â§¥\n",
    "           ‚îÇ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   Á≠ñÁï•Â§¥ÔºàPolicy HeadÔºâ               ‚îÇ  ‚Üê ÂéüÊúâÁöÑÔºàÁî®‰∫éÁîüÊàêÔºâ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ LM Head (vocab_size)          ‚îÇ\n",
    "‚îÇ                                      ‚îÇ\n",
    "‚îÇ   ÂÄºÂáΩÊï∞Â§¥ÔºàValue HeadÔºâ               ‚îÇ  ‚Üê Êñ∞Ê∑ªÂä†ÁöÑÔºàÁî®‰∫é PPOÔºâ\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Linear Layer 1 (hidden_size)  ‚îÇ    ÂÖ≥ÈîÆÁªÑ‰ª∂ÔºÅ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Output: 1 (Áä∂ÊÄÅ‰ª∑ÂÄºÊ†áÈáè)       ‚îÇ    Â§ßÂ∞èÔºö~Âá†MB\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "‰øùÂ≠òË∑ØÂæÑÔºöoutputs/ppo_model/\n",
    "‚îú‚îÄ‚îÄ config.json                       ‚Üê Ê®°ÂûãÈÖçÁΩÆ\n",
    "‚îú‚îÄ‚îÄ pytorch_model.bin                  ‚Üê Á≠ñÁï•Ê®°ÂûãÊùÉÈáç\n",
    "‚îÇ   Êàñ adapter_model.safetensors      ‚Üê Â¶ÇÊûúÁî®‰∫Ü LoRAÔºà‰ªÖ LoRA ÊùÉÈáçÔºâ\n",
    "‚îú‚îÄ‚îÄ value_head/                        ‚Üê ÂÄºÂáΩÊï∞Â§¥ÔºàÂçïÁã¨‰øùÂ≠òÔºâ\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ config.json\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ pytorch_model.bin\n",
    "‚îî‚îÄ‚îÄ tokenizer.json                     ‚Üê Tokenizer\n",
    "```\n",
    "\n",
    "**ÁâπÁÇπ**Ôºö\n",
    "- ‚úÖ **ÂÄºÂáΩÊï∞Â§¥ÔºàValue HeadÔºâ**ÊòØ PPO ÁâπÊúâÁöÑÁªÑ‰ª∂\n",
    "  - Áî®‰∫é‰º∞ËÆ°Áä∂ÊÄÅ‰ª∑ÂÄºÔºàstate valueÔºâÔºåËÆ°ÁÆó‰ºòÂäøÂáΩÊï∞ÔºàadvantageÔºâ\n",
    "  - ËæìÂá∫Áª¥Â∫¶‰∏∫ 1ÔºàÊ†áÈáè‰ª∑ÂÄºÔºâ\n",
    "- ‚úÖ Á≠ñÁï•Ê®°ÂûãÂèØ‰ª•ÊòØÂü∫Á°ÄÊ®°ÂûãÊàñÂ∏¶ LoRA ÁöÑÊ®°Âûã\n",
    "- ‚úÖ Â¶ÇÊûú‰ΩøÁî® LoRAÔºåÂèØËÉΩÂè™‰øùÂ≠ò LoRA ÊùÉÈáçÔºõÂÄºÂáΩÊï∞Â§¥ÈÄöÂ∏∏ÂÖ®Èáè‰øùÂ≠ò\n",
    "\n",
    "**‰ª£Á†ÅÁ§∫‰æã**Ôºö\n",
    "```python\n",
    "# PPO Ê®°ÂûãÂàõÂª∫\n",
    "ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    base_model_or_dir,  # Ëá™Âä®Ê∑ªÂä†ÂÄºÂáΩÊï∞Â§¥\n",
    ")\n",
    "\n",
    "# PPO ËÆ≠ÁªÉÂêé‰øùÂ≠ò\n",
    "ppo_trainer.save_model(ppo_output_dir)  # ‰øùÂ≠òÁ≠ñÁï•Ê®°Âûã + ÂÄºÂáΩÊï∞Â§¥\n",
    "\n",
    "# ‰ΩøÁî®Êó∂ÁöÑÂä†ËΩΩ\n",
    "ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(ppo_output_dir)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä ‰∏âÈò∂ÊÆµÊùÉÈáçÂ≠òÂÇ®ÂØπÊØî\n",
    "\n",
    "| Èò∂ÊÆµ | Â≠òÂÇ®ÂÜÖÂÆπ | Â≠òÂÇ®Â§ßÂ∞è | ÂÖ≥ÈîÆÁªÑ‰ª∂ | ËÉΩÂê¶Áã¨Á´ã‰ΩøÁî® |\n",
    "|------|---------|---------|---------|------------|\n",
    "| **SFT** | LoRA Adapter ÊùÉÈáç | ~Âá†MBÔºà0.1-1% ÂèÇÊï∞ÈáèÔºâ | LoRA_A, LoRA_B | ‚ùå ÈúÄË¶ÅÂü∫Á°ÄÊ®°Âûã |\n",
    "| **RM** | Âü∫Á°ÄÊ®°Âûã + Â•ñÂä±Â§¥ | ~ÂÆåÊï¥Ê®°ÂûãÔºàÊàñ LoRAÔºâ | Â•ñÂä±Â§¥ÔºàReward HeadÔºâ | ‚úÖ ÂèØ‰ª•Áã¨Á´ã‰ΩøÁî® |\n",
    "| **PPO** | Á≠ñÁï•Ê®°Âûã + ÂÄºÂáΩÊï∞Â§¥ | ~ÂÆåÊï¥Ê®°ÂûãÔºàÊàñ LoRAÔºâ | ÂÄºÂáΩÊï∞Â§¥ÔºàValue HeadÔºâ | ‚úÖ ÂèØ‰ª•Áã¨Á´ã‰ΩøÁî® |\n",
    "\n",
    "## üîë ÂÖ≥ÈîÆÂå∫Âà´\n",
    "\n",
    "### SFT vs RM vs PPO ÁöÑÂ≠òÂÇ®Â∑ÆÂºÇ\n",
    "\n",
    "1. **SFT**Ôºö\n",
    "   - Âè™‰øùÂ≠ò LoRA ÊùÉÈáçÔºàËΩªÈáèÁ∫ßÔºâ\n",
    "   - ‰∏ç‰øÆÊîπÂü∫Á°ÄÊ®°Âûã\n",
    "   - Êé®ÁêÜÊó∂ÈúÄË¶ÅÂêàÂπ∂\n",
    "\n",
    "2. **RM**Ôºö\n",
    "   - ‰øùÂ≠òÂ•ñÂä±Â§¥ÔºàÊñ∞Â¢ûÁöÑ Linear Â±ÇÔºâ\n",
    "   - Â¶ÇÊûúÂÖ®ÈáèÂæÆË∞ÉÔºå‰øùÂ≠òÂÆåÊï¥Ê®°ÂûãÔºõÂ¶ÇÊûú LoRAÔºå‰øùÂ≠ò LoRA + Â•ñÂä±Â§¥ÈÖçÁΩÆ\n",
    "   - ÂèØ‰ª•Áã¨Á´ãÁî®‰∫éÊâìÂàÜ\n",
    "\n",
    "3. **PPO**Ôºö\n",
    "   - ‰øùÂ≠òÂÄºÂáΩÊï∞Â§¥ÔºàÊñ∞Â¢ûÁöÑÁªÑ‰ª∂ÔºåÁî®‰∫é RLÔºâ\n",
    "   - Á≠ñÁï•Ê®°ÂûãÂèØ‰ª•‰øùÊåÅ LoRA Ê†ºÂºèÊàñÂÖ®Èáè‰øùÂ≠ò\n",
    "   - ÂèØ‰ª•Áã¨Á´ãÁî®‰∫éÁîüÊàêÔºà‰ΩÜÂÄºÂáΩÊï∞Â§¥Âè™Âú®ËÆ≠ÁªÉÊó∂‰ΩøÁî®Ôºâ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64e31f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# „ÄêPPO ËÆ≠ÁªÉÈÖçÁΩÆ„Äë\n",
    "# PPO ËÆ≠ÁªÉÂèÇÊï∞ÔºöÊéßÂà∂Âº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÊµÅÁ®ã\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    # ËæìÂá∫‰∏é‰øùÂ≠ò\n",
    "    output_dir=ppo_output_dir,              # Ê®°Âûã/Êó•ÂøóËæìÂá∫ÁõÆÂΩï\n",
    "    save_steps=500,                         # ÊØè N Ê≠•‰øùÂ≠ò‰∏ÄÊ¨° checkpoint\n",
    "    save_total_limit=2,                     # ÊúÄÂ§ö‰øùÁïô N ‰∏™ checkpoint\n",
    "    \n",
    "    # ÁîüÊàêÂèÇÊï∞ÔºàÁ≠ñÁï•Ê®°ÂûãÁîüÊàêÂõûÁ≠îÊó∂ÁöÑÂèÇÊï∞Ôºâ\n",
    "    mini_batch_size=1,                     # PPO mini-batch Â§ßÂ∞èÔºàÊØè‰∏™ prompt ÁöÑÂ§ÑÁêÜÊâπÊ¨°Ôºâ\n",
    "    batch_size=8,                          # PPO batch Â§ßÂ∞èÔºàÊî∂ÈõÜÁªèÈ™åÁöÑÊâπÊ¨°Ôºâ\n",
    "    gradient_accumulation_steps=1,         # Ê¢ØÂ∫¶Á¥ØÁßØÊ≠•Êï∞\n",
    "    \n",
    "    # PPO ÁÆóÊ≥ïÂèÇÊï∞\n",
    "    ppo_epochs=4,                           # PPO Êõ¥Êñ∞ËΩÆÊ¨°ÔºàÊØèÊ¨°Êî∂ÈõÜÁªèÈ™åÂêéÊõ¥Êñ∞Â§öÂ∞ëËΩÆÔºâ\n",
    "    learning_rate=1e-6,                     # PPO Â≠¶‰π†ÁéáÔºàÈÄöÂ∏∏ËæÉÂ∞èÔºå1e-6 Âà∞ 1e-5Ôºâ\n",
    "    lr_scheduler_type=\"linear\",             # Â≠¶‰π†ÁéáË∞ÉÂ∫¶Âô®Á±ªÂûã\n",
    "    warmup_ratio=0.1,                       # warmup ÊØî‰æã\n",
    "    \n",
    "    # Â•ñÂä±‰∏é KL Á∫¶Êùü\n",
    "    init_kl_coef=0.1,                       # ÂàùÂßã KL ÊÉ©ÁΩöÁ≥ªÊï∞ÔºàÂπ≥Ë°°Â•ñÂä±‰∏é KL Êï£Â∫¶Ôºâ\n",
    "    target=6.0,                             # KL Êï£Â∫¶ÁõÆÊ†áÂÄºÔºàÊéßÂà∂‰∏éÂèÇËÄÉÊ®°ÂûãÁöÑÂÅèÁ¶ªÁ®ãÂ∫¶Ôºâ\n",
    "    horizon=10000,                          # PPO horizonÔºàÂ•ñÂä±ÂΩí‰∏ÄÂåñÂèÇÊï∞Ôºâ\n",
    "    gamma=1.0,                              # ÊäòÊâ£Âõ†Â≠êÔºàRL ‰∏≠ÁöÑÊú™Êù•Â•ñÂä±ÊäòÊâ£Ôºâ\n",
    "    lam=0.95,                               # GAE lambda ÂèÇÊï∞Ôºà‰ºòÂäø‰º∞ËÆ°Ôºâ\n",
    "    \n",
    "    # ÁîüÊàêÂèÇÊï∞ÔºàÊØè‰∏™ prompt ÁîüÊàêÂ§öÂ∞ë‰∏™ÂÄôÈÄâÂõûÁ≠îÔºâ\n",
    "    num_padding_at_beginning=1,             # Â°´ÂÖÖ‰ΩçÁΩÆÔºàÊüê‰∫õÊ®°ÂûãÈúÄË¶ÅÔºâ\n",
    "    \n",
    "    # ËÆ≠ÁªÉÊéßÂà∂\n",
    "    max_grad_norm=1.0,                      # Ê¢ØÂ∫¶Ë£ÅÂâ™ÔºàÈò≤Ê≠¢Ê¢ØÂ∫¶ÁàÜÁÇ∏Ôºâ\n",
    "    report_to=[\"none\"],                     # ‰∏çÂêëÂ§ñÈÉ®‰∏äÊä•ÔºàÂèØÊîπ‰∏∫ [\"wandb\"] Á≠âÔºâ\n",
    "    \n",
    "    # ËØÑ‰º∞‰∏éÊó•Âøó\n",
    "    log_with=\"none\",                        # Êó•ÂøóËÆ∞ÂΩïÂ∑•ÂÖ∑Ôºà\"wandb\", \"tensorboard\" Á≠âÔºâ\n",
    "    logging_steps=10,                       # ÊØè N Ê≠•ÊâìÂç∞‰∏ÄÊ¨°Êó•Âøó\n",
    "    \n",
    "    # ÊÄßËÉΩ‰ºòÂåñ\n",
    "    bf16=use_cuda,                          # CUDA ‰∏äÁî® bfloat16\n",
    "    fp16=False,                             # Á¶ÅÁî® FP16\n",
    "    \n",
    "    # Â∫èÂàóÈïøÂ∫¶\n",
    "    max_length=max_seq_length,              # ÊúÄÂ§ßÂ∫èÂàóÈïøÂ∫¶\n",
    "    max_new_tokens=512,                    # ÊØèÊ¨°ÁîüÊàêÁöÑÊúÄÂ§ßÊñ∞ token Êï∞\n",
    ")\n",
    "\n",
    "print(\"[PPO] ËÆ≠ÁªÉÈÖçÁΩÆÂÆåÊàê\")\n",
    "print(f\"[PPO] ÂÖ≥ÈîÆÂèÇÊï∞Ôºö\")\n",
    "print(f\"  - PPO epochs: {ppo_config.ppo_epochs}ÔºàÊØèÊ¨°Êõ¥Êñ∞ {ppo_config.ppo_epochs} ËΩÆÔºâ\")\n",
    "print(f\"  - Batch size: {ppo_config.batch_size}ÔºàÊî∂ÈõÜÁªèÈ™åÁöÑÊâπÊ¨°Â§ßÂ∞èÔºâ\")\n",
    "print(f\"  - Mini batch size: {ppo_config.mini_batch_size}ÔºàPPO Êõ¥Êñ∞ÁöÑÂ∞èÊâπÊ¨°Ôºâ\")\n",
    "print(f\"  - Learning rate: {ppo_config.learning_rate}ÔºàPPO Â≠¶‰π†ÁéáÔºâ\")\n",
    "print(f\"  - KL coefficient: {ppo_config.init_kl_coef}ÔºàKL ÊÉ©ÁΩöÁ≥ªÊï∞Ôºâ\")\n",
    "print(f\"  - Target KL: {ppo_config.target}ÔºàÁõÆÊ†á KL Êï£Â∫¶Ôºâ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1755d942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# „ÄêPPO ËÆ≠ÁªÉÂô®ÊûÑÈÄ†‰∏éËÆ≠ÁªÉÊµÅÁ®ãËØ¥Êòé„Äë\n",
    "from trl import PPOTrainer\n",
    "\n",
    "print(\"[PPO] ÊûÑÈÄ† PPO ËÆ≠ÁªÉÂô®...\")\n",
    "\n",
    "# Ê≥®ÊÑèÔºöÂú®ÊûÑÈÄ†ËÆ≠ÁªÉÂô®‰πãÂâçÔºåÈúÄË¶ÅÁ°Æ‰øùÂ•ñÂä±Ê®°ÂûãÂ∑≤Âä†ËΩΩ\n",
    "if reward_model is None:\n",
    "    print(\"[PPO] ‚ö†Ô∏è  Ë≠¶ÂëäÔºöÂ•ñÂä±Ê®°ÂûãÊú™Âä†ËΩΩÔºåÊó†Ê≥ïËøõË°å PPO ËÆ≠ÁªÉ\")\n",
    "    print(\"[PPO] ËØ∑ÂÖàÂÆåÊàê RM ËÆ≠ÁªÉÔºåÁ°Æ‰øù rm_model_dir ÊåáÂêëÊúâÊïàÁöÑÂ•ñÂä±Ê®°ÂûãË∑ØÂæÑ\")\n",
    "else:\n",
    "    # ÊûÑÈÄ† PPO ËÆ≠ÁªÉÂô®\n",
    "    ppo_trainer = PPOTrainer(\n",
    "        config=ppo_config,\n",
    "        model=ppo_model,                    # Á≠ñÁï•Ê®°ÂûãÔºàÂ∏¶ÂÄºÂáΩÊï∞Â§¥Ôºâ\n",
    "        ref_model=reference_model,          # ÂèÇËÄÉÊ®°ÂûãÔºàÁî®‰∫é KL Á∫¶ÊùüÔºâ\n",
    "        tokenizer=tokenizer,\n",
    "        dataset=ppo_dataset,                # ÊèêÁ§∫Êï∞ÊçÆÈõÜ\n",
    "    )\n",
    "    \n",
    "    print(\"[PPO] ËÆ≠ÁªÉÂô®ÊûÑÈÄ†ÂÆåÊàê\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"[PPO] PPO ËÆ≠ÁªÉÊµÅÁ®ãËØ¥ÊòéÔºö\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\"\"\n",
    "    PPOÔºàProximal Policy OptimizationÔºâÁöÑËÆ≠ÁªÉÊµÅÁ®ãÔºö\n",
    "    \n",
    "    1Ô∏è‚É£ „ÄêÊî∂ÈõÜÁªèÈ™åÈò∂ÊÆµ„ÄëÔºàExperience CollectionÔºâ\n",
    "       - Á≠ñÁï•Ê®°ÂûãÊ†πÊçÆ prompts ÁîüÊàêÂõûÁ≠î\n",
    "       - Â•ñÂä±Ê®°ÂûãÂØπÁîüÊàêÁöÑÂõûÁ≠îÊâìÂàÜÔºàrewardÔºâ\n",
    "       - ËÆ°ÁÆóÊØè‰∏™ÂõûÁ≠îÁöÑ‰ºòÂäøÔºàadvantageÔºâÂíå‰ª∑ÂÄºÔºàvalueÔºâ\n",
    "       - Êî∂ÈõÜ‰∏Ä‰∏™ batch ÁöÑÁªèÈ™åÔºàprompts, responses, rewards, advantagesÔºâ\n",
    "    \n",
    "    2Ô∏è‚É£ „ÄêPPO Êõ¥Êñ∞Èò∂ÊÆµ„ÄëÔºàPolicy UpdateÔºâ\n",
    "       - ÂØπÊî∂ÈõÜÁöÑÁªèÈ™åËøõË°åÂ§öËΩÆ PPO Êõ¥Êñ∞Ôºàppo_epochs ËΩÆÔºâ\n",
    "       - ËÆ°ÁÆóÁ≠ñÁï•ÊçüÂ§±Ôºàpolicy lossÔºâÔºöÊúÄÂ§ßÂåñÊúüÊúõÂ•ñÂä±\n",
    "       - ËÆ°ÁÆó‰ª∑ÂÄºÊçüÂ§±Ôºàvalue lossÔºâÔºö‰ª∑ÂÄºÂáΩÊï∞ÁöÑÂõûÂΩíÊçüÂ§±\n",
    "       - ËÆ°ÁÆó KL Êï£Â∫¶ÊçüÂ§±ÔºàKL penaltyÔºâÔºö‰øùÊåÅ‰∏éÂèÇËÄÉÁ≠ñÁï•ÁöÑÊé•Ëøë\n",
    "       - ÊÄªÊçüÂ§± = policy_loss - value_loss + kl_penalty\n",
    "       - ‰ΩøÁî®Ê¢ØÂ∫¶‰∏äÂçá‰ºòÂåñÁ≠ñÁï•ÂèÇÊï∞\n",
    "    \n",
    "    3Ô∏è‚É£ „ÄêKL Á∫¶ÊùüÊéßÂà∂„ÄëÔºàKL Divergence ControlÔºâ\n",
    "       - Âä®ÊÄÅË∞ÉÊï¥ KL ÊÉ©ÁΩöÁ≥ªÊï∞Ôºàkl_coefÔºâ\n",
    "       - Â¶ÇÊûú KL Êï£Â∫¶Ë∂ÖËøáÁõÆÊ†áÂÄºÔºåÂ¢ûÂä†ÊÉ©ÁΩö\n",
    "       - Â¶ÇÊûú KL Êï£Â∫¶‰Ωé‰∫éÁõÆÊ†áÂÄºÔºåÂáèÂ∞ëÊÉ©ÁΩö\n",
    "       - Á°Æ‰øùÁ≠ñÁï•‰∏ç‰ºöÂÅèÁ¶ªÂèÇËÄÉÊ®°ÂûãÂ§™ËøúÔºàÈÅøÂÖçÊ®°ÂºèÂùçÁº©Ôºâ\n",
    "    \n",
    "    4Ô∏è‚É£ „ÄêÈáçÂ§çËø≠‰ª£„Äë\n",
    "       - ÈáçÂ§çÊ≠•È™§ 1-3ÔºåÁõ¥Âà∞Á≠ñÁï•Êî∂ÊïõÊàñËææÂà∞ÊúÄÂ§ßÊ≠•Êï∞\n",
    "       - ÂÆöÊúü‰øùÂ≠ò checkpoint\n",
    "       - ÁõëÊéßÂ•ñÂä±„ÄÅKL Êï£Â∫¶„ÄÅÁîüÊàêË¥®ÈáèÁ≠âÊåáÊ†á\n",
    "    \n",
    "    ÂÖ≥ÈîÆ‰ºòÂäøÔºö\n",
    "    - ‚úÖ Á®≥ÂÆöÁöÑÁ≠ñÁï•Êõ¥Êñ∞ÔºàÂâ™ÂàáÁ≠ñÁï•Ê¢ØÂ∫¶ÔºåÈÅøÂÖçÂ§ßÂπÖÊ≥¢Âä®Ôºâ\n",
    "    - ‚úÖ KL Á∫¶ÊùüÈò≤Ê≠¢Ê®°ÂºèÂùçÁº©\n",
    "    - ‚úÖ ÊîØÊåÅÂú®Á∫øÂ≠¶‰π†ÔºàËæπÁîüÊàêËæπ‰ºòÂåñÔºâ\n",
    "    - ‚úÖ ÈÄÇÁî®‰∫éÂ§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°Âûã\n",
    "    \n",
    "    Ê≥®ÊÑè‰∫ãÈ°πÔºö\n",
    "    - ‚ö†Ô∏è  ÈúÄË¶ÅÈ´òË¥®ÈáèÂ•ñÂä±Ê®°ÂûãÔºàRM ÁöÑË¥®ÈáèÁõ¥Êé•ÂΩ±Âìç PPO ÊïàÊûúÔºâ\n",
    "    - ‚ö†Ô∏è  KL Á≥ªÊï∞ÈúÄË¶Å‰ªîÁªÜË∞É‰ºòÔºàËøáÂ§ßÂØºËá¥Êõ¥Êñ∞ÊÖ¢ÔºåËøáÂ∞èÂØºËá¥ÂÅèÁ¶ªÔºâ\n",
    "    - ‚ö†Ô∏è  ÁõëÊéßÈïøÂ∫¶ÂÅèÁΩÆÔºàÊ®°ÂûãÂèØËÉΩÂÄæÂêë‰∫éÁîüÊàêÊõ¥ÈïøÁöÑÂõûÁ≠î‰ª•Ëé∑ÂæóÊõ¥È´òÂ•ñÂä±Ôºâ\n",
    "    - ‚ö†Ô∏è  Èò≤Ê≠¢ËøáÊãüÂêàÔºàÈúÄË¶ÅÂÆöÊúüËØÑ‰º∞ÁîüÊàêË¥®ÈáèÔºâ\n",
    "    \"\"\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ÊûÑÈÄ†Â•ñÂä±ÂáΩÊï∞ÔºàÁî®‰∫é PPO ËÆ≠ÁªÉÔºâ\n",
    "    def reward_function(samples):\n",
    "        \"\"\"Â•ñÂä±ÂáΩÊï∞Ôºö‰ΩøÁî®Â•ñÂä±Ê®°ÂûãÂØπÁîüÊàêÁöÑÂõûÁ≠îÊâìÂàÜ\"\"\"\n",
    "        # ÂØπÊØè‰∏™ÁîüÊàêÁöÑÂõûÁ≠î‰ΩøÁî®Â•ñÂä±Ê®°ÂûãÊâìÂàÜ\n",
    "        inputs = tokenizer(\n",
    "            samples,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "        ).to(reward_model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Â•ñÂä±Ê®°ÂûãËøîÂõû logitsÔºàÊ†áÈáèÂ•ñÂä±ÂàÜÊï∞Ôºâ\n",
    "            rewards = reward_model(**inputs).logits.squeeze(-1)\n",
    "        \n",
    "        return rewards\n",
    "    \n",
    "    print(\"\\n[PPO] Â•ñÂä±ÂáΩÊï∞Â∑≤ÂÆö‰πâÔºà‰ΩøÁî®Â•ñÂä±Ê®°ÂûãÊâìÂàÜÔºâ\")\n",
    "    print(\"[PPO] ËÆ≠ÁªÉÂô®ÂáÜÂ§áÂ∞±Áª™ÔºåÂèØ‰ª•ÂºÄÂßãËÆ≠ÁªÉ\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"[PPO] ËÆ≠ÁªÉÁ§∫‰æã‰ª£Á†ÅÔºàÊ≥®ÈáäÊéâÔºå‰∏çÂÆûÈôÖÊâßË°åÔºâÔºö\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    example_code = '''\n",
    "    # ÂºÄÂßã PPO ËÆ≠ÁªÉÂæ™ÁéØ\n",
    "    generation_kwargs = {\n",
    "        \"max_new_tokens\": 512,              # ÁîüÊàêÁöÑÊúÄÂ§ßÊñ∞ token Êï∞\n",
    "        \"temperature\": 1.0,                # ÁîüÊàêÊ∏©Â∫¶ÔºàÊéßÂà∂ÈöèÊú∫ÊÄßÔºâ\n",
    "        \"do_sample\": True,                  # ÊòØÂê¶ÈááÊ†∑\n",
    "        \"top_p\": 0.95,                      # nucleus sampling\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    }\n",
    "    \n",
    "    # PPO ËÆ≠ÁªÉÂæ™ÁéØ\n",
    "    for epoch in range(1):  # ÂèØ‰ª•ËÆæÁΩÆÂ§ö‰∏™ epoch\n",
    "        for batch in ppo_trainer.dataloader:\n",
    "            # 1. Á≠ñÁï•Ê®°ÂûãÁîüÊàêÂõûÁ≠î\n",
    "            query_tensors = batch[\"input_ids\"]\n",
    "            response_tensors = ppo_trainer.generate(\n",
    "                query_tensors,\n",
    "                return_prompt=False,\n",
    "                length_sampler=None,\n",
    "                batch_size=ppo_config.batch_size,\n",
    "                **generation_kwargs,\n",
    "            )\n",
    "            \n",
    "            # 2. ÊèêÂèñÁîüÊàêÁöÑÂõûÁ≠îÊñáÊú¨\n",
    "            batch[\"response\"] = [\n",
    "                tokenizer.decode(r.squeeze(), skip_special_tokens=True)\n",
    "                for r in response_tensors\n",
    "            ]\n",
    "            \n",
    "            # 3. ËÆ°ÁÆóÂ•ñÂä±Ôºà‰ΩøÁî®Â•ñÂä±Ê®°ÂûãÔºâ\n",
    "            texts = [\n",
    "                q + r for q, r in zip(batch[\"query\"], batch[\"response\"])\n",
    "            ]\n",
    "            rewards = reward_function(texts)\n",
    "            \n",
    "            # 4. PPO Êõ¥Êñ∞\n",
    "            stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "            ppo_trainer.log_stats(stats, batch, rewards)\n",
    "    \n",
    "    # ‰øùÂ≠òÊúÄÁªàÊ®°Âûã\n",
    "    ppo_trainer.save_model(ppo_output_dir)\n",
    "    tokenizer.save_pretrained(ppo_output_dir)\n",
    "    print(f\"[Done] PPO ËÆ≠ÁªÉÂÆåÊàêÔºåÊ®°ÂûãÂ∑≤‰øùÂ≠òËá≥: {ppo_output_dir}\")\n",
    "    '''\n",
    "    \n",
    "    print(example_code)\n",
    "    print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336dc170",
   "metadata": {},
   "source": [
    "## DPOÔºàDirect Preference OptimizationÔºâ\n",
    "- **ÂÆö‰Ωç**Ôºö‰Ωú‰∏∫ÔºàRM+PPOÔºâÁöÑÂ∏∏ËßÅÊõø‰ª£ÊñπÊ°àÔºåÁî®ÂÅèÂ•ΩÂØπÁõ¥Êé•‰ºòÂåñÁ≠ñÁï•„ÄÇ\n",
    "- **Ê†∏ÂøÉ**ÔºöÂü∫‰∫é \\((x, y_{pos}, y_{neg})\\) ÊèêÈ´ò \\(y_{pos}\\) Ê¶ÇÁéá„ÄÅÈôç‰Ωé \\(y_{neg}\\)ÔºåÂπ∂‰ª•ÂèÇËÄÉÁ≠ñÁï• \\(\\pi_{ref}\\) ÁöÑÂØπÊï∞Ê¶ÇÁéáÂ∑Æ‰ΩúÈöêÂºè KL Á∫¶Êùü„ÄÇ\n",
    "- **Áõ¥ËßÇÁõÆÊ†á**ÔºöÊúÄÂ∞èÂåñ \\(-\\log \\sigma\\big(\\beta[(\\log \\pi_\\theta(y_{pos}|x) - \\log \\pi_\\theta(y_{neg}|x)) - (\\log \\pi_{ref}(y_{pos}|x) - \\log \\pi_{ref}(y_{neg}|x))]\\big)\\)\n",
    "- **‰ºòÁÇπ**ÔºöÊµÅÁ®ãÁÆÄÂçï„ÄÅÊó†Â•ñÂä±Ê®°Âûã‰∏é RL ÂõûË∑Ø„ÄÅÁ®≥ÂÆöÊòìÂ§çÁé∞„ÄÅÂêûÂêêÈ´ò„ÄÇ\n",
    "- **Â±ÄÈôê**Ôºö‰æùËµñÈ´òË¥®ÈáèÂÅèÂ•ΩÊï∞ÊçÆÔºõÊûÅÁ´ØÂàÜÂ∏ÉËøÅÁßª‰∏ãÂèØÊéßÊÄßËæÉÂº±„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcb38e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# „ÄêDPO Êï∞ÊçÆÂáÜÂ§áÔºöÂ§çÁî®ÂÅèÂ•ΩÊï∞ÊçÆÈõÜ„Äë\n",
    "# DPO ‰ΩøÁî®‰∏é RM Áõ∏ÂêåÁöÑÂÅèÂ•ΩÊï∞ÊçÆÊ†ºÂºèÔºàprompt, chosen, rejectedÔºâ\n",
    "# ÂèØ‰ª•Áõ¥Êé•Â§çÁî®‰πãÂâçÂä†ËΩΩÁöÑ RM Êï∞ÊçÆÈõÜ\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "print(\"[DPO] ÂáÜÂ§áÂÅèÂ•ΩÊï∞ÊçÆ...\")\n",
    "\n",
    "# ÊñπÂºè 1ÔºöÁõ¥Êé•Â§çÁî® RM ËÆ≠ÁªÉÊï∞ÊçÆÔºàÊé®ËçêÔºâ\n",
    "if 'rm_train_ds' in globals() and len(rm_train_ds) > 0:\n",
    "    # Â§çÁî® RM ÁöÑÂÅèÂ•ΩÊï∞ÊçÆÈõÜÔºàÂ∑≤ÁªèÊòØ prompt, chosen, rejected Ê†ºÂºèÔºâ\n",
    "    dpo_train_ds = rm_train_ds.select(range(min(5000, len(rm_train_ds))))  # ÂèØ‰ª•ÈÄâÊã©ÈÉ®ÂàÜÊï∞ÊçÆ\n",
    "    dpo_eval_ds = rm_eval_ds.select(range(min(200, len(rm_eval_ds))))\n",
    "    print(f\"[DPO] Â§çÁî® RM ËÆ≠ÁªÉÊï∞ÊçÆÔºöËÆ≠ÁªÉÈõÜ {len(dpo_train_ds)} Êù°ÔºåÈ™åËØÅÈõÜ {len(dpo_eval_ds)} Êù°\")\n",
    "else:\n",
    "    # ÊñπÂºè 2ÔºöÈáçÊñ∞Âä†ËΩΩÂÅèÂ•ΩÊï∞ÊçÆÔºàÂ¶ÇÊûú RM Êï∞ÊçÆ‰∏çÂèØÁî®Ôºâ\n",
    "    print(\"[DPO] RM Êï∞ÊçÆ‰∏çÂèØÁî®ÔºåÂ∞ùËØïÈáçÊñ∞Âä†ËΩΩÂÅèÂ•ΩÊï∞ÊçÆ...\")\n",
    "    if 'rm_ds' in globals():\n",
    "        dpo_split = rm_ds.train_test_split(test_size=0.02, seed=42)\n",
    "        dpo_train_ds = dpo_split[\"train\"].select(range(min(5000, len(dpo_split[\"train\"]))))\n",
    "        dpo_eval_ds = dpo_split[\"test\"].select(range(min(200, len(dpo_split[\"test\"]))))\n",
    "        print(f\"[DPO] ‰ªé rm_ds Âä†ËΩΩÔºöËÆ≠ÁªÉÈõÜ {len(dpo_train_ds)} Êù°ÔºåÈ™åËØÅÈõÜ {len(dpo_eval_ds)} Êù°\")\n",
    "    else:\n",
    "        # ÊñπÂºè 3Ôºö‰ΩøÁî®Á§∫‰æãÊï∞ÊçÆÔºà‰ªÖÁî®‰∫éÊºîÁ§∫Ôºâ\n",
    "        print(\"[DPO] ‚ö†Ô∏è  Ë≠¶ÂëäÔºö‰ΩøÁî®Á§∫‰æãÊï∞ÊçÆÔºåÂÆûÈôÖËÆ≠ÁªÉÂ∫î‰ΩøÁî®ÁúüÂÆûÂÅèÂ•ΩÊï∞ÊçÆ\")\n",
    "        dpo_examples = [\n",
    "            {\n",
    "                \"prompt\": \"Ëß£Èáä‰∏Ä‰∏ã‰ªÄ‰πàÊòØÊú∫Âô®Â≠¶‰π†„ÄÇ\",\n",
    "                \"chosen\": \"Êú∫Âô®Â≠¶‰π†ÊòØ‰∫∫Â∑•Êô∫ËÉΩÁöÑ‰∏Ä‰∏™ÂàÜÊîØÔºåÈÄöËøáÁÆóÊ≥ïËÆ©ËÆ°ÁÆóÊú∫‰ªéÊï∞ÊçÆ‰∏≠Â≠¶‰π†ËßÑÂæãÔºåÊó†ÈúÄÊòéÁ°ÆÁºñÁ®ãÂ∞±ËÉΩÂÅöÂá∫È¢ÑÊµãÊàñÂÜ≥Á≠ñ„ÄÇ\",\n",
    "                \"rejected\": \"Êú∫Âô®Â≠¶‰π†Â∞±ÊòØ‰∏ÄÁßçÁºñÁ®ãÊñπÂºè„ÄÇ\"\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": \"Â¶Ç‰ΩïÂ≠¶‰π† PythonÔºü\",\n",
    "                \"chosen\": \"Â≠¶‰π† Python ÂèØ‰ª•‰ªéÂü∫Á°ÄËØ≠Ê≥ïÂºÄÂßãÔºåÁÑ∂ÂêéÈÄêÊ≠•Â≠¶‰π†Êï∞ÊçÆÁªìÊûÑ„ÄÅÈù¢ÂêëÂØπË±°ÁºñÁ®ãÔºåÂπ∂ÈÄöËøáÂÆûÈôÖÈ°πÁõÆÁªÉ‰π†ÊèêÂçá„ÄÇ\",\n",
    "                \"rejected\": \"Python ÂæàÁÆÄÂçïÔºåÈöè‰æøÂ≠¶Â≠¶Â∞±Ë°å„ÄÇ\"\n",
    "            },\n",
    "        ]\n",
    "        dpo_train_ds = Dataset.from_list(dpo_examples)\n",
    "        dpo_eval_ds = Dataset.from_list(dpo_examples[:1])\n",
    "\n",
    "print(f\"\\n[DPO] Êï∞ÊçÆÂáÜÂ§áÂÆåÊàêÔºö\")\n",
    "print(f\"  ËÆ≠ÁªÉÈõÜÔºö{len(dpo_train_ds)} Êù°\")\n",
    "print(f\"  È™åËØÅÈõÜÔºö{len(dpo_eval_ds)} Êù°\")\n",
    "print(f\"\\n[DPO] Á§∫‰æãÊï∞ÊçÆÔºö\")\n",
    "sample = dpo_train_ds[0]\n",
    "for k, v in sample.items():\n",
    "    if isinstance(v, str) and len(v) > 100:\n",
    "        print(f\"  {k}: {v[:100]}...\")\n",
    "    else:\n",
    "        print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6a0c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# „ÄêDPO Ê®°ÂûãÂä†ËΩΩÔºöÁ≠ñÁï•Ê®°ÂûãÂíåÂèÇËÄÉÊ®°Âûã„Äë\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "import torch\n",
    "\n",
    "# DPO ËæìÂá∫ÈÖçÁΩÆ\n",
    "dpo_output_dir = \"outputs/dpo_qlora\"\n",
    "max_seq_length = 2048\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_mps = torch.backends.mps.is_available()\n",
    "\n",
    "print(\"[DPO] ÂºÄÂßãÂä†ËΩΩÊ®°Âûã...\")\n",
    "\n",
    "# ========== 1. Âä†ËΩΩÁ≠ñÁï•Ê®°ÂûãÔºàÈÄöÂ∏∏ÊòØ SFT Ê®°ÂûãÔºâ==========\n",
    "# DPO ÈúÄË¶ÅÁ≠ñÁï•Ê®°ÂûãÔºàËÆ≠ÁªÉ‰∏≠ÁöÑÊ®°ÂûãÔºâÂíåÂèÇËÄÉÊ®°ÂûãÔºàÁî®‰∫é KL Á∫¶ÊùüÔºåÂÜªÁªìÂèÇÊï∞Ôºâ\n",
    "if 'adapter_dir' in globals() and os.path.exists(adapter_dir):\n",
    "    try:\n",
    "        # ‰ªé SFT Ê®°ÂûãÂä†ËΩΩÔºàÂ∏¶ LoRA ÊùÉÈáçÔºâ\n",
    "        base_policy = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_or_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        dpo_policy_model = PeftModel.from_pretrained(base_policy, adapter_dir)\n",
    "        print(\"[DPO] Á≠ñÁï•Ê®°ÂûãÔºö‰ΩøÁî® SFT Ê®°ÂûãÔºàÂ∏¶ LoRA ÊùÉÈáçÔºâ\")\n",
    "    except Exception as e:\n",
    "        print(f\"[DPO] Êó†Ê≥ïÂä†ËΩΩ SFT Ê®°ÂûãÔºö{e}Ôºå‰ΩøÁî®Âü∫Á°ÄÊ®°Âûã\")\n",
    "        dpo_policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_or_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "else:\n",
    "    # Â¶ÇÊûúÊú™ÊâæÂà∞ SFT Ê®°ÂûãÔºå‰ΩøÁî®Âü∫Á°ÄÊ®°Âûã\n",
    "    dpo_policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_or_dir,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    print(\"[DPO] Á≠ñÁï•Ê®°ÂûãÔºö‰ΩøÁî®Âü∫Á°ÄÊ®°ÂûãÔºàÊú™ÊâæÂà∞ SFT Ê®°ÂûãÔºâ\")\n",
    "\n",
    "# ========== 2. Âä†ËΩΩÂèÇËÄÉÊ®°ÂûãÔºàÁî®‰∫é KL Á∫¶ÊùüÔºåÂÜªÁªìÂèÇÊï∞Ôºâ==========\n",
    "# ÂèÇËÄÉÊ®°ÂûãÈÄöÂ∏∏ÊòØ SFT Ê®°ÂûãÁöÑÂâØÊú¨Ôºå‰ΩÜÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÂÜªÁªìÂèÇÊï∞\n",
    "if 'adapter_dir' in globals() and os.path.exists(adapter_dir):\n",
    "    try:\n",
    "        # Âä†ËΩΩÂèÇËÄÉÊ®°ÂûãÔºà‰∏éÁ≠ñÁï•Ê®°ÂûãÁõ∏ÂêåÔºå‰ΩÜÂÜªÁªìÂèÇÊï∞Ôºâ\n",
    "        base_ref = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_or_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        # ÂØπ‰∫é LoRA Ê®°ÂûãÔºåÂèØ‰ª•Âä†ËΩΩÁõ∏ÂêåÁöÑ adapterÔºåÁÑ∂ÂêéÂú®ËÆ≠ÁªÉÂô®‰∏≠ÂÜªÁªì\n",
    "        dpo_ref_model = PeftModel.from_pretrained(base_ref, adapter_dir)\n",
    "        # ÂÜªÁªìÂèÇËÄÉÊ®°ÂûãÁöÑÂèÇÊï∞ÔºàDPOTrainer ‰ºöËá™Âä®Â§ÑÁêÜÔºâ\n",
    "        print(\"[DPO] ÂèÇËÄÉÊ®°ÂûãÔºö‰ΩøÁî® SFT Ê®°ÂûãÔºàÂ∏¶ LoRA ÊùÉÈáçÔºåËÆ≠ÁªÉÊó∂ÂÜªÁªìÔºâ\")\n",
    "    except Exception as e:\n",
    "        print(f\"[DPO] Êó†Ê≥ïÂä†ËΩΩ SFT Ê®°Âûã‰Ωú‰∏∫ÂèÇËÄÉÔºö{e}Ôºå‰ΩøÁî®Âü∫Á°ÄÊ®°Âûã\")\n",
    "        dpo_ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_or_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "else:\n",
    "    # Â¶ÇÊûúÊú™ÊâæÂà∞ SFT Ê®°ÂûãÔºå‰ΩøÁî®Âü∫Á°ÄÊ®°Âûã\n",
    "    dpo_ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_or_dir,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    print(\"[DPO] ÂèÇËÄÉÊ®°ÂûãÔºö‰ΩøÁî®Âü∫Á°ÄÊ®°ÂûãÔºàÊú™ÊâæÂà∞ SFT Ê®°ÂûãÔºâ\")\n",
    "\n",
    "# Tokenizer ÂàùÂßãÂåñÔºàÂ¶ÇÊûú‰πãÂâçÊú™ÂÆö‰πâÔºâ\n",
    "if 'tokenizer' not in globals() or tokenizer is None:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        base_model_or_dir,\n",
    "        use_fast=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"[DPO] Ê®°ÂûãÂä†ËΩΩÂÆåÊàê\")\n",
    "print(f\"[DPO] Á≠ñÁï•Ê®°ÂûãÔºö{type(dpo_policy_model).__name__}\")\n",
    "print(f\"[DPO] ÂèÇËÄÉÊ®°ÂûãÔºö{type(dpo_ref_model).__name__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef44db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# „ÄêDPO LoRA ÈÖçÁΩÆ‰∏éËÆ≠ÁªÉÂèÇÊï∞„Äë\n",
    "# DPO ÂèØ‰ª•‰ΩøÁî® LoRA ÂæÆË∞ÉÔºå‰πüÂèØ‰ª•ÂÖ®ÈáèÂæÆË∞É\n",
    "\n",
    "# LoRA ÈÖçÁΩÆÔºàÂ¶ÇÊûúÈúÄË¶Å‰ΩøÁî® LoRAÔºâ\n",
    "dpo_lora_config = None\n",
    "if True:  # ÈªòËÆ§‰ΩøÁî® LoRAÔºàÊõ¥ËäÇÁúÅÊòæÂ≠òÔºâ\n",
    "    dpo_lora_config = LoraConfig(\n",
    "        r=16,                    # Áß©ÔºàrankÔºâ\n",
    "        lora_alpha=32,          # Áº©ÊîæÁ≥ªÊï∞ÔºàÈÄöÂ∏∏ alpha=2*rÔºâ\n",
    "        lora_dropout=0.05,      # ÈÄÇÈÖçÂô®Â±Ç dropout\n",
    "        bias=\"none\",            # ‰∏çËÆ≠ÁªÉ bias\n",
    "        task_type=\"CAUSAL_LM\",  # ‰ªªÂä°Á±ªÂûãÔºöÂõ†ÊûúËØ≠Ë®ÄÊ®°Âûã\n",
    "    )\n",
    "    print(\"[DPO] ‰ΩøÁî® LoRA ÈÖçÁΩÆÔºàr=16, alpha=32Ôºâ\")\n",
    "else:\n",
    "    print(\"[DPO] ‰ΩøÁî®ÂÖ®ÈáèÂæÆË∞ÉÔºà‰∏ç‰ΩøÁî® LoRAÔºâ\")\n",
    "\n",
    "# bitsandbytes QLoRA ÈÖçÁΩÆÔºàÂèØÈÄâÔºâ\n",
    "dpo_quantization_config = None\n",
    "if use_cuda:\n",
    "    try:\n",
    "        dpo_quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "        print(\"[DPO] ‰ΩøÁî® bitsandbytes 4-bit ÈáèÂåñÂä†ËΩΩÊ®°Âûã\")\n",
    "    except Exception as e:\n",
    "        print(f\"[DPO] Êú™ÂêØÁî®4bitÈáèÂåñÔºö{e}\")\n",
    "else:\n",
    "    print(\"[DPO] ÂΩìÂâç‰∏∫Èùû CUDA ÁéØÂ¢ÉÔºå‰ΩøÁî®Â∏∏ËßÑÁ≤æÂ∫¶Âä†ËΩΩ\")\n",
    "\n",
    "# DPO ËÆ≠ÁªÉÂèÇÊï∞ÔºöÊéßÂà∂ËÆ≠ÁªÉÊµÅÁ®ã‰∏é‰ºòÂåñ\n",
    "dpo_args = TrainingArguments(\n",
    "    # ËæìÂá∫‰∏é‰øùÂ≠ò\n",
    "    output_dir=dpo_output_dir,              # Ê®°Âûã/Êó•ÂøóËæìÂá∫ÁõÆÂΩï\n",
    "    save_steps=500,                          # ÊØè N Ê≠•‰øùÂ≠ò‰∏ÄÊ¨° checkpoint\n",
    "    save_total_limit=2,                      # ÊúÄÂ§ö‰øùÁïô N ‰∏™ checkpoint\n",
    "    save_safetensors=True,                   # ‰ΩøÁî® safetensors Ê†ºÂºè‰øùÂ≠ò\n",
    "    \n",
    "    # ÊâπÂ§ßÂ∞è‰∏éÊ¢ØÂ∫¶\n",
    "    per_device_train_batch_size=2,          # ÊØèËÆæÂ§áËÆ≠ÁªÉ batch Â§ßÂ∞èÔºàDPO ÈúÄË¶ÅÂ§ÑÁêÜ chosen/rejected ÂØπÔºâ\n",
    "    per_device_eval_batch_size=2,           # ÊØèËÆæÂ§áÈ™åËØÅ batch Â§ßÂ∞è\n",
    "    gradient_accumulation_steps=4,          # Ê¢ØÂ∫¶Á¥ØÁßØÊ≠•Êï∞ÔºàÁ≠âÊïà batch = 2 * 4 = 8Ôºâ\n",
    "    \n",
    "    # ËÆ≠ÁªÉËΩÆÊ¨°‰∏éÂ≠¶‰π†Áéá\n",
    "    num_train_epochs=1,                      # ËÆ≠ÁªÉËΩÆÊ¨°Êï∞\n",
    "    learning_rate=1e-5,                     # ÂàùÂßãÂ≠¶‰π†ÁéáÔºàDPO Â∏∏Áî® 1e-5 Âà∞ 5e-5Ôºâ\n",
    "    lr_scheduler_type=\"cosine\",             # Â≠¶‰π†ÁéáË∞ÉÂ∫¶Âô®Ôºà‰ΩôÂº¶ÈÄÄÁÅ´Ôºâ\n",
    "    warmup_ratio=0.1,                       # warmup ÊØî‰æãÔºàÂâç 10% Ê≠•Êï∞Á∫øÊÄßÂ¢ûÈïø LRÔºâ\n",
    "    \n",
    "    # ËØÑ‰º∞‰∏éÊó•Âøó\n",
    "    eval_strategy=\"steps\",                  # ËØÑ‰º∞Á≠ñÁï•Ôºà\"steps\"/\"epoch\"/\"no\"Ôºâ\n",
    "    eval_steps=250,                         # ÊØè N Ê≠•ËØÑ‰º∞‰∏ÄÊ¨°\n",
    "    logging_steps=50,                       # ÊØè N Ê≠•ÊâìÂç∞‰∏ÄÊ¨°Êó•Âøó\n",
    "    report_to=[\"none\"],                     # ‰∏çÂêëÂ§ñÈÉ®‰∏äÊä•ÔºàÂèØÊîπ‰∏∫ [\"wandb\"] Á≠âÔºâ\n",
    "    \n",
    "    # Ê®°ÂûãÊ£ÄÊü•ÁÇπ\n",
    "    load_best_model_at_end=True,            # ËÆ≠ÁªÉÁªìÊùüÂä†ËΩΩÊúÄ‰Ω≥Ê®°Âûã\n",
    "    metric_for_best_model=\"eval_loss\",     # ÊúÄ‰Ω≥Ê®°ÂûãÊåáÊ†á\n",
    "    greater_is_better=False,                # ËØ•ÊåáÊ†áË∂äÂ∞èË∂äÂ•Ω\n",
    "    \n",
    "    # ÊÄßËÉΩ‰ºòÂåñ\n",
    "    bf16=use_cuda,                          # CUDA ‰∏äÁî® bfloat16\n",
    "    fp16=False,                             # Á¶ÅÁî® FP16\n",
    "    gradient_checkpointing=True,           # Ê¢ØÂ∫¶Ê£ÄÊü•ÁÇπÔºàÁâ∫Áâ≤Êó∂Èó¥Êç¢ÊòæÂ≠òÔºâ\n",
    ")\n",
    "\n",
    "# DPO ÁâπÂÆöÈÖçÁΩÆÔºàbeta Ê∏©Â∫¶ÂèÇÊï∞Ôºâ\n",
    "dpo_config = DPOConfig(\n",
    "    beta=0.1,                               # Ê∏©Â∫¶ÂèÇÊï∞ÔºàÊéßÂà∂ KL Á∫¶ÊùüÂº∫Â∫¶ÔºåÂ∏∏Áî® 0.1-0.5Ôºâ\n",
    "    loss_type=\"sigmoid\",                    # ÊçüÂ§±Á±ªÂûãÔºà\"sigmoid\" Êàñ \"hinge\"Ôºâ\n",
    "    label_smoothing=0.0,                    # Ê†áÁ≠æÂπ≥ÊªëÔºà0.0 Ë°®Á§∫‰∏çÂπ≥ÊªëÔºâ\n",
    "    reference_free=False,                   # ÊòØÂê¶‰∏ç‰ΩøÁî®ÂèÇËÄÉÊ®°ÂûãÔºàFalse Ë°®Á§∫‰ΩøÁî®Ôºâ\n",
    ")\n",
    "\n",
    "print(\"[DPO] ËÆ≠ÁªÉÂèÇÊï∞ÈÖçÁΩÆÂÆåÊàê\")\n",
    "print(f\"[DPO] ÂÖ≥ÈîÆÂèÇÊï∞Ôºö\")\n",
    "print(f\"  - Learning rate: {dpo_args.learning_rate}ÔºàDPO Â≠¶‰π†ÁéáÔºâ\")\n",
    "print(f\"  - Beta (Œ≤): {dpo_config.beta}ÔºàKL Á∫¶ÊùüÂº∫Â∫¶ÔºåË∂äÂ§ßË∂äÊé•ËøëÂèÇËÄÉÊ®°ÂûãÔºâ\")\n",
    "print(f\"  - Batch size: {dpo_args.per_device_train_batch_size}ÔºàÊØèËÆæÂ§áÊâπÊ¨°Â§ßÂ∞èÔºâ\")\n",
    "print(f\"  - Gradient accumulation: {dpo_args.gradient_accumulation_steps}ÔºàÊ¢ØÂ∫¶Á¥ØÁßØÊ≠•Êï∞Ôºâ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ab23bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# „ÄêDPO ËÆ≠ÁªÉÂô®ÊûÑÈÄ†‰∏éËÆ≠ÁªÉ„Äë\n",
    "from trl import DPOTrainer\n",
    "\n",
    "print(\"[DPO] ÊûÑÈÄ† DPO ËÆ≠ÁªÉÂô®...\")\n",
    "\n",
    "# Â¶ÇÊûúÁ≠ñÁï•Ê®°Âûã‰ΩøÁî®‰∫ÜÈáèÂåñÔºåÈúÄË¶ÅÂ∫îÁî® LoRAÔºàÂ¶ÇÊûú‰ΩøÁî®Ôºâ\n",
    "if dpo_quantization_config is not None or use_cuda:\n",
    "    # Â¶ÇÊûúÂ∑≤ÁªèÈáèÂåñÊàñ‰ΩøÁî® CUDAÔºåÂ∞ùËØïÂ∫îÁî® LoRA\n",
    "    try:\n",
    "        if dpo_lora_config is not None:\n",
    "            from peft import get_peft_model\n",
    "            # Ê≥®ÊÑèÔºöÂ¶ÇÊûúÁ≠ñÁï•Ê®°ÂûãÂ∑≤ÁªèÊòØ PeftModelÔºåÈúÄË¶ÅÁâπÊÆäÂ§ÑÁêÜ\n",
    "            if not isinstance(dpo_policy_model, PeftModel):\n",
    "                dpo_policy_model = get_peft_model(dpo_policy_model, dpo_lora_config)\n",
    "                print(\"[DPO] LoRA ÈÖçÁΩÆÂ∑≤Â∫îÁî®Âà∞Á≠ñÁï•Ê®°Âûã\")\n",
    "    except Exception as e:\n",
    "        print(f\"[DPO] Â∫îÁî® LoRA Êó∂Âá∫ÈîôÔºö{e}ÔºåÁªßÁª≠‰ΩøÁî®ÂΩìÂâçÈÖçÁΩÆ\")\n",
    "\n",
    "# ÊûÑÈÄ† DPO ËÆ≠ÁªÉÂô®\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=dpo_policy_model,                  # Á≠ñÁï•Ê®°ÂûãÔºàË¶Å‰ºòÂåñÁöÑÊ®°ÂûãÔºâ\n",
    "    ref_model=dpo_ref_model,                 # ÂèÇËÄÉÊ®°ÂûãÔºàÁî®‰∫é KL Á∫¶ÊùüÔºåÂÜªÁªìÂèÇÊï∞Ôºâ\n",
    "    args=dpo_args,                           # ËÆ≠ÁªÉÂèÇÊï∞\n",
    "    beta=dpo_config.beta,                    # Ê∏©Â∫¶ÂèÇÊï∞ÔºàKL Á∫¶ÊùüÂº∫Â∫¶Ôºâ\n",
    "    train_dataset=dpo_train_ds,             # ËÆ≠ÁªÉÊï∞ÊçÆÈõÜÔºàprompt, chosen, rejectedÔºâ\n",
    "    eval_dataset=dpo_eval_ds,                # È™åËØÅÊï∞ÊçÆÈõÜ\n",
    "    tokenizer=tokenizer,                     # Tokenizer\n",
    "    peft_config=dpo_lora_config if dpo_quantization_config is None and not use_cuda else None,  # LoRA ÈÖçÁΩÆ\n",
    "    max_length=max_seq_length,              # ÊúÄÂ§ßÂ∫èÂàóÈïøÂ∫¶\n",
    "    max_target_length=max_seq_length,        # ÊúÄÂ§ßÁõÆÊ†áÈïøÂ∫¶\n",
    "    loss_type=dpo_config.loss_type,          # ÊçüÂ§±Á±ªÂûãÔºà\"sigmoid\" Êàñ \"hinge\"Ôºâ\n",
    "    label_smoothing=dpo_config.label_smoothing,  # Ê†áÁ≠æÂπ≥Êªë\n",
    ")\n",
    "\n",
    "print(\"[DPO] ËÆ≠ÁªÉÂô®ÊûÑÈÄ†ÂÆåÊàê\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"[DPO] DPO ËÆ≠ÁªÉÊµÅÁ®ãËØ¥ÊòéÔºö\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "DPOÔºàDirect Preference OptimizationÔºâÁöÑËÆ≠ÁªÉÊµÅÁ®ãÔºö\n",
    "\n",
    "1Ô∏è‚É£ „ÄêÊï∞ÊçÆÂáÜÂ§á„Äë\n",
    "   - ÂáÜÂ§áÂÅèÂ•ΩÊï∞ÊçÆÔºàprompt, chosen, rejectedÔºâ\n",
    "   - chosenÔºöÊõ¥Â•ΩÁöÑÂõûÁ≠î\n",
    "   - rejectedÔºöÊõ¥Â∑ÆÁöÑÂõûÁ≠î\n",
    "\n",
    "2Ô∏è‚É£ „ÄêÊ®°ÂûãÂáÜÂ§á„Äë\n",
    "   - Á≠ñÁï•Ê®°ÂûãÔºàœÄ_Œ∏ÔºâÔºöË¶Å‰ºòÂåñÁöÑÊ®°ÂûãÔºàÈÄöÂ∏∏ÊòØ SFT Ê®°ÂûãÔºâ\n",
    "   - ÂèÇËÄÉÊ®°ÂûãÔºàœÄ_refÔºâÔºöÁî®‰∫é KL Á∫¶ÊùüÁöÑÂü∫ÂáÜÊ®°ÂûãÔºàÂÜªÁªìÂèÇÊï∞Ôºâ\n",
    "\n",
    "3Ô∏è‚É£ „ÄêDPO ËÆ≠ÁªÉ„Äë\n",
    "   - ËÆ°ÁÆóÁ≠ñÁï•Ê®°ÂûãÂØπ chosen/rejected ÁöÑÂØπÊï∞Ê¶ÇÁéá\n",
    "   - ËÆ°ÁÆóÂèÇËÄÉÊ®°ÂûãÂØπ chosen/rejected ÁöÑÂØπÊï∞Ê¶ÇÁéá\n",
    "   - ËÆ°ÁÆó DPO ÊçüÂ§±Ôºö\n",
    "     L = -log œÉ(Œ≤[(log œÄ_Œ∏(chosen|x) - log œÄ_Œ∏(rejected|x)) - \n",
    "                  (log œÄ_ref(chosen|x) - log œÄ_ref(rejected|x))])\n",
    "   - ‰ºòÂåñÁ≠ñÁï•Ê®°ÂûãÔºåÊúÄÂ§ßÂåñ chosen Ê¶ÇÁéáÔºåÊúÄÂ∞èÂåñ rejected Ê¶ÇÁéá\n",
    "   - ÈÄöËøáÈöêÂºè KL Á∫¶ÊùüÈò≤Ê≠¢ÂÅèÁ¶ªÂèÇËÄÉÊ®°Âûã\n",
    "\n",
    "4Ô∏è‚É£ „Äê‰øùÂ≠òÊ®°Âûã„Äë\n",
    "   - ‰øùÂ≠ò‰ºòÂåñÂêéÁöÑÁ≠ñÁï•Ê®°ÂûãÔºàÂèØ‰ª•ÊòØ LoRA ÊùÉÈáçÔºâ\n",
    "\n",
    "ÂÖ≥ÈîÆ‰ºòÂäøÔºö\n",
    "- ‚úÖ Êó†ÈúÄËÆ≠ÁªÉÂ•ñÂä±Ê®°ÂûãÔºàRMÔºâ\n",
    "- ‚úÖ Êó†ÈúÄÂº∫ÂåñÂ≠¶‰π†ÔºàPPOÔºâ\n",
    "- ‚úÖ ËÆ≠ÁªÉÁÆÄÂçïÁ®≥ÂÆöÔºàÁõëÁù£Â≠¶‰π†Ôºâ\n",
    "- ‚úÖ ËÆ≠ÁªÉÈÄüÂ∫¶Âø´ÔºàÁõ¥Êé•‰ºòÂåñÔºâ\n",
    "- ‚úÖ ÊòìÂ§çÁé∞ÔºàÁ°ÆÂÆöÊÄßËÆ≠ÁªÉÔºâ\n",
    "\n",
    "ÂÖ≥ÈîÆÂèÇÊï∞Ôºö\n",
    "- Œ≤ (beta)ÔºöÊ∏©Â∫¶ÂèÇÊï∞ÔºåÊéßÂà∂ KL Á∫¶ÊùüÂº∫Â∫¶ÔºàÂ∏∏Áî® 0.1-0.5Ôºâ\n",
    "  - Œ≤ Ë∂äÂ§ß ‚Üí Êõ¥Êé•ËøëÂèÇËÄÉÊ®°ÂûãÔºà‰øùÂÆàÔºâ\n",
    "  - Œ≤ Ë∂äÂ∞è ‚Üí Êõ¥ÂÅèÁ¶ªÂèÇËÄÉÊ®°ÂûãÔºàÊøÄËøõÔºâ\n",
    "\"\"\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ÂºÄÂßãËÆ≠ÁªÉÔºàÊ≥®ÈáäÊéâÔºåÁî®Êà∑ÂèØ‰ª•ÈÄâÊã©ÊòØÂê¶ÊâßË°åÔºâ\n",
    "print(\"\\n[DPO] ÂáÜÂ§áÂºÄÂßãËÆ≠ÁªÉ...\")\n",
    "print(\"[DPO] Ë¶ÅÂºÄÂßãËÆ≠ÁªÉÔºåËØ∑ÂèñÊ∂à‰∏ãÈù¢‰ª£Á†ÅÁöÑÊ≥®Èáä\")\n",
    "\n",
    "# ËÆ≠ÁªÉ‰ª£Á†ÅÔºàÊ≥®ÈáäÊéâÔºå‰∏çÂÆûÈôÖÊâßË°åÔºâ\n",
    "# dpo_trainer.train()\n",
    "\n",
    "# ‰øùÂ≠òÊ®°ÂûãÔºàÊ≥®ÈáäÊéâÔºâ\n",
    "# dpo_model_dir = os.path.join(dpo_output_dir, \"dpo_model\")\n",
    "# dpo_trainer.model.save_pretrained(dpo_model_dir)\n",
    "# tokenizer.save_pretrained(dpo_model_dir)\n",
    "# print(f\"[Done] DPO ËÆ≠ÁªÉÂÆåÊàêÔºåÊ®°ÂûãÂ∑≤‰øùÂ≠òËá≥: {dpo_model_dir}\")\n",
    "\n",
    "print(\"\\n[DPO] ËÆ≠ÁªÉ‰ª£Á†ÅÂ∑≤ÂáÜÂ§áÔºåÂèØ‰ª•ÂºÄÂßãËÆ≠ÁªÉ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e1dc02",
   "metadata": {},
   "source": [
    "## üîç KL Á∫¶ÊùüËØ¶Ëß£Ôºö‰ªÄ‰πàÊòØ KL Êï£Â∫¶Âíå KL Á∫¶ÊùüÔºü\n",
    "\n",
    "### üìö KL Êï£Â∫¶ÔºàKullback-Leibler DivergenceÔºâ\n",
    "\n",
    "**KL Êï£Â∫¶**Ôºà‰πüÁß∞‰∏∫Áõ∏ÂØπÁÜµÔºâÊòØË°°Èáè‰∏§‰∏™Ê¶ÇÁéáÂàÜÂ∏ÉÂ∑ÆÂºÇÁöÑÊåáÊ†á„ÄÇ\n",
    "\n",
    "#### Êï∞Â≠¶ÂÆö‰πâ\n",
    "\n",
    "ÂØπ‰∫é‰∏§‰∏™Ê¶ÇÁéáÂàÜÂ∏É P Âíå QÔºåKL Êï£Â∫¶ÂÆö‰πâ‰∏∫Ôºö\n",
    "\n",
    "\\[\n",
    "D_{KL}(P || Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)}\n",
    "\\]\n",
    "\n",
    "Âú®ËøûÁª≠ÊÉÖÂÜµ‰∏ãÔºö\n",
    "\n",
    "\\[\n",
    "D_{KL}(P || Q) = \\int_{-\\infty}^{\\infty} p(x) \\log \\frac{p(x)}{q(x)} dx\n",
    "\\]\n",
    "\n",
    "#### Áõ¥ËßÇÁêÜËß£\n",
    "\n",
    "- **KL Êï£Â∫¶ = 0**Ôºö‰∏§‰∏™ÂàÜÂ∏ÉÂÆåÂÖ®Áõ∏Âêå\n",
    "- **KL Êï£Â∫¶ > 0**Ôºö‰∏§‰∏™ÂàÜÂ∏É‰∏çÂêåÔºåÂÄºË∂äÂ§ßÂ∑ÆÂºÇË∂äÂ§ß\n",
    "- **KL Êï£Â∫¶ ‚â† ÂØπÁß∞**Ôºö\\(D_{KL}(P||Q) \\neq D_{KL}(Q||P)\\)\n",
    "\n",
    "#### Âú®ËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÂ∫îÁî®\n",
    "\n",
    "Âú® RLHF ‰∏≠ÔºåKL Êï£Â∫¶Ë°°ÈáèÁöÑÊòØÔºö\n",
    "- **Á≠ñÁï•Ê®°Âûã** \\(\\pi_\\theta\\)ÔºàËÆ≠ÁªÉ‰∏≠ÁöÑÊ®°ÂûãÔºâ\n",
    "- **ÂèÇËÄÉÊ®°Âûã** \\(\\pi_{ref}\\)ÔºàÂü∫ÂáÜÊ®°ÂûãÔºåÈÄöÂ∏∏ÊòØ SFT Ê®°ÂûãÔºâ\n",
    "\n",
    "ÁöÑÊ¶ÇÁéáÂàÜÂ∏ÉÂ∑ÆÂºÇÔºö\n",
    "\n",
    "\\[\n",
    "D_{KL}(\\pi_\\theta || \\pi_{ref}) = \\mathbb{E}_{x \\sim \\pi_\\theta} \\left[ \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)} \\right]\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ KL Á∫¶ÊùüÁöÑÁõÆÁöÑ\n",
    "\n",
    "#### ‰∏∫‰ªÄ‰πàÈúÄË¶Å KL Á∫¶ÊùüÔºü\n",
    "\n",
    "Âú® RLHF ËÆ≠ÁªÉ‰∏≠ÔºåÂ¶ÇÊûú**Âè™ÊúÄÂ§ßÂåñÂ•ñÂä±**ÔºåÊ®°ÂûãÂèØËÉΩ‰ºöÔºö\n",
    "\n",
    "1. **ËøáÂ∫¶‰ºòÂåñ**Ôºö‰∏∫‰∫ÜËé∑ÂæóÊõ¥È´òÂ•ñÂä±ÔºåÁîüÊàê‰∏çËá™ÁÑ∂ÁöÑÂõûÁ≠î\n",
    "2. **Ê®°ÂºèÂùçÁº©**ÔºöÂè™ÁîüÊàêÂ∞ëÊï∞Âá†ÁßçÈ´òÂàÜÂõûÁ≠îÔºåÂ§±ÂéªÂ§öÊ†∑ÊÄß\n",
    "3. **ÂÅèÁ¶ªÂü∫Á°ÄÊ®°Âûã**ÔºöÂÆåÂÖ®ÊîπÂèòÊ®°ÂûãË°å‰∏∫Ôºå‰∏¢Â§±ÂéüÊúâËÉΩÂäõ\n",
    "4. **ÈïøÂ∫¶ÂÅèÁΩÆ**ÔºöÁîüÊàêË∂ÖÈïøÂõûÁ≠îÔºàÂõ†‰∏∫Êõ¥ÈïøÂèØËÉΩËé∑ÂæóÊõ¥È´òÂ•ñÂä±Ôºâ\n",
    "\n",
    "**KL Á∫¶ÊùüÁöÑ‰ΩúÁî®**ÔºöÈò≤Ê≠¢Á≠ñÁï•Ê®°ÂûãÂÅèÁ¶ªÂèÇËÄÉÊ®°ÂûãÂ§™ËøúÔºå‰øùÊåÅÊ®°ÂûãÁöÑÔºö\n",
    "- ‚úÖ **Á®≥ÂÆöÊÄß**Ôºö‰∏ç‰ºö‰∫ßÁîüÊûÅÁ´ØÂèòÂåñ\n",
    "- ‚úÖ **Â§öÊ†∑ÊÄß**Ôºö‰øùÊåÅÁîüÊàêÂ§öÊ†∑ÊÄß\n",
    "- ‚úÖ **Âü∫Á°ÄËÉΩÂäõ**Ôºö‰øùÁïôÂèÇËÄÉÊ®°ÂûãÔºàÈÄöÂ∏∏ÊòØ SFTÔºâÁöÑËÉΩÂäõ\n",
    "\n",
    "---\n",
    "\n",
    "### üîß KL Á∫¶ÊùüÁöÑÂÆûÁé∞ÊñπÂºè\n",
    "\n",
    "#### 1Ô∏è‚É£ PPO ‰∏≠ÁöÑÊòæÂºè KL Á∫¶Êùü\n",
    "\n",
    "Âú® PPO ËÆ≠ÁªÉ‰∏≠ÔºåKL Á∫¶ÊùüÈÄöËøá**ÊòæÂºè KL ÊÉ©ÁΩö**ÂÆûÁé∞Ôºö\n",
    "\n",
    "**ÊçüÂ§±ÂáΩÊï∞**Ôºö\n",
    "\\[\n",
    "\\mathcal{L}_{PPO} = -\\mathcal{L}_{policy} + \\lambda_{KL} \\cdot D_{KL}(\\pi_\\theta || \\pi_{ref})\n",
    "\\]\n",
    "\n",
    "ÂÖ∂‰∏≠Ôºö\n",
    "- \\(\\mathcal{L}_{policy}\\)ÔºöÁ≠ñÁï•ÊçüÂ§±ÔºàÊúÄÂ§ßÂåñÂ•ñÂä±Ôºâ\n",
    "- \\(D_{KL}(\\pi_\\theta || \\pi_{ref})\\)ÔºöKL Êï£Â∫¶ÊÉ©ÁΩöÈ°π\n",
    "- \\(\\lambda_{KL}\\)ÔºöKL ÊÉ©ÁΩöÁ≥ªÊï∞Ôºà`kl_coef`Ôºâ\n",
    "\n",
    "**‰ª£Á†Å‰∏≠ÁöÑ‰ΩìÁé∞**Ôºö\n",
    "```python\n",
    "# PPO ÈÖçÁΩÆ‰∏≠ÁöÑ KL Á∫¶ÊùüÂèÇÊï∞\n",
    "ppo_config = PPOConfig(\n",
    "    init_kl_coef=0.1,      # KL ÊÉ©ÁΩöÁ≥ªÊï∞ÔºàŒªÔºâ\n",
    "    target=6.0,            # ÁõÆÊ†á KL Êï£Â∫¶ÂÄº\n",
    "    # ...\n",
    ")\n",
    "```\n",
    "\n",
    "**Âä®ÊÄÅË∞ÉÊï¥**Ôºö\n",
    "- Â¶ÇÊûú KL Êï£Â∫¶ > ÁõÆÊ†áÂÄº ‚Üí Â¢ûÂä†ÊÉ©ÁΩöÔºàÂ¢ûÂä† `kl_coef`Ôºâ\n",
    "- Â¶ÇÊûú KL Êï£Â∫¶ < ÁõÆÊ†áÂÄº ‚Üí ÂáèÂ∞ëÊÉ©ÁΩöÔºàÂáèÂ∞ë `kl_coef`Ôºâ\n",
    "- Á°Æ‰øùÁ≠ñÁï•‰∏ç‰ºöÂÅèÁ¶ªÂèÇËÄÉÊ®°ÂûãÂ§™Ëøú\n",
    "\n",
    "---\n",
    "\n",
    "#### 2Ô∏è‚É£ DPO ‰∏≠ÁöÑÈöêÂºè KL Á∫¶Êùü\n",
    "\n",
    "Âú® DPO ËÆ≠ÁªÉ‰∏≠ÔºåKL Á∫¶ÊùüÈÄöËøá**ÂèÇËÄÉÊ®°ÂûãÁöÑÂØπÊï∞Ê¶ÇÁéáÂ∑Æ**ÈöêÂºèÂÆûÁé∞Ôºö\n",
    "\n",
    "**ÊçüÂ§±ÂáΩÊï∞**Ôºö\n",
    "\\[\n",
    "\\mathcal{L}_{DPO} = -\\log \\sigma\\left(\\beta\\left[\n",
    "  (\\log \\pi_\\theta(y_{pos}|x) - \\log \\pi_\\theta(y_{neg}|x)) - \n",
    "  (\\log \\pi_{ref}(y_{pos}|x) - \\log \\pi_{ref}(y_{neg}|x))\n",
    "\\right]\\right)\n",
    "\\]\n",
    "\n",
    "**ÈöêÂºè KL Á∫¶Êùü**Ôºö\n",
    "- ÈÄöËøáÂèÇËÄÉÊ®°ÂûãÁöÑÂØπÊï∞Ê¶ÇÁéáÂ∑Æ \\(\\log \\pi_{ref}(y_{pos}|x) - \\log \\pi_{ref}(y_{neg}|x)\\) ÂÆûÁé∞\n",
    "- **‰∏çÈúÄË¶ÅÊòæÂºèËÆ°ÁÆó KL Êï£Â∫¶**\n",
    "- **beta (Œ≤)** ÂèÇÊï∞ÊéßÂà∂Á∫¶ÊùüÂº∫Â∫¶ÔºàÁõ∏ÂΩì‰∫é PPO ‰∏≠ÁöÑ `kl_coef`Ôºâ\n",
    "\n",
    "**‰ª£Á†Å‰∏≠ÁöÑ‰ΩìÁé∞**Ôºö\n",
    "```python\n",
    "# DPO ÈÖçÁΩÆ‰∏≠ÁöÑ KL Á∫¶ÊùüÂèÇÊï∞ÔºàÈöêÂºèÔºâ\n",
    "dpo_config = DPOConfig(\n",
    "    beta=0.1,              # Ê∏©Â∫¶ÂèÇÊï∞ÔºàÊéßÂà∂ KL Á∫¶ÊùüÂº∫Â∫¶Ôºâ\n",
    "    # ...\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìä PPO vs DPO ÁöÑ KL Á∫¶ÊùüÂØπÊØî\n",
    "\n",
    "| ÁâπÊÄß | **PPOÔºàÊòæÂºè KL Á∫¶ÊùüÔºâ** | **DPOÔºàÈöêÂºè KL Á∫¶ÊùüÔºâ** |\n",
    "|------|----------------------|----------------------|\n",
    "| **ËÆ°ÁÆóÊñπÂºè** | ÊòæÂºèËÆ°ÁÆó KL Êï£Â∫¶ | ÈÄöËøáÂèÇËÄÉÊ®°ÂûãÂØπÊï∞Ê¶ÇÁéáÂ∑ÆÈöêÂºèÂÆûÁé∞ |\n",
    "| **Á∫¶ÊùüÂèÇÊï∞** | `kl_coef`ÔºàKL ÊÉ©ÁΩöÁ≥ªÊï∞Ôºâ | `beta`ÔºàÊ∏©Â∫¶ÂèÇÊï∞Ôºâ |\n",
    "| **ÂÆûÁé∞Â§çÊùÇÂ∫¶** | ÈúÄË¶ÅËÆ°ÁÆóÂÆåÊï¥ÁöÑ KL Êï£Â∫¶ | Âè™ÈúÄËÆ°ÁÆóÂØπÊï∞Ê¶ÇÁéáÂ∑Æ |\n",
    "| **Âä®ÊÄÅË∞ÉÊï¥** | Ê†πÊçÆ KL Êï£Â∫¶Âä®ÊÄÅË∞ÉÊï¥ÊÉ©ÁΩö | Âõ∫ÂÆö betaÔºåÊó†ÈúÄÂä®ÊÄÅË∞ÉÊï¥ |\n",
    "| **ËÆ°ÁÆóÊàêÊú¨** | ËæÉÈ´òÔºàÈúÄË¶ÅËÆ°ÁÆó KL Êï£Â∫¶Ôºâ | ËæÉ‰ΩéÔºàÂè™ÈúÄËÆ°ÁÆóÂØπÊï∞Ê¶ÇÁéáÔºâ |\n",
    "\n",
    "---\n",
    "\n",
    "### üîë ÂÖ≥ÈîÆÂèÇÊï∞ÁêÜËß£\n",
    "\n",
    "#### PPO ‰∏≠ÁöÑ KL ÂèÇÊï∞\n",
    "\n",
    "1. **`init_kl_coef`ÔºàÂàùÂßã KL ÊÉ©ÁΩöÁ≥ªÊï∞Ôºâ**\n",
    "   - **Âê´‰πâ**ÔºöÂàùÂßãÁöÑ KL ÊÉ©ÁΩöÊùÉÈáç \\(\\lambda_{KL}\\)\n",
    "   - **ËåÉÂõ¥**ÔºöÈÄöÂ∏∏ 0.01 - 0.5\n",
    "   - **‰ΩúÁî®**ÔºöÂπ≥Ë°°Â•ñÂä±ÊúÄÂ§ßÂåñ‰∏é KL Á∫¶Êùü\n",
    "   - **ÂΩ±Âìç**Ôºö\n",
    "     - ÂÄºË∂äÂ§ß ‚Üí Êõ¥‰øùÂÆàÔºåÊõ¥Êé•ËøëÂèÇËÄÉÊ®°ÂûãÔºà‰ΩÜÂèØËÉΩÁâ∫Áâ≤Â•ñÂä±Ôºâ\n",
    "     - ÂÄºË∂äÂ∞è ‚Üí Êõ¥ÊøÄËøõÔºåÊõ¥ÂÆπÊòìÂÅèÁ¶ªÂèÇËÄÉÊ®°ÂûãÔºà‰ΩÜÂèØËÉΩÊ®°ÂºèÂùçÁº©Ôºâ\n",
    "\n",
    "2. **`target`ÔºàÁõÆÊ†á KL Êï£Â∫¶Ôºâ**\n",
    "   - **Âê´‰πâ**ÔºöÊúüÊúõÁöÑ KL Êï£Â∫¶ÁõÆÊ†áÂÄº\n",
    "   - **ËåÉÂõ¥**ÔºöÈÄöÂ∏∏ 2.0 - 10.0\n",
    "   - **‰ΩúÁî®**ÔºöÊéßÂà∂Á≠ñÁï•‰∏éÂèÇËÄÉÊ®°ÂûãÁöÑÂÅèÁ¶ªÁ®ãÂ∫¶\n",
    "   - **ÂΩ±Âìç**Ôºö\n",
    "     - ÂÄºË∂äÂ§ß ‚Üí ÂÖÅËÆ∏Êõ¥Â§ßÂÅèÁ¶ªÔºà‰ΩÜÂèØËÉΩËøáÂ∫¶‰ºòÂåñÔºâ\n",
    "     - ÂÄºË∂äÂ∞è ‚Üí Ë¶ÅÊ±ÇÊõ¥Êé•ËøëÂèÇËÄÉÊ®°ÂûãÔºà‰ΩÜÂèØËÉΩÊõ¥Êñ∞Â§™ÊÖ¢Ôºâ\n",
    "\n",
    "#### DPO ‰∏≠ÁöÑ KL ÂèÇÊï∞\n",
    "\n",
    "1. **`beta`ÔºàÊ∏©Â∫¶ÂèÇÊï∞Ôºâ**\n",
    "   - **Âê´‰πâ**ÔºöÊéßÂà∂ KL Á∫¶ÊùüÂº∫Â∫¶ÁöÑÊ∏©Â∫¶ÂèÇÊï∞\n",
    "   - **ËåÉÂõ¥**ÔºöÈÄöÂ∏∏ 0.1 - 0.5\n",
    "   - **‰ΩúÁî®**ÔºöÈöêÂºèÊéßÂà∂Á≠ñÁï•‰∏éÂèÇËÄÉÊ®°ÂûãÁöÑÂÅèÁ¶ªÁ®ãÂ∫¶\n",
    "   - **ÂΩ±Âìç**Ôºö\n",
    "     - ÂÄºË∂äÂ§ß ‚Üí Êõ¥‰øùÂÆàÔºåÊõ¥Êé•ËøëÂèÇËÄÉÊ®°Âûã\n",
    "     - ÂÄºË∂äÂ∞è ‚Üí Êõ¥ÊøÄËøõÔºåÊõ¥ÂÆπÊòìÂÅèÁ¶ªÂèÇËÄÉÊ®°Âûã\n",
    "\n",
    "---\n",
    "\n",
    "### üí° ÂÆûÈôÖÂ∫îÁî®Âª∫ËÆÆ\n",
    "\n",
    "#### Â¶Ç‰ΩïÈÄâÊã© KL ÂèÇÊï∞Ôºü\n",
    "\n",
    "1. **PPO ‰∏≠ÁöÑ `kl_coef`**Ôºö\n",
    "   - **ÂàùÂßãÂÄº**Ôºö‰ªé 0.1 ÂºÄÂßã\n",
    "   - **ËßÇÂØü**ÔºöÁõëÊéß KL Êï£Â∫¶ÊòØÂê¶Á®≥ÂÆö\n",
    "   - **Ë∞ÉÊï¥**Ôºö\n",
    "     - KL Êï£Â∫¶Â§™Â§ß ‚Üí Â¢ûÂä† `kl_coef`\n",
    "     - KL Êï£Â∫¶Â§™Â∞è ‚Üí ÂáèÂ∞ë `kl_coef`\n",
    "   - **Âπ≥Ë°°**ÔºöÂú®Â•ñÂä±ÊúÄÂ§ßÂåñ‰∏é KL Á∫¶Êùü‰πãÈó¥ÊâæÂà∞Âπ≥Ë°°\n",
    "\n",
    "2. **DPO ‰∏≠ÁöÑ `beta`**Ôºö\n",
    "   - **ÂàùÂßãÂÄº**Ôºö‰ªé 0.1 ÂºÄÂßã\n",
    "   - **ËßÇÂØü**ÔºöÁõëÊéß chosen/rejected ÁöÑÊ¶ÇÁéáÂ∑ÆÂºÇ\n",
    "   - **Ë∞ÉÊï¥**Ôºö\n",
    "     - Ê®°ÂûãÂ§™‰øùÂÆà ‚Üí ÂáèÂ∞è `beta`\n",
    "     - Ê®°ÂûãÂÅèÁ¶ªÂ§™Ëøú ‚Üí Â¢ûÂ§ß `beta`\n",
    "   - **Âπ≥Ë°°**ÔºöÂú®ÂÅèÂ•Ω‰ºòÂåñ‰∏éÊ®°ÂûãÁ®≥ÂÆöÊÄß‰πãÈó¥ÊâæÂà∞Âπ≥Ë°°\n",
    "\n",
    "#### Â∏∏ËßÅÈóÆÈ¢ò\n",
    "\n",
    "1. **KL Êï£Â∫¶Â§™Â§ßÊÄé‰πàÂäûÔºü**\n",
    "   - Â¢ûÂä† `kl_coef`ÔºàPPOÔºâÊàñ `beta`ÔºàDPOÔºâ\n",
    "   - Èôç‰ΩéÂ≠¶‰π†Áéá\n",
    "   - Â¢ûÂä†ÁõÆÊ†á KL Êï£Â∫¶ÂÄºÔºàPPOÔºâ\n",
    "\n",
    "2. **KL Êï£Â∫¶Â§™Â∞èÊÄé‰πàÂäûÔºü**\n",
    "   - ÂáèÂ∞ë `kl_coef`ÔºàPPOÔºâÊàñ `beta`ÔºàDPOÔºâ\n",
    "   - Â¢ûÂä†Â≠¶‰π†Áéá\n",
    "   - ÂáèÂ∞ëÁõÆÊ†á KL Êï£Â∫¶ÂÄºÔºàPPOÔºâ\n",
    "\n",
    "3. **Â¶Ç‰ΩïÁõëÊéß KL Êï£Â∫¶Ôºü**\n",
    "   - Âú®ËÆ≠ÁªÉÊó•Âøó‰∏≠ËßÇÂØü KL Êï£Â∫¶ÂÄº\n",
    "   - Á°Æ‰øù KL Êï£Â∫¶Á®≥ÂÆöÔºà‰∏çË¶ÅÂâßÁÉàÊ≥¢Âä®Ôºâ\n",
    "   - Â¶ÇÊûú KL Êï£Â∫¶ÊåÅÁª≠Â¢ûÂ§ßÔºåËØ¥ÊòéÊ®°ÂûãÂú®ÂÅèÁ¶ªÂèÇËÄÉÊ®°Âûã\n",
    "\n",
    "---\n",
    "\n",
    "### üéì ÊÄªÁªì\n",
    "\n",
    "**KL Á∫¶ÊùüÁöÑÊ†∏ÂøÉÊÄùÊÉ≥**Ôºö\n",
    "- **ÁõÆÊ†á**ÔºöÂú®‰ºòÂåñÂ•ñÂä±ÁöÑÂêåÊó∂ÔºåÈò≤Ê≠¢Ê®°ÂûãÂÅèÁ¶ªÂèÇËÄÉÊ®°ÂûãÂ§™Ëøú\n",
    "- **ÊñπÂºè**ÔºöÈÄöËøá KL Êï£Â∫¶ÊÉ©ÁΩöÔºàPPOÔºâÊàñÈöêÂºèÁ∫¶ÊùüÔºàDPOÔºâÂÆûÁé∞\n",
    "- **‰ΩúÁî®**Ôºö‰øùÊåÅÊ®°ÂûãÁ®≥ÂÆöÊÄß„ÄÅÂ§öÊ†∑ÊÄßÔºå‰øùÁïôÂü∫Á°ÄËÉΩÂäõ\n",
    "- **ÂèÇÊï∞**Ôºö`kl_coef`ÔºàPPOÔºâÊàñ `beta`ÔºàDPOÔºâÊéßÂà∂Á∫¶ÊùüÂº∫Â∫¶\n",
    "\n",
    "**ÂÖ≥ÈîÆË¶ÅÁÇπ**Ôºö\n",
    "- ‚úÖ KL Á∫¶ÊùüÊòØ RLHF ËÆ≠ÁªÉ‰∏≠ÁöÑÂÖ≥ÈîÆÊú∫Âà∂\n",
    "- ‚úÖ PPO ‰ΩøÁî®ÊòæÂºè KL Á∫¶ÊùüÔºàËÆ°ÁÆó KL Êï£Â∫¶Ôºâ\n",
    "- ‚úÖ DPO ‰ΩøÁî®ÈöêÂºè KL Á∫¶ÊùüÔºàÈÄöËøáÂèÇËÄÉÊ®°ÂûãÂØπÊï∞Ê¶ÇÁéáÂ∑ÆÔºâ\n",
    "- ‚úÖ ÂèÇÊï∞Ë∞É‰ºòÂæàÈáçË¶ÅÔºåÈúÄË¶ÅÂπ≥Ë°°Â•ñÂä±‰∏éÁ∫¶Êùü\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74162724",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86705062",
   "metadata": {},
   "source": [
    "## üìä DPO vs RM+PPO ÂÆåÊï¥ÂØπÊØî\n",
    "\n",
    "### üîÑ ÊµÅÁ®ãÂØπÊØî\n",
    "\n",
    "```\n",
    "‰º†Áªü RLHFÔºàRM + PPOÔºâÊµÅÁ®ãÔºö\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  1Ô∏è‚É£ SFTÔºàÁõëÁù£ÂæÆË∞ÉÔºâ                        ‚îÇ\n",
    "‚îÇ     ‚Üì                                    ‚îÇ\n",
    "‚îÇ  2Ô∏è‚É£ RMÔºàÂ•ñÂä±Ê®°ÂûãËÆ≠ÁªÉÔºâ                     ‚îÇ\n",
    "‚îÇ     ‚îú‚îÄ‚îÄ ËÆ≠ÁªÉÂ•ñÂä±Â§¥                        ‚îÇ\n",
    "‚îÇ     ‚îî‚îÄ‚îÄ Â≠¶‰ºöÊâìÂàÜÔºàchosen > rejectedÔºâ      ‚îÇ\n",
    "‚îÇ     ‚Üì                                    ‚îÇ\n",
    "‚îÇ  3Ô∏è‚É£ PPOÔºàÂº∫Âåñ‰ºòÂåñÔºâ                        ‚îÇ\n",
    "‚îÇ     ‚îú‚îÄ‚îÄ Á≠ñÁï•Ê®°ÂûãÁîüÊàêÂõûÁ≠î                   ‚îÇ\n",
    "‚îÇ     ‚îú‚îÄ‚îÄ Â•ñÂä±Ê®°ÂûãÊâìÂàÜ                       ‚îÇ\n",
    "‚îÇ     ‚îú‚îÄ‚îÄ PPO ‰ºòÂåñÁ≠ñÁï•                      ‚îÇ\n",
    "‚îÇ     ‚îî‚îÄ‚îÄ ÂÄºÂáΩÊï∞Â§¥ÔºàÁî®‰∫é‰ºòÂäø‰º∞ËÆ°Ôºâ            ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "DPO ÊµÅÁ®ãÔºö\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  1Ô∏è‚É£ SFTÔºàÁõëÁù£ÂæÆË∞ÉÔºâ                        ‚îÇ\n",
    "‚îÇ     ‚Üì                                    ‚îÇ\n",
    "‚îÇ  2Ô∏è‚É£ DPOÔºàÁõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñÔºâ                    ‚îÇ\n",
    "‚îÇ     ‚îú‚îÄ‚îÄ Áõ¥Êé•Áî®ÂÅèÂ•ΩÊï∞ÊçÆËÆ≠ÁªÉ                ‚îÇ\n",
    "‚îÇ     ‚îú‚îÄ‚îÄ ÊúÄÂ§ßÂåñ chosen Ê¶ÇÁéá               ‚îÇ\n",
    "‚îÇ     ‚îú‚îÄ‚îÄ ÊúÄÂ∞èÂåñ rejected Ê¶ÇÁéá              ‚îÇ\n",
    "‚îÇ     ‚îî‚îÄ‚îÄ ÈöêÂºè KL Á∫¶ÊùüÔºàÈÄöËøáÂèÇËÄÉÊ®°ÂûãÔºâ       ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### üéØ ÂÖ≥ÈîÆÂå∫Âà´\n",
    "\n",
    "| ÁâπÊÄß | **RM + PPO** | **DPO** |\n",
    "|------|------------|---------|\n",
    "| **ËÆ≠ÁªÉÈò∂ÊÆµ** | 3 Èò∂ÊÆµÔºàSFT ‚Üí RM ‚Üí PPOÔºâ | 2 Èò∂ÊÆµÔºàSFT ‚Üí DPOÔºâ |\n",
    "| **ÈúÄË¶ÅÂ•ñÂä±Ê®°Âûã** | ‚úÖ ÊòØ | ‚ùå Âê¶ |\n",
    "| **ÈúÄË¶ÅÂº∫ÂåñÂ≠¶‰π†** | ‚úÖ ÊòØÔºàPPOÔºâ | ‚ùå Âê¶ |\n",
    "| **ÈúÄË¶ÅÂÄºÂáΩÊï∞Â§¥** | ‚úÖ ÊòØÔºàÁî®‰∫é PPOÔºâ | ‚ùå Âê¶ |\n",
    "| **KL Á∫¶ÊùüÊñπÂºè** | ÊòæÂºè KL ÊÉ©ÁΩö | ÈöêÂºè KL Á∫¶ÊùüÔºàÈÄöËøáÂèÇËÄÉÊ®°ÂûãÔºâ |\n",
    "| **ËÆ≠ÁªÉÂ§çÊùÇÂ∫¶** | È´òÔºàRL ÂõûË∑ØÔºâ | ‰ΩéÔºàÁõëÁù£Â≠¶‰π†Ôºâ |\n",
    "| **ËÆ≠ÁªÉÁ®≥ÂÆöÊÄß** | ‰∏≠Á≠âÔºàRL ‰∏çÁ®≥ÂÆöÔºâ | È´òÔºàÁõëÁù£Â≠¶‰π†Ôºâ |\n",
    "| **ËÆ≠ÁªÉÈÄüÂ∫¶** | ÊÖ¢ÔºàÁîüÊàê+ÊâìÂàÜÂæ™ÁéØÔºâ | Âø´ÔºàÁõ¥Êé•‰ºòÂåñÔºâ |\n",
    "| **Êï∞ÊçÆÈúÄÊ±Ç** | PromptsÔºàÁîüÊàêÊó∂Áî®Ôºâ | ÂÅèÂ•ΩÂØπÔºàchosen/rejectedÔºâ |\n",
    "| **ËµÑÊ∫êÈúÄÊ±Ç** | È´òÔºàÈúÄË¶Å RMÔºâ | ‰ΩéÔºàÊó†ÈúÄ RMÔºâ |\n",
    "| **ÈÄÇÁî®Âú∫ÊôØ** | ÈúÄË¶ÅÁÅµÊ¥ªÂ•ñÂä±ÊéßÂà∂ | Âõ∫ÂÆöÂÅèÂ•ΩÊï∞ÊçÆ |\n",
    "\n",
    "### üì¶ ÊùÉÈáçÂ≠òÂÇ®ÂØπÊØî\n",
    "\n",
    "| Èò∂ÊÆµ | **RM + PPO** | **DPO** |\n",
    "|------|------------|---------|\n",
    "| **SFT** | LoRA ÊùÉÈáç | LoRA ÊùÉÈáç |\n",
    "| **RM** | Âü∫Á°ÄÊ®°Âûã + Â•ñÂä±Â§¥ | ‚ùå ‰∏çÈúÄË¶Å |\n",
    "| **PPO** | Á≠ñÁï•Ê®°Âûã + ÂÄºÂáΩÊï∞Â§¥ | ‚ùå ‰∏çÈúÄË¶Å |\n",
    "| **DPO** | ‚ùå ‰∏çÈúÄË¶Å | Á≠ñÁï•Ê®°ÂûãÔºàÂèØ‰ª•ÊòØ LoRAÔºâ |\n",
    "\n",
    "### ‚úÖ ‰ΩïÊó∂‰ΩøÁî® DPO\n",
    "\n",
    "**ÈÄÇÂêà‰ΩøÁî® DPO ÁöÑÂú∫ÊôØ**Ôºö\n",
    "- ‚úÖ ÊúâÈ´òË¥®ÈáèÂÅèÂ•ΩÊï∞ÊçÆÔºàchosen/rejected ÂØπÔºâ\n",
    "- ‚úÖ ‰∏çÈúÄË¶ÅÂä®ÊÄÅÂ•ñÂä±Ë∞ÉÊï¥\n",
    "- ‚úÖ ËøΩÊ±ÇËÆ≠ÁªÉÁÆÄÂçïÊÄßÂíåÁ®≥ÂÆöÊÄß\n",
    "- ‚úÖ ËµÑÊ∫êÂèóÈôêÔºà‰∏çÊÉ≥ËÆ≠ÁªÉÂ•ñÂä±Ê®°ÂûãÔºâ\n",
    "- ‚úÖ ÈúÄË¶ÅÂø´ÈÄüËø≠‰ª£\n",
    "\n",
    "**ÈÄÇÂêà‰ΩøÁî® RM+PPO ÁöÑÂú∫ÊôØ**Ôºö\n",
    "- ‚úÖ ÈúÄË¶ÅÂä®ÊÄÅÂ•ñÂä±Ë∞ÉÊï¥ÔºàÂú®Á∫øÂ≠¶‰π†Ôºâ\n",
    "- ‚úÖ ÈúÄË¶ÅÁÅµÊ¥ªÁöÑÂ•ñÂä±ÊéßÂà∂\n",
    "- ‚úÖ ÂÅèÂ•ΩÊï∞ÊçÆËæÉÂ∞ëÔºå‰ΩÜÊúâÂæàÂ§ö prompts\n",
    "- ‚úÖ ÈúÄË¶ÅÁ≤æÁªÜÁöÑ RL ÊéßÂà∂\n",
    "\n",
    "### üîë DPO ÁöÑÂÖ≥ÈîÆÂèÇÊï∞\n",
    "\n",
    "1. **beta (Œ≤)**ÔºöÊ∏©Â∫¶ÂèÇÊï∞ÔºåÊéßÂà∂ KL Á∫¶ÊùüÂº∫Â∫¶\n",
    "   - **ËåÉÂõ¥**Ôºö0.1 - 0.5ÔºàÂ∏∏Áî®Ôºâ\n",
    "   - **Œ≤ Ë∂äÂ§ß**ÔºöÊõ¥‰øùÂÆàÔºåÊõ¥Êé•ËøëÂèÇËÄÉÊ®°Âûã\n",
    "   - **Œ≤ Ë∂äÂ∞è**ÔºöÊõ¥ÊøÄËøõÔºåÊõ¥ÂÆπÊòìÂÅèÁ¶ªÂèÇËÄÉÊ®°Âûã\n",
    "   - **Êé®ËçêÂÄº**Ôºö0.1ÔºàÈªòËÆ§ÔºâÔºåÊ†πÊçÆÂÆûÈôÖÊïàÊûúË∞ÉÊï¥\n",
    "\n",
    "2. **ÂèÇËÄÉÊ®°Âûã**ÔºöÁî®‰∫é KL Á∫¶ÊùüÁöÑÂü∫ÂáÜÊ®°Âûã\n",
    "   - **ÈÄöÂ∏∏ÊòØ**ÔºöSFT Ê®°ÂûãÔºàÂÜªÁªìÂèÇÊï∞Ôºâ\n",
    "   - **‰ΩúÁî®**ÔºöÈò≤Ê≠¢Á≠ñÁï•Ê®°ÂûãÂÅèÁ¶ªÂ§™Ëøú\n",
    "   - **ÂÖ≥ÈîÆ**ÔºöÂèÇËÄÉÊ®°ÂûãÂøÖÈ°ª‰∏éÁ≠ñÁï•Ê®°ÂûãÂàùÂßãÂåñÁõ∏Âêå\n",
    "\n",
    "3. **ÊçüÂ§±Á±ªÂûã**Ôºö\n",
    "   - **sigmoid**ÔºöÊ†áÂáÜÁöÑ sigmoid ÊçüÂ§±ÔºàÊé®ËçêÔºâ\n",
    "   - **hinge**Ôºöhinge ÊçüÂ§±ÔºàÂú®Êüê‰∫õÂú∫ÊôØ‰∏ãÊõ¥Á®≥ÂÆöÔºâ\n",
    "\n",
    "### üí° DPO ÂÆûË∑µÂª∫ËÆÆ\n",
    "\n",
    "1. **Êï∞ÊçÆË¥®Èáè**ÔºöÂÅèÂ•ΩÊï∞ÊçÆË¥®ÈáèÁõ¥Êé•ÂΩ±Âìç DPO ÊïàÊûú\n",
    "2. **beta Ë∞É‰ºò**ÔºöÊ†πÊçÆÂÆûÈôÖÊÉÖÂÜµË∞ÉÊï¥ beta ÂèÇÊï∞\n",
    "3. **ÂèÇËÄÉÊ®°Âûã**ÔºöÁ°Æ‰øùÂèÇËÄÉÊ®°Âûã‰∏éÁ≠ñÁï•Ê®°ÂûãÂàùÂßãÂåñ‰∏ÄËá¥\n",
    "4. **ËØÑ‰º∞ÊåáÊ†á**ÔºöÁõëÊéß chosen/rejected ÁöÑÊ¶ÇÁéáÂ∑ÆÂºÇ\n",
    "5. **Èò≤Ê≠¢ËøáÊãüÂêà**Ôºö‰ΩøÁî®È™åËØÅÈõÜÂÆöÊúüËØÑ‰º∞\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b952911c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
