{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5a11e90",
   "metadata": {},
   "source": [
    "## RLHFï¼ˆReinforcement Learning from Human Feedbackï¼‰\n",
    "\n",
    "- **ç›®æ ‡**ï¼šè®©æ¨¡å‹æ›´ç¬¦åˆäººç±»æ„å›¾ã€æ›´å®‰å…¨ã€æ›´æœ‰ç”¨\n",
    "- **æ ¸å¿ƒæ€æƒ³**ï¼š\n",
    "  - ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ•™ä¼šæ¨¡å‹åŸºæœ¬çš„æŒ‡ä»¤è·Ÿéš\n",
    "  - ç”¨åå¥½æ•°æ®è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ï¼Œå­¦ä¼šæ‰“åˆ†â€œæ›´å¥½/æ›´å·®â€çš„å›ç­”\n",
    "  - ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆPPOï¼‰åœ¨å¥–åŠ±ä¿¡å·ä¸‹ä¼˜åŒ–ç­–ç•¥ï¼Œæƒè¡¡è´¨é‡ã€ç¨³å®šæ€§ä¸å¤šæ ·æ€§\n",
    "- **å…³é”®ç»„ä»¶**ï¼šæŒ‡ä»¤æ•°æ®ã€åå¥½æ•°æ®ï¼ˆA/B å¯¹æ¯”ï¼‰ã€å¥–åŠ±æ¨¡å‹ã€å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€KL çº¦æŸ/å‚è€ƒç­–ç•¥\n",
    "- **å…¸å‹äº§ç‰©**ï¼š\n",
    "  - SFT æ¨¡å‹ï¼ˆä¼šåšäº‹ï¼‰\n",
    "  - RM å¥–åŠ±æ¨¡å‹ï¼ˆä¼šæ‰“åˆ†ï¼‰\n",
    "  - PPO åçš„å¯¹é½æ¨¡å‹ï¼ˆåšå¾—æ›´å¥½ï¼‰\n",
    "  - DPOï¼ˆå–ä»£ RM+PPO çš„ç›´æ¥åå¥½ä¼˜åŒ–ï¼‰\n",
    "\n",
    "### RLHF çš„ä¸‰é˜¶æ®µæµç¨‹ï¼ˆå·¥ç¨‹åŒ–è§†è§’ï¼‰\n",
    "\n",
    "| é˜¶æ®µ | åç§° | ä½œç”¨ | æŠ€æœ¯ |\n",
    "|---|---|---|---|\n",
    "| 1ï¸âƒ£ | SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰ | æ•™æ¨¡å‹æ‰§è¡ŒæŒ‡ä»¤ | CrossEntropyLoss |\n",
    "| 2ï¸âƒ£ | å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰è®­ç»ƒ | å­¦ä¼šâ€œä»€ä¹ˆæ ·çš„å›ç­”æ›´å¥½â€ | Pairwise ranking (A > B) |\n",
    "| 3ï¸âƒ£ | PPO å¼ºåŒ–ä¼˜åŒ– | ç”¨å¥–åŠ±ä¿¡å·ä¼˜åŒ–ç”Ÿæˆç­–ç•¥ | PPO ç®—æ³•ï¼ˆPolicy Gradientï¼‰ |\n",
    "\n",
    "### å®éªŒè®¾ç½®ï¼šæ¨¡å‹ä¸æ•°æ®é›†é€‰æ‹©\n",
    "- **æ¨¡å‹**ï¼šQwen2.5-1.5B-Instructï¼ˆä¸­æ–‡æŒ‡ä»¤èƒ½åŠ›å¼ºï¼Œå°å‚æ•°ã€æ˜“äº LoRA/QLoRAï¼‰\n",
    "- **SFT æ•°æ®**ï¼šBelleGroup/train_0.5M_CNï¼ˆä¸­æ–‡æŒ‡ä»¤-å›ç­”å¯¹ï¼Œä½“é‡é€‚ä¸­ï¼Œå¯é‡‡æ ·ï¼‰\n",
    "- **åå¥½æ•°æ®ï¼ˆç”¨äº DPO/RMï¼‰**ï¼šargilla/ultrafeedback-binarized-preferencesï¼ˆæˆå¯¹åå¥½ï¼Œæ˜“ç›´æ¥ç”¨äº DPOï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3104c69a",
   "metadata": {},
   "source": [
    "## ç¯å¢ƒå®‰è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b3c68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…éœ€è¦çš„åº“\n",
    "%env TOKENIZERS_PARALLELISM=false\n",
    "%pip install -q torch torchvision torchaudio\n",
    "%pip install -q \"transformers>=4.44.0\" \"datasets>=2.18.0\" \"accelerate>=0.33.0\" \\\n",
    "                \"peft>=0.12.0\" \"trl>=0.9.6\" \"sentencepiece>=0.1.99\" \"safetensors>=0.4.5\" \\\n",
    "                \"huggingface_hub>=0.24.0\" \"modelscope>=1.14.0\" \"protobuf>=4.25.0\" \\\n",
    "                \"numpy>=1.24.0\" \"scipy>=1.10.0\" \"tiktoken>=0.7.0\" \n",
    "\n",
    "# ä»…åœ¨ CUDA å¯ç”¨æ—¶å®‰è£… bitsandbytesï¼ˆå¯é€‰ï¼‰\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    %pip install -q \"bitsandbytes>=0.43.0\"\n",
    "\n",
    "# æ‰“å°å…³é”®ç‰ˆæœ¬ï¼Œä¾¿äºæ’æŸ¥\n",
    "import importlib.metadata as im\n",
    "v = lambda n: (im.version(n) if n in {d.metadata['Name'] for d in im.distributions()} else 'N/A')\n",
    "print(\"[Versions]\",\n",
    "      \"torch=\", v(\"torch\"),\n",
    "      \"transformers=\", v(\"transformers\"),\n",
    "      \"datasets=\", v(\"datasets\"),\n",
    "      \"accelerate=\", v(\"accelerate\"),\n",
    "      \"peft=\", v(\"peft\"),\n",
    "      \"trl=\", v(\"trl\"),\n",
    "      \"modelscope=\", v(\"modelscope\"),\n",
    "      \"sentencepiece=\", v(\"sentencepiece\"),\n",
    "      \"bitsandbytes=\", v(\"bitsandbytes\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a578b03c",
   "metadata": {},
   "source": [
    "## æ¨¡å‹ä¸‹è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a203c0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸‹è½½Qwen/Qwen2.5-1.5B-Instruct\n",
    "import torch\n",
    "from modelscope.hub.snapshot_download import snapshot_download\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"  # ModelScope ä¸Šçš„æ¨¡å‹æ ‡è¯†ï¼ˆå…¬å…±å¯ç›´æ¥ä¸‹è½½ï¼‰\n",
    "\n",
    "# é€‰æ‹©è®¾å¤‡\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# é€šè¿‡ ModelScope ä¸‹è½½åˆ°æœ¬åœ°ç¼“å­˜ï¼Œç„¶åç”¨ Transformers ä»æœ¬åœ°ç›®å½•åŠ è½½\n",
    "model_dir = snapshot_download(model_id)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir, trust_remote_code=True)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"[Device] {device}\")\n",
    "\n",
    "# ç®€å•è‡ªæ£€\n",
    "txt = \"ä½ å¥½ï¼Œç®€è¦ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚\"\n",
    "inputs = tokenizer(txt, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "    out = model.generate(**inputs, max_new_tokens=64, do_sample=False)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8784880",
   "metadata": {},
   "source": [
    "## æ•°æ®é›†ä¸‹è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb0edcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸‹è½½sftæ•°æ®é›†å’Œåå¥½æ•°æ®é›†\n",
    "from modelscope.hub.snapshot_download import snapshot_download\n",
    "\n",
    "# æŒ‡å®šä¸¤ä¸ªæ•°æ®é›†åç§°ï¼ˆå¯æŒ‰éœ€ä¿®æ”¹ï¼‰\n",
    "sft_id = \"AI-ModelScope/train_0.5M_CN\"  # ä¸­æ–‡æ•°æ®é›†\n",
    "pref_id_primary = \"HuggingFaceH4/ultrafeedback_binarized\" # å¤šè¯­ï¼Œè‹±æ–‡ä¸ºä¸»ï¼Œå¯ç­›å‡ºä¸­æ–‡å­é›†\n",
    " \n",
    "# ä»…ä½¿ç”¨ ModelScope ä¸‹è½½åˆ°æœ¬åœ°ç¼“å­˜ï¼ˆä¸åšå›é€€ï¼‰\n",
    "sft_dir = snapshot_download(sft_id, repo_type=\"dataset\")\n",
    "pref_dir = snapshot_download(pref_id_primary, repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6915e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¢„è§ˆ SFT å’Œåå¥½æ•°æ®é›†\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd, os, json, glob\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "# ========== é€šç”¨å·¥å…·å‡½æ•° ==========\n",
    "def normalize_text(x):\n",
    "    \"\"\"å°†åµŒå¥—ç»“æ„æ ‡å‡†åŒ–ä¸ºå¯è¯»å­—ç¬¦ä¸²\"\"\"\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, str):\n",
    "        return x\n",
    "    if isinstance(x, dict):\n",
    "        return x.get(\"content\") or x.get(\"text\") or x.get(\"value\") or str(x)\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        parts = []\n",
    "        for it in x:\n",
    "            if isinstance(it, str):\n",
    "                parts.append(it)\n",
    "            elif isinstance(it, dict):\n",
    "                role = it.get(\"role\")\n",
    "                content = it.get(\"content\") or it.get(\"text\") or it.get(\"value\")\n",
    "                if content:\n",
    "                    parts.append((f\"{role}: \" if role else \"\") + str(content))\n",
    "        return \"\\n---\\n\".join(parts)\n",
    "    return str(x)\n",
    "# ========== SFT æ•°æ®åŠ è½½ ==========\n",
    "print(\"ğŸ“˜ åŠ è½½å¹¶é¢„è§ˆ SFT æ•°æ®\")\n",
    "sft_preview = load_dataset(sft_dir, split=\"train[:2]\")\n",
    "sft_rows = []\n",
    "for ex in sft_preview:\n",
    "    sft_rows.append({\n",
    "        \"instruction\": normalize_text(ex.get(\"instruction\")),\n",
    "        \"input\": normalize_text(ex.get(\"input\")),\n",
    "        \"output\": normalize_text(ex.get(\"output\")),\n",
    "        \"instr_len\": len(str(ex.get(\"instruction\", \"\"))),\n",
    "        \"input_len\": len(str(ex.get(\"input\", \"\"))),\n",
    "        \"output_len\": len(str(ex.get(\"output\", \"\"))),\n",
    "    })\n",
    "print(f\"[SFT] é¢„è§ˆ {len(sft_rows)} æ¡ / split=train[:2]\")\n",
    "display(pd.DataFrame(sft_rows))\n",
    "\n",
    "# ========== åå¥½æ•°æ®åŠ è½½ï¼ˆPrimaryï¼‰ ==========\n",
    "def load_pref_data(ds_dir, name=\"Primary\"):\n",
    "    \"\"\"å°è¯•å¤šç§æ–¹å¼åŠ è½½åå¥½æ•°æ®\"\"\"\n",
    "    for split in [\"train_prefs[:2]\", \"train[:2]\"]:\n",
    "        try:\n",
    "            ds = load_dataset(ds_dir, split=split)\n",
    "            return ds, f\"{name} ({split.split('[')[0]})\"\n",
    "        except:\n",
    "            continue\n",
    "    # å°è¯•ä» data ç›®å½•åŠ è½½\n",
    "    data_dir = os.path.join(ds_dir, \"data\")\n",
    "    if not os.path.exists(data_dir):\n",
    "        return None, None\n",
    "    parquet = glob.glob(os.path.join(data_dir, \"*.parquet\"))\n",
    "    if parquet:\n",
    "        ds = Dataset.from_parquet(parquet[0]).select(range(2))\n",
    "        return ds, f\"{name} (Parquet)\"\n",
    "    jsonl = glob.glob(os.path.join(data_dir, \"*.jsonl\"))\n",
    "    if jsonl:\n",
    "        data = []\n",
    "        with open(jsonl[0], \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= 2: break\n",
    "                try: data.append(json.loads(line))\n",
    "                except: pass\n",
    "        return Dataset.from_list(data), f\"{name} (JSONL)\" if data else (None, None)\n",
    "    return None, None\n",
    "\n",
    "def format_pref_data(pref_ds, name=\"Primary\"):\n",
    "    if pref_ds is None:\n",
    "        print(f\"[Preference {name}] âŒ æ— æ³•åŠ è½½\")\n",
    "        return\n",
    "    rows = []\n",
    "    for ex in pref_ds:\n",
    "        row = {\n",
    "            \"prompt\": normalize_text(ex.get(\"prompt\") or ex.get(\"instruction\") or ex.get(\"input\")),\n",
    "            \"y_pos\": normalize_text(ex.get(\"chosen\") or ex.get(\"better_response\") or ex.get(\"pos\")),\n",
    "            \"y_neg\": normalize_text(ex.get(\"rejected\") or ex.get(\"worse_response\") or ex.get(\"neg\")),\n",
    "        }\n",
    "        row[\"prompt_len\"] = len(str(row[\"prompt\"] or \"\"))\n",
    "        row[\"y_pos_len\"] = len(str(row[\"y_pos\"] or \"\"))\n",
    "        row[\"y_neg_len\"] = len(str(row[\"y_neg\"] or \"\"))\n",
    "        rows.append(row)\n",
    "    print(f\"[Preference {name}] é¢„è§ˆ {len(rows)} æ¡\")\n",
    "    display(pd.DataFrame(rows))\n",
    "\n",
    "print(\"ğŸ“— åŠ è½½å¹¶é¢„è§ˆ Preference æ•°æ®\")\n",
    "pref_preview, pref_source = load_pref_data(pref_dir, \"Primary\")\n",
    "format_pref_data(pref_preview, \"Primary\")\n",
    "\n",
    "print(f\"\\n[Preference Primary] æ•°æ®é›†: {pref_dir.split('/')[-1]}\")\n",
    "if pref_preview and len(pref_preview) > 0:\n",
    "    fields = list(pref_preview[0].keys())\n",
    "    has_chosen = any(k in fields for k in [\"chosen\", \"better_response\"])\n",
    "    has_rejected = any(k in fields for k in [\"rejected\", \"worse_response\"])\n",
    "    print(f\"  - å­—æ®µ: {fields}\")\n",
    "    print(f\"  - æ”¯æŒ: {'âœ… RM/DPO' if has_chosen and has_rejected else 'âš ï¸  éƒ¨åˆ†æ”¯æŒ'}\")\n",
    "else:\n",
    "    print(\"  - çŠ¶æ€: âŒ æœªåŠ è½½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c63d2f",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰\n",
    "- **è¾“å…¥**ï¼šæŒ‡ä»¤-å›ç­”å¯¹ï¼ˆé«˜è´¨é‡ã€äººç±»ä¹¦å†™/ç­›é€‰ï¼‰\n",
    "- **ç›®æ ‡**ï¼šè®©æ¨¡å‹åŸºæœ¬å­¦ä¼šâ€œæŒ‰æŒ‡ä»¤ä½œç­”â€\n",
    "- **è®­ç»ƒ**ï¼šæœ€å°åŒ–äº¤å‰ç†µæŸå¤±ï¼ˆå‚è€ƒå¸¸ç”¨æŒ‡ä»¤æ•°æ®é›†ï¼‰\n",
    "- **è¾“å‡º**ï¼šSFT æ¨¡å‹ï¼ˆä½œä¸ºåç»­ RM/PPO çš„å‚è€ƒç­–ç•¥ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd883faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã€åŠ è½½sftæ•°æ®é›†ã€‘ dataset\n",
    "from datasets import Dataset\n",
    "import os\n",
    "import json\n",
    "\n",
    "sft_path = os.path.expanduser(sft_dir)\n",
    "\n",
    "# æ£€æŸ¥ç›®å½•å†…å®¹\n",
    "print(\"[æ–‡ä»¶åˆ—è¡¨]\", os.listdir(sft_path))\n",
    "\n",
    "# å°è¯•æ‰¾åˆ° JSON æˆ– JSONL æ–‡ä»¶\n",
    "json_files = [f for f in os.listdir(sft_path) if f.endswith(\".json\") or f.endswith(\".jsonl\")]\n",
    "if not json_files:\n",
    "    raise RuntimeError(f\"åœ¨ {sft_path} æœªæ‰¾åˆ° JSON/JSONL æ–‡ä»¶ï¼Œè¯·æ‰‹åŠ¨æŸ¥çœ‹æ–‡ä»¶ç»“æ„ã€‚\")\n",
    "\n",
    "json_path = os.path.join(sft_path, json_files[0])\n",
    "print(f\"[åŠ è½½æ–‡ä»¶] {json_path}\")\n",
    "\n",
    "# åŠ è½½ JSON æ•°æ®\n",
    "data = []\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            data.append(json.loads(line))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# è½¬æ¢æˆ HuggingFace Dataset\n",
    "sft_ds = Dataset.from_list(data)\n",
    "print(f\"[SFT] æˆåŠŸåŠ è½½ {len(sft_ds)} æ¡æ ·æœ¬ã€‚\")\n",
    "print(\"[ç¤ºä¾‹æ ·æœ¬]\", sft_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026ca34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã€æ•°æ®é›†åˆ‡åˆ†ã€‘æŒ‰æ¯”ä¾‹åˆ’åˆ†ï¼Œæ¯”å¦‚ 98% è®­ç»ƒã€2% éªŒè¯\n",
    "sft_split = sft_ds.train_test_split(test_size=0.02, seed=42)\n",
    "\n",
    "# é‡æ„æ•°æ®æ ¼å¼ï¼šæ„é€  prompt-completion ç»“æ„ï¼ˆç¬¦åˆ TRL 0.24 æ ‡å‡†ï¼‰\n",
    "def _to_sft(example):\n",
    "    instr = example.get(\"instruction\", \"\")\n",
    "    inp = example.get(\"input\", \"\")\n",
    "    output = example.get(\"output\", None)\n",
    "    # ä½¿ç”¨ç©ºæ ¼ç»“å°¾ï¼Œé¿å…åˆ†è¯è¾¹ç•Œé—®é¢˜\n",
    "    prompt = (instr + (\"\\n\" + inp if inp else \"\")).strip() + \" \"\n",
    "    return {\"prompt\": prompt, \"completion\": output}\n",
    "\n",
    "# åˆ‡åˆ†å¹¶æ ¼å¼åŒ–\n",
    "train_ds = sft_split[\"train\"].map(_to_sft, remove_columns=sft_split[\"train\"].column_names)\n",
    "eval_ds = sft_split[\"test\"].map(_to_sft, remove_columns=sft_split[\"test\"].column_names)\n",
    "\n",
    "print(\"è®­ç»ƒé›†æ ·æœ¬æ•°:\", len(train_ds))\n",
    "print(\"éªŒè¯é›†æ ·æœ¬æ•°:\", len(eval_ds))\n",
    "print(\"[ç¤ºä¾‹]\", train_ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9549d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã€LoRA é…ç½®ä¸è®­ç»ƒå‚æ•°ã€‘\n",
    "from peft import LoraConfig\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# æ¨¡å‹è¾“å…¥è¾“å‡ºé…ç½®\n",
    "base_model_or_dir = model_dir  # å¤ç”¨ ModelScope ä¸‹è½½çš„æ¨¡å‹ç›®å½•\n",
    "output_dir = \"outputs/sft_qlora\"\n",
    "max_seq_length = 2048\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_mps = torch.backends.mps.is_available()\n",
    "\n",
    "# bitsandbytes QLoRA é…ç½® \n",
    "quantization_config = None\n",
    "if use_cuda:\n",
    "    try:\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "        print(\"[QLoRA] ä½¿ç”¨ bitsandbytes 4-bit é‡åŒ–åŠ è½½æ¨¡å‹\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Warn] æœªå¯ç”¨4bité‡åŒ–ï¼š{e}\")\n",
    "else:\n",
    "    print(\"[Info] å½“å‰ä¸ºé CUDA ç¯å¢ƒï¼Œä½¿ç”¨å¸¸è§„ç²¾åº¦åŠ è½½\")\n",
    "\n",
    "# Tokenizer åˆå§‹åŒ–\n",
    "if tokenizer is None:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_or_dir, use_fast=True, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528cc6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA é…ç½®ï¼šä½ç§©é€‚é…å™¨å‚æ•°\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                    # ç§©ï¼ˆrankï¼‰ï¼Œæ§åˆ¶é€‚é…å™¨çŸ©é˜µçš„ç»´åº¦ï¼Œè¶Šå¤§å®¹é‡è¶Šå¤§ä½†å‚æ•°é‡æ›´å¤šï¼ˆå¸¸ç”¨ 8/16/32ï¼‰\n",
    "    lora_alpha=32,          # ç¼©æ”¾ç³»æ•°ï¼Œä¸ r æˆæ¯”ä¾‹ä½¿ç”¨ï¼ˆé€šå¸¸ alpha=2*rï¼‰\n",
    "    lora_dropout=0.05,      # é€‚é…å™¨å±‚ dropoutï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œå¸¸ç”¨ 0.05-0.1ï¼‰\n",
    "    bias=\"none\",            # æ˜¯å¦è®­ç»ƒ biasï¼ˆ\"none\"/\"all\"/\"lora_only\"ï¼‰\n",
    "    task_type=\"CAUSAL_LM\",   # ä»»åŠ¡ç±»å‹ï¼šå› æœè¯­è¨€æ¨¡å‹ï¼ˆç”Ÿæˆä»»åŠ¡ï¼‰\n",
    ")\n",
    "\n",
    "# è®­ç»ƒå‚æ•°ï¼šæ§åˆ¶è®­ç»ƒæµç¨‹ä¸ä¼˜åŒ–\n",
    "args = TrainingArguments(\n",
    "    # è¾“å‡ºä¸ä¿å­˜\n",
    "    output_dir=output_dir,              # æ¨¡å‹/æ—¥å¿—è¾“å‡ºç›®å½•\n",
    "    save_steps=200,                     # æ¯ N æ­¥ä¿å­˜ä¸€æ¬¡ checkpoint\n",
    "    save_total_limit=2,                 # æœ€å¤šä¿ç•™ N ä¸ª checkpointï¼ˆé¿å…å ç£ç›˜ï¼‰\n",
    "    save_safetensors=True,              # ä½¿ç”¨ safetensors æ ¼å¼ä¿å­˜ï¼ˆæ›´å¿«æ›´å®‰å…¨ï¼‰\n",
    "    \n",
    "    # æ‰¹å¤§å°ä¸æ¢¯åº¦\n",
    "    per_device_train_batch_size=1,      # æ¯è®¾å¤‡è®­ç»ƒ batch å¤§å°ï¼ˆæ˜¾å­˜å—é™æ—¶å…ˆç”¨ 1ï¼‰\n",
    "    per_device_eval_batch_size=1,       # æ¯è®¾å¤‡éªŒè¯ batch å¤§å°\n",
    "    gradient_accumulation_steps=8,      # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆç­‰æ•ˆ batch = 1 * 8 = 8ï¼‰\n",
    "    \n",
    "    # è®­ç»ƒè½®æ¬¡ä¸å­¦ä¹ ç‡\n",
    "    num_train_epochs=1,                 # è®­ç»ƒè½®æ¬¡æ•°\n",
    "    learning_rate=2e-4,                 # åˆå§‹å­¦ä¹ ç‡ï¼ˆLoRA å¸¸ç”¨ 1e-4 åˆ° 5e-4ï¼‰\n",
    "    lr_scheduler_type=\"cosine\",         # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆä½™å¼¦é€€ç«ï¼‰\n",
    "    warmup_ratio=0.03,                  # warmup æ¯”ä¾‹ï¼ˆå‰ 3% æ­¥æ•°çº¿æ€§å¢é•¿ LRï¼‰\n",
    "    \n",
    "    # è¯„ä¼°ä¸æ—¥å¿—\n",
    "    eval_strategy=\"steps\",              # è¯„ä¼°ç­–ç•¥ï¼ˆ\"steps\"/\"epoch\"/\"no\"ï¼‰\n",
    "    eval_steps=100,                     # æ¯ N æ­¥è¯„ä¼°ä¸€æ¬¡\n",
    "    logging_steps=10,                   # æ¯ N æ­¥æ‰“å°ä¸€æ¬¡æ—¥å¿—\n",
    "    report_to=[\"none\"],                 # ä¸å‘å¤–éƒ¨ä¸ŠæŠ¥ï¼ˆå¯æ”¹ä¸º [\"wandb\"] ç­‰ï¼‰\n",
    "    \n",
    "    # æ¨¡å‹æ£€æŸ¥ç‚¹\n",
    "    load_best_model_at_end=True,        # è®­ç»ƒç»“æŸåŠ è½½æœ€ä½³æ¨¡å‹\n",
    "    metric_for_best_model=\"eval_loss\",  # æœ€ä½³æ¨¡å‹æŒ‡æ ‡\n",
    "    greater_is_better=False,            # è¯¥æŒ‡æ ‡è¶Šå°è¶Šå¥½\n",
    "    \n",
    "    # æ€§èƒ½ä¼˜åŒ–\n",
    "    bf16=use_cuda,                      # CUDA ä¸Šç”¨ bfloat16ï¼ˆç®—åŠ›ä¸ç¨³å®šæ€§å¹³è¡¡ï¼‰\n",
    "    fp16=False,                         # ç¦ç”¨ FP16ï¼ˆé¿å…å†²çªï¼‰\n",
    "    gradient_checkpointing=True,        # æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆç‰ºç‰²æ—¶é—´æ¢æ˜¾å­˜ï¼‰\n",
    ")\n",
    "\n",
    "print(\"è®­ç»ƒå‚æ•°é…ç½®å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5244aa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰æ ¼å¼åŒ–å‡½æ•°ï¼šæ„é€ è®­ç»ƒæ–‡æœ¬æ¨¡æ¿\n",
    "def formatting_func(example):\n",
    "    \"\"\"å°† prompt-response å¯¹è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ–‡æœ¬ï¼ˆå•æ¡å¤„ç†ï¼‰\"\"\"\n",
    "    prompt = str(example[\"prompt\"]).strip()\n",
    "    resp = str(example[\"response\"]).strip()\n",
    "    # æ‹¼æ¥æ ¼å¼ï¼šç¬¦åˆ Qwen çš„æ¨¡æ¿\n",
    "    text = f\"<|user|>\\n{prompt}\\n<|assistant|>\\n{resp}{tokenizer.eos_token}\"\n",
    "    return text\n",
    "\n",
    "print(\"[æ ¼å¼åŒ–å‡½æ•°å·²å®šä¹‰]\")\n",
    "print(\"[ç¤ºä¾‹]\", formatting_func({\"prompt\": \"ä½ å¥½\", \"response\": \"ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f75cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒå™¨çš„æ„é€ \n",
    "from trl import SFTTrainer\n",
    "\n",
    "# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆQLoRA æ¨¡å¼ä¸‹è‡ªåŠ¨åº”ç”¨é‡åŒ–ï¼‰\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_or_dir,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\" if use_cuda else None,\n",
    ")\n",
    "\n",
    "# æ„é€  SFT è®­ç»ƒå™¨\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    peft_config=lora_config,\n",
    "    args=args,\n",
    "    # ä¸ä½¿ç”¨ formatting_funcï¼Œç›´æ¥ç”¨ prompt-completion ç»“æ„\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eba8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¼€å§‹è®­ç»ƒ\n",
    "trainer.train()\n",
    "\n",
    "# === ä¿å­˜é€‚é…å™¨ï¼ˆLoRA æƒé‡ï¼‰===\n",
    "adapter_dir = os.path.join(output_dir, \"adapter\")\n",
    "trainer.model.save_pretrained(adapter_dir)\n",
    "print(f\"[Done] SFT + QLoRA è®­ç»ƒå®Œæˆï¼ŒLoRA æƒé‡å·²ä¿å­˜è‡³: {adapter_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcc3230",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰è®­ç»ƒ\n",
    "- **è¾“å…¥**ï¼šåŒä¸€æŒ‡ä»¤ä¸‹æˆå¯¹å›ç­”ï¼ˆAã€Bï¼‰ï¼Œä»¥åŠåå¥½æ ‡ç­¾ï¼ˆA > Bï¼‰\n",
    "- **ç›®æ ‡**ï¼šå­¦ä¹ â€œåå¥½è¯„åˆ†å‡½æ•°â€ r(x, y)\n",
    "- **è®­ç»ƒ**ï¼šPairwise rankingï¼ˆå¦‚ Bradleyâ€“Terry/Logistic lossï¼‰\n",
    "- **è¾“å‡º**ï¼šèƒ½å¯¹ä»»æ„å›ç­”æ‰“åˆ†çš„å¥–åŠ±æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e6eb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½åå¥½æ•°æ®é›†\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "if 'pref_dir' in globals():\n",
    "    for split in [\"train_prefs\", \"train\"]:\n",
    "        try:\n",
    "            pref_ds = load_dataset(pref_dir, split=split)\n",
    "            print(f\"[RM] ä»æœ¬åœ°ç›®å½•åŠ è½½æˆåŠŸï¼ˆ{split}ï¼‰ï¼Œå…± {len(pref_ds)} æ¡æ ·æœ¬\")\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "    # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•ä» data ç›®å½•åŠ è½½ Parquet/JSONL\n",
    "    if pref_ds is None:\n",
    "        data_dir = os.path.join(pref_dir, \"data\") if os.path.exists(os.path.join(pref_dir, \"data\")) else pref_dir\n",
    "        parquet_files = glob.glob(os.path.join(data_dir, \"*.parquet\"))\n",
    "        if parquet_files:\n",
    "            pref_ds = Dataset.from_parquet(parquet_files[0])\n",
    "            print(f\"[RM] ä» Parquet åŠ è½½æˆåŠŸï¼Œå…± {len(pref_ds)} æ¡æ ·æœ¬\")\n",
    "        else:\n",
    "            jsonl_files = glob.glob(os.path.join(data_dir, \"*.jsonl\"))\n",
    "            if jsonl_files:\n",
    "                data = []\n",
    "                with open(jsonl_files[0], \"r\", encoding=\"utf-8\") as f:\n",
    "                    for line in f:\n",
    "                        try:\n",
    "                            data.append(json.loads(line))\n",
    "                        except:\n",
    "                            pass\n",
    "                pref_ds = Dataset.from_list(data)\n",
    "                print(f\"[RM] ä» JSONL åŠ è½½æˆåŠŸï¼Œå…± {len(pref_ds)} æ¡æ ·æœ¬\")\n",
    "if pref_ds is None:\n",
    "    raise RuntimeError(\"æ— æ³•åŠ è½½åå¥½æ•°æ®é›†ï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„\")\n",
    "\n",
    "# é¢„è§ˆæ•°æ®ç»“æ„\n",
    "print(f\"\\n[RM] æ•°æ®é›†å­—æ®µï¼š{pref_ds[0].keys()}\")\n",
    "print(f\"[RM] ç¤ºä¾‹æ ·æœ¬ï¼š\")\n",
    "example = pref_ds[0]\n",
    "for k, v in example.items():\n",
    "    if isinstance(v, str) and len(v) > 100:\n",
    "        print(f\"  {k}: {v[:100]}...\")\n",
    "    else:\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66ae9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è½¬æ¢ä¸º RM è®­ç»ƒæ ¼å¼ï¼ˆprompt, chosen, rejectedï¼‰\n",
    "def normalize_text(x):\n",
    "    \"\"\"å°†åµŒå¥—ç»“æ„æ ‡å‡†åŒ–ä¸ºå¯è¯»å­—ç¬¦ä¸²\"\"\"\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, str):\n",
    "        return x\n",
    "    if isinstance(x, dict):\n",
    "        return x.get(\"content\") or x.get(\"text\") or x.get(\"value\") or str(x)\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        parts = []\n",
    "        for it in x:\n",
    "            if isinstance(it, str):\n",
    "                parts.append(it)\n",
    "            elif isinstance(it, dict):\n",
    "                role = it.get(\"role\")\n",
    "                content = it.get(\"content\") or it.get(\"text\") or it.get(\"value\")\n",
    "                if content:\n",
    "                    parts.append((f\"{role}: \" if role else \"\") + str(content))\n",
    "        return \"\\n---\\n\".join(parts)\n",
    "    return str(x)\n",
    "\n",
    "def _to_rm_format(example):\n",
    "    \"\"\"å°†åå¥½æ•°æ®è½¬æ¢ä¸º RM è®­ç»ƒæ ¼å¼ï¼ˆprompt, chosen, rejectedï¼‰\"\"\"\n",
    "    # å°è¯•å¤šç§å­—æ®µå\n",
    "    prompt = normalize_text(\n",
    "        example.get(\"prompt\") or \n",
    "        example.get(\"instruction\") or \n",
    "        example.get(\"input\") or\n",
    "        example.get(\"messages\")\n",
    "    )\n",
    "    \n",
    "    chosen = normalize_text(\n",
    "        example.get(\"chosen\") or \n",
    "        example.get(\"better_response\") or \n",
    "        example.get(\"positive\") or\n",
    "        example.get(\"response_j\")\n",
    "    )\n",
    "    \n",
    "    rejected = normalize_text(\n",
    "        example.get(\"rejected\") or \n",
    "        example.get(\"worse_response\") or \n",
    "        example.get(\"negative\") or\n",
    "        example.get(\"response_k\")\n",
    "    )\n",
    "    \n",
    "    # å¤„ç† messages æ ¼å¼ï¼ˆå¯¹è¯æ ¼å¼ï¼‰\n",
    "    if prompt is None and isinstance(example.get(\"messages\"), list):\n",
    "        msgs = example.get(\"messages\", [])\n",
    "        # æå–æœ€åä¸€ä¸ª user æ¶ˆæ¯ä½œä¸º promptï¼Œä¹‹å‰çš„ä½œä¸º context\n",
    "        user_msgs = [m.get(\"content\", \"\") for m in msgs if m.get(\"role\") == \"user\"]\n",
    "        if user_msgs:\n",
    "            prompt = user_msgs[-1]\n",
    "    \n",
    "    # å¤„ç† chosen/rejected å¦‚æœæ˜¯åˆ—è¡¨æ ¼å¼\n",
    "    if isinstance(chosen, list):\n",
    "        chosen = \"\\n---\\n\".join([normalize_text(c) for c in chosen])\n",
    "    if isinstance(rejected, list):\n",
    "        rejected = \"\\n---\\n\".join([normalize_text(r) for r in rejected])\n",
    "    \n",
    "    # ç¡®ä¿ prompt, chosen, rejected éƒ½ä¸ä¸ºç©º\n",
    "    if prompt is None or chosen is None or rejected is None:\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": str(prompt).strip(),\n",
    "        \"chosen\": str(chosen).strip(),\n",
    "        \"rejected\": str(rejected).strip(),\n",
    "    }\n",
    "\n",
    "# è½¬æ¢æ•°æ®æ ¼å¼\n",
    "print(\"\\n[RM] å¼€å§‹è½¬æ¢æ•°æ®æ ¼å¼...\")\n",
    "rm_ds = pref_ds.map(\n",
    "    _to_rm_format,\n",
    "    remove_columns=pref_ds.column_names,\n",
    "    desc=\"è½¬æ¢ä¸º RM æ ¼å¼\"\n",
    ")\n",
    "\n",
    "# è¿‡æ»¤æ‰ None å€¼ï¼ˆæ ¼å¼è½¬æ¢å¤±è´¥çš„æ•°æ®ï¼‰\n",
    "rm_ds = rm_ds.filter(lambda x: x[\"prompt\"] is not None and x[\"chosen\"] is not None and x[\"rejected\"] is not None)\n",
    "\n",
    "print(f\"[RM] æ ¼å¼è½¬æ¢å®Œæˆï¼Œæœ‰æ•ˆæ ·æœ¬æ•°ï¼š{len(rm_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73255c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®åˆ‡åˆ†ï¼š98% è®­ç»ƒï¼Œ2% éªŒè¯\n",
    "rm_split = rm_ds.train_test_split(test_size=0.02, seed=42)\n",
    "rm_train_ds = rm_split[\"train\"]\n",
    "rm_eval_ds = rm_split[\"test\"]\n",
    "\n",
    "print(f\"\\n[RM] æ•°æ®åˆ‡åˆ†å®Œæˆï¼š\")\n",
    "print(f\"  è®­ç»ƒé›†ï¼š{len(rm_train_ds)} æ¡\")\n",
    "print(f\"  éªŒè¯é›†ï¼š{len(rm_eval_ds)} æ¡\")\n",
    "\n",
    "# é¢„è§ˆè®­ç»ƒé›†æ ·æœ¬\n",
    "print(f\"\\n[RM] è®­ç»ƒé›†ç¤ºä¾‹ï¼š\")\n",
    "sample = rm_train_ds[0]\n",
    "for k, v in sample.items():\n",
    "    if isinstance(v, str) and len(v) > 150:\n",
    "        print(f\"  {k}: {v[:150]}...\")\n",
    "    else:\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58be0c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã€RM æ¨¡å‹åŠ è½½ä¸é…ç½®ã€‘\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from trl import RewardTrainer, RewardConfig\n",
    "import torch\n",
    "\n",
    "# æ¨¡å‹è¾“å…¥è¾“å‡ºé…ç½®\n",
    "rm_output_dir = \"outputs/rm_qlora\"\n",
    "max_seq_length = 2048\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_mps = torch.backends.mps.is_available()\n",
    "\n",
    "# é€‰æ‹©åŸºç¡€æ¨¡å‹ï¼šä¼˜å…ˆä½¿ç”¨ SFT æ¨¡å‹ï¼Œå¦åˆ™ä½¿ç”¨åŸºç¡€æ¨¡å‹\n",
    "# æ³¨æ„ï¼šRM è®­ç»ƒéœ€è¦å°†å› æœè¯­è¨€æ¨¡å‹è½¬æ¢ä¸ºåºåˆ—åˆ†ç±»æ¨¡å‹ï¼ˆæ·»åŠ å¥–åŠ±å¤´ï¼‰\n",
    "if 'adapter_dir' in globals() and os.path.exists(adapter_dir):\n",
    "    try:\n",
    "        # å°è¯•ä» SFT æ¨¡å‹åŠ è½½ï¼ˆéœ€è¦åˆå¹¶ LoRA æƒé‡ï¼‰\n",
    "        from peft import PeftModel\n",
    "        base_model_for_rm = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_or_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        sft_model = PeftModel.from_pretrained(base_model_for_rm, adapter_dir)\n",
    "        # åˆå¹¶ LoRA æƒé‡åˆ°åŸºç¡€æ¨¡å‹\n",
    "        merged_model = sft_model.merge_and_unload()\n",
    "        rm_base_model_dir = None  # æ ‡è®°ä½¿ç”¨åˆå¹¶åçš„æ¨¡å‹\n",
    "        print(\"[RM] ä½¿ç”¨ SFT æ¨¡å‹ä½œä¸ºåŸºç¡€ï¼ˆå·²åˆå¹¶ LoRA æƒé‡ï¼‰\")\n",
    "    except Exception as e:\n",
    "        print(f\"[RM] æ— æ³•åŠ è½½ SFT æ¨¡å‹ï¼š{e}ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹\")\n",
    "        rm_base_model_dir = base_model_or_dir\n",
    "else:\n",
    "    rm_base_model_dir = base_model_or_dir\n",
    "    print(\"[RM] ä½¿ç”¨åŸºç¡€æ¨¡å‹ï¼ˆæœªæ‰¾åˆ° SFT æ¨¡å‹ï¼‰\")\n",
    "\n",
    "# bitsandbytes QLoRA é…ç½®ï¼ˆå¯é€‰ï¼‰\n",
    "rm_quantization_config = None\n",
    "if use_cuda:\n",
    "    try:\n",
    "        rm_quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "        print(\"[RM] ä½¿ç”¨ bitsandbytes 4-bit é‡åŒ–åŠ è½½æ¨¡å‹\")\n",
    "    except Exception as e:\n",
    "        print(f\"[RM] æœªå¯ç”¨4bité‡åŒ–ï¼š{e}\")\n",
    "else:\n",
    "    print(\"[RM] å½“å‰ä¸ºé CUDA ç¯å¢ƒï¼Œä½¿ç”¨å¸¸è§„ç²¾åº¦åŠ è½½\")\n",
    "\n",
    "# Tokenizer åˆå§‹åŒ–ï¼ˆå¤ç”¨ä¹‹å‰çš„ tokenizerï¼‰\n",
    "if 'tokenizer' not in globals() or tokenizer is None:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        rm_base_model_dir if rm_base_model_dir else base_model_or_dir,\n",
    "        use_fast=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# åŠ è½½å¥–åŠ±æ¨¡å‹ï¼ˆå°†å› æœè¯­è¨€æ¨¡å‹è½¬æ¢ä¸ºåºåˆ—åˆ†ç±»æ¨¡å‹ï¼‰\n",
    "# AutoModelForSequenceClassification ä¼šè‡ªåŠ¨æ·»åŠ åˆ†ç±»å¤´ï¼ˆå¥–åŠ±å¤´ï¼‰\n",
    "if rm_base_model_dir:\n",
    "    rm_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        rm_base_model_dir,\n",
    "        trust_remote_code=True,\n",
    "        num_labels=1,  # å¥–åŠ±åˆ†æ•°æ˜¯æ ‡é‡\n",
    "        quantization_config=rm_quantization_config,\n",
    "        device_map=\"auto\" if use_cuda else None,\n",
    "    )\n",
    "else:\n",
    "    # å¦‚æœä½¿ç”¨åˆå¹¶åçš„ SFT æ¨¡å‹ï¼Œéœ€è¦ä»åˆå¹¶æ¨¡å‹åˆ›å»º RM æ¨¡å‹\n",
    "    # ç®€åŒ–å¤„ç†ï¼šç›´æ¥ä½¿ç”¨åŸºç¡€æ¨¡å‹ï¼ˆå®é™…ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„å¤„ç†ï¼‰\n",
    "    rm_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        base_model_or_dir,\n",
    "        trust_remote_code=True,\n",
    "        num_labels=1,\n",
    "        quantization_config=rm_quantization_config,\n",
    "        device_map=\"auto\" if use_cuda else None,\n",
    "    )\n",
    "\n",
    "print(\"[RM] å¥–åŠ±æ¨¡å‹åŠ è½½å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb3109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã€RM æ•°æ®æ ¼å¼åŒ–ï¼šæ„é€  prompt-response å¯¹ã€‘\n",
    "def format_rm_prompt(prompt, response):\n",
    "    \"\"\"å°† prompt å’Œ response æ ¼å¼åŒ–ä¸ºæ¨¡å‹è¾“å…¥æ–‡æœ¬ï¼ˆç¬¦åˆ Qwen æ¨¡æ¿ï¼‰\"\"\"\n",
    "    # æ„é€ ç¬¦åˆ Qwen æ¨¡æ¿çš„æ–‡æœ¬æ ¼å¼\n",
    "    text = f\"<|user|>\\n{prompt}\\n<|assistant|>\\n{response}\"\n",
    "    return text\n",
    "\n",
    "def tokenize_rm_dataset(examples):\n",
    "    \"\"\"å¯¹ RM æ•°æ®é›†è¿›è¡Œåˆ†è¯ï¼ˆå¤„ç† prompt+chosen å’Œ prompt+rejected å¯¹ï¼‰\"\"\"\n",
    "    # æ„é€  chosen å’Œ rejected çš„å®Œæ•´æ–‡æœ¬\n",
    "    chosen_texts = [format_rm_prompt(p, c) for p, c in zip(examples[\"prompt\"], examples[\"chosen\"])]\n",
    "    rejected_texts = [format_rm_prompt(p, r) for p, r in zip(examples[\"prompt\"], examples[\"rejected\"])]\n",
    "    \n",
    "    # åˆ†è¯ï¼ˆchosenï¼‰- ä¸ä½¿ç”¨ return_tensorsï¼Œå› ä¸º Dataset.map æœŸæœ›è¿”å›æ™®é€šåˆ—è¡¨\n",
    "    chosen_tokenized = tokenizer(\n",
    "        chosen_texts,\n",
    "        max_length=max_seq_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    \n",
    "    # åˆ†è¯ï¼ˆrejectedï¼‰- ä¸ä½¿ç”¨ return_tensors\n",
    "    rejected_tokenized = tokenizer(\n",
    "        rejected_texts,\n",
    "        max_length=max_seq_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    \n",
    "    # è¿”å›æ ¼å¼åŒ–çš„æ•°æ®ï¼ˆTRL RewardTrainer æœŸæœ›çš„æ ¼å¼ï¼‰\n",
    "    return {\n",
    "        \"input_ids_chosen\": chosen_tokenized[\"input_ids\"],\n",
    "        \"attention_mask_chosen\": chosen_tokenized[\"attention_mask\"],\n",
    "        \"input_ids_rejected\": rejected_tokenized[\"input_ids\"],\n",
    "        \"attention_mask_rejected\": rejected_tokenized[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "# å¯¹è®­ç»ƒé›†å’ŒéªŒè¯é›†è¿›è¡Œåˆ†è¯\n",
    "print(\"[RM] å¼€å§‹å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯...\")\n",
    "rm_train_tokenized = rm_train_ds.map(\n",
    "    tokenize_rm_dataset,\n",
    "    batched=True,\n",
    "    desc=\"åˆ†è¯è®­ç»ƒé›†\"\n",
    ")\n",
    "rm_eval_tokenized = rm_eval_ds.map(\n",
    "    tokenize_rm_dataset,\n",
    "    batched=True,\n",
    "    desc=\"åˆ†è¯éªŒè¯é›†\"\n",
    ")\n",
    "\n",
    "print(f\"[RM] åˆ†è¯å®Œæˆï¼š\")\n",
    "print(f\"  è®­ç»ƒé›†ï¼š{len(rm_train_tokenized)} æ¡\")\n",
    "print(f\"  éªŒè¯é›†ï¼š{len(rm_eval_tokenized)} æ¡\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97636ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã€RM LoRA é…ç½®ä¸è®­ç»ƒå‚æ•°ã€‘\n",
    "# LoRA é…ç½®ï¼ˆå¦‚æœä½¿ç”¨ LoRA å¾®è°ƒï¼‰\n",
    "rm_lora_config = LoraConfig(\n",
    "    r=16,                    # ç§©ï¼ˆrankï¼‰\n",
    "    lora_alpha=32,          # ç¼©æ”¾ç³»æ•°ï¼ˆé€šå¸¸ alpha=2*rï¼‰\n",
    "    lora_dropout=0.05,      # é€‚é…å™¨å±‚ dropout\n",
    "    bias=\"none\",            # ä¸è®­ç»ƒ bias\n",
    "    task_type=\"SEQ_CLS\",    # ä»»åŠ¡ç±»å‹ï¼šåºåˆ—åˆ†ç±»ï¼ˆå¥–åŠ±æ¨¡å‹ï¼‰\n",
    ")\n",
    "\n",
    "# è®­ç»ƒå‚æ•°ï¼šæ§åˆ¶è®­ç»ƒæµç¨‹ä¸ä¼˜åŒ–\n",
    "rm_args = RewardConfig(\n",
    "    # è¾“å‡ºä¸ä¿å­˜\n",
    "    output_dir=rm_output_dir,           # æ¨¡å‹/æ—¥å¿—è¾“å‡ºç›®å½•\n",
    "    save_steps=500,                     # æ¯ N æ­¥ä¿å­˜ä¸€æ¬¡ checkpoint\n",
    "    save_total_limit=2,                 # æœ€å¤šä¿ç•™ N ä¸ª checkpoint\n",
    "    save_safetensors=True,              # ä½¿ç”¨ safetensors æ ¼å¼ä¿å­˜\n",
    "    \n",
    "    # æ‰¹å¤§å°ä¸æ¢¯åº¦\n",
    "    per_device_train_batch_size=1,      # æ¯è®¾å¤‡è®­ç»ƒ batch å¤§å°\n",
    "    per_device_eval_batch_size=1,       # æ¯è®¾å¤‡éªŒè¯ batch å¤§å°\n",
    "    gradient_accumulation_steps=8,      # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆç­‰æ•ˆ batch = 1 * 8 = 8ï¼‰\n",
    "    \n",
    "    # è®­ç»ƒè½®æ¬¡ä¸å­¦ä¹ ç‡\n",
    "    num_train_epochs=1,                 # è®­ç»ƒè½®æ¬¡æ•°\n",
    "    learning_rate=1e-5,                 # åˆå§‹å­¦ä¹ ç‡ï¼ˆRM è®­ç»ƒå¸¸ç”¨ 1e-5 åˆ° 5e-5ï¼‰\n",
    "    lr_scheduler_type=\"cosine\",         # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆä½™å¼¦é€€ç«ï¼‰\n",
    "    warmup_ratio=0.03,                  # warmup æ¯”ä¾‹ï¼ˆå‰ 3% æ­¥æ•°çº¿æ€§å¢é•¿ LRï¼‰\n",
    "    \n",
    "    # è¯„ä¼°ä¸æ—¥å¿—\n",
    "    eval_strategy=\"steps\",              # è¯„ä¼°ç­–ç•¥ï¼ˆ\"steps\"/\"epoch\"/\"no\"ï¼‰\n",
    "    eval_steps=250,                     # æ¯ N æ­¥è¯„ä¼°ä¸€æ¬¡\n",
    "    logging_steps=50,                   # æ¯ N æ­¥æ‰“å°ä¸€æ¬¡æ—¥å¿—\n",
    "    report_to=[\"none\"],                 # ä¸å‘å¤–éƒ¨ä¸ŠæŠ¥ï¼ˆå¯æ”¹ä¸º [\"wandb\"] ç­‰ï¼‰\n",
    "    \n",
    "    # æ¨¡å‹æ£€æŸ¥ç‚¹\n",
    "    load_best_model_at_end=True,        # è®­ç»ƒç»“æŸåŠ è½½æœ€ä½³æ¨¡å‹\n",
    "    metric_for_best_model=\"eval_loss\",  # æœ€ä½³æ¨¡å‹æŒ‡æ ‡\n",
    "    greater_is_better=False,            # è¯¥æŒ‡æ ‡è¶Šå°è¶Šå¥½\n",
    "    \n",
    "    # æ€§èƒ½ä¼˜åŒ–\n",
    "    bf16=use_cuda,                      # CUDA ä¸Šç”¨ bfloat16\n",
    "    fp16=False,                         # ç¦ç”¨ FP16\n",
    "    gradient_checkpointing=True,        # æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆç‰ºç‰²æ—¶é—´æ¢æ˜¾å­˜ï¼‰\n",
    "    \n",
    "    # RM ç‰¹å®šå‚æ•°\n",
    "    max_length=max_seq_length,          # æœ€å¤§åºåˆ—é•¿åº¦\n",
    ")\n",
    "\n",
    "print(\"[RM] è®­ç»ƒå‚æ•°é…ç½®å®Œæˆ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447d7694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã€æ„é€  RM è®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒã€‘\n",
    "# å¦‚æœä½¿ç”¨ LoRAï¼Œåº”ç”¨ LoRA é…ç½®åˆ°æ¨¡å‹\n",
    "if rm_quantization_config is not None or use_cuda:\n",
    "    # å¦‚æœå·²ç»é‡åŒ–æˆ–ä½¿ç”¨ CUDAï¼Œåº”ç”¨ LoRA\n",
    "    try:\n",
    "        rm_model = get_peft_model(rm_model, rm_lora_config)\n",
    "        print(\"[RM] LoRA é…ç½®å·²åº”ç”¨åˆ°æ¨¡å‹\")\n",
    "    except Exception as e:\n",
    "        print(f\"[RM] åº”ç”¨ LoRA æ—¶å‡ºé”™ï¼š{e}ï¼Œç»§ç»­ä½¿ç”¨å…¨é‡å¾®è°ƒ\")\n",
    "\n",
    "# æ„é€  RewardTrainer\n",
    "rm_trainer = RewardTrainer(\n",
    "    model=rm_model,\n",
    "    args=rm_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=rm_train_tokenized,\n",
    "    eval_dataset=rm_eval_tokenized,\n",
    "    peft_config=rm_lora_config if rm_quantization_config is None and not use_cuda else None,\n",
    ")\n",
    "\n",
    "print(\"[RM] è®­ç»ƒå™¨æ„é€ å®Œæˆï¼Œå‡†å¤‡å¼€å§‹è®­ç»ƒ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20f807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¼€å§‹è®­ç»ƒ\n",
    "rm_trainer.train()\n",
    "\n",
    "# ä¿å­˜æ¨¡å‹\n",
    "rm_model_dir = os.path.join(rm_output_dir, \"reward_model\")\n",
    "rm_trainer.model.save_pretrained(rm_model_dir)\n",
    "tokenizer.save_pretrained(rm_model_dir)\n",
    "print(f\"[Done] RM + QLoRA è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜è‡³: {rm_model_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8fe69d",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ PPO å¼ºåŒ–ä¼˜åŒ–\n",
    "- **è¾“å…¥**ï¼šSFT æ¨¡å‹ä½œä¸ºåˆå§‹ç­–ç•¥ \\(\\pi_\\theta\\)ï¼Œå¥–åŠ±æ¨¡å‹ r ä½œä¸ºå¥–åŠ±ä¿¡å·\n",
    "- **ç›®æ ‡**ï¼šåœ¨ KL çº¦æŸä¸‹æœ€å¤§åŒ–æœŸæœ›å¥–åŠ±ï¼Œæå‡å¯¹é½åº¦ä¸æœ‰ç”¨æ€§\n",
    "- **è®­ç»ƒ**ï¼šPPOï¼ˆå‰ªåˆ‡ç­–ç•¥æ¢¯åº¦ï¼‰ï¼Œå¼•å…¥ KL æƒ©ç½šä»¥ä¿æŒä¸å‚è€ƒç­–ç•¥æ¥è¿‘\n",
    "- **è¾“å‡º**ï¼šPPO åçš„å¯¹é½æ¨¡å‹ï¼ˆæ›´ç¬¦åˆäººç±»åå¥½ï¼‰\n",
    "- **å®è·µè¦ç‚¹**ï¼šé«˜è´¨é‡åå¥½æ•°æ®ä¸ç¨³å®šçš„ KL æ§åˆ¶æ˜¯æˆåŠŸå…³é”®ï¼›ç›‘æ§é•¿åº¦åç½®ã€æ¨¡å¼åç¼©ä¸è¿‡æ‹Ÿåˆã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a52435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã€PPO è®­ç»ƒå‡†å¤‡ï¼šæ¨¡å‹åŠ è½½ã€‘\n",
    "from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "import torch\n",
    "\n",
    "# PPO è¾“å‡ºé…ç½®\n",
    "ppo_output_dir = \"outputs/ppo_model\"\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_mps = torch.backends.mps.is_available()\n",
    "\n",
    "print(\"[PPO] å¼€å§‹åŠ è½½æ¨¡å‹...\")\n",
    "\n",
    "# ========== 1. åŠ è½½ç­–ç•¥æ¨¡å‹ï¼ˆSFT æ¨¡å‹ï¼‰ä½œä¸ºåˆå§‹ç­–ç•¥ Ï€_Î¸ ==========\n",
    "# PPO éœ€è¦ç­–ç•¥æ¨¡å‹ï¼ˆè®­ç»ƒä¸­çš„æ¨¡å‹ï¼‰å’Œå‚è€ƒæ¨¡å‹ï¼ˆç”¨äº KL çº¦æŸï¼‰\n",
    "if 'adapter_dir' in globals() and os.path.exists(adapter_dir):\n",
    "    try:\n",
    "        # ä» SFT æ¨¡å‹åŠ è½½ï¼ˆå¸¦ LoRA æƒé‡ï¼‰\n",
    "        from peft import PeftModel\n",
    "        base_policy = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_or_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        policy_model = PeftModel.from_pretrained(base_policy, adapter_dir)\n",
    "        print(\"[PPO] ç­–ç•¥æ¨¡å‹ï¼šä½¿ç”¨ SFT æ¨¡å‹ï¼ˆå¸¦ LoRA æƒé‡ï¼‰\")\n",
    "        \n",
    "        # å‚è€ƒæ¨¡å‹ï¼šä½¿ç”¨åˆå¹¶åçš„åŸºç¡€æ¨¡å‹ï¼ˆç”¨äº KL çº¦æŸï¼‰\n",
    "        reference_model = PeftModel.from_pretrained(base_policy, adapter_dir)\n",
    "        # åˆ›å»ºå‚è€ƒæ¨¡å‹çš„å‰¯æœ¬ï¼ˆå†»ç»“å‚æ•°ï¼Œä¸æ›´æ–°ï¼‰\n",
    "        reference_model.merge_and_unload()\n",
    "        print(\"[PPO] å‚è€ƒæ¨¡å‹ï¼šä½¿ç”¨ SFT æ¨¡å‹ï¼ˆå·²åˆå¹¶ï¼Œç”¨äº KL çº¦æŸï¼‰\")\n",
    "    except Exception as e:\n",
    "        print(f\"[PPO] æ— æ³•åŠ è½½ SFT æ¨¡å‹ï¼š{e}ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹\")\n",
    "        policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_or_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        reference_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_or_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "else:\n",
    "    # å¦‚æœæœªæ‰¾åˆ° SFT æ¨¡å‹ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹\n",
    "    policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_or_dir,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    reference_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_or_dir,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    print(\"[PPO] ç­–ç•¥æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹ï¼šä½¿ç”¨åŸºç¡€æ¨¡å‹\")\n",
    "\n",
    "# å°†ç­–ç•¥æ¨¡å‹åŒ…è£…ä¸ºå¸¦å€¼å‡½æ•°çš„æ¨¡å‹ï¼ˆPPO éœ€è¦ï¼‰\n",
    "# AutoModelForCausalLMWithValueHead ä¼šåœ¨ç­–ç•¥æ¨¡å‹åŸºç¡€ä¸Šæ·»åŠ å€¼å‡½æ•°å¤´ï¼ˆç”¨äºä»·å€¼ä¼°è®¡ï¼‰\n",
    "# æ³¨æ„ï¼šå¦‚æœ policy_model æ˜¯ PeftModelï¼Œéœ€è¦å…ˆè·å–åŸºç¡€æ¨¡å‹è·¯å¾„\n",
    "try:\n",
    "    if hasattr(policy_model, 'merge_and_unload'):\n",
    "        # å¯¹äº PeftModelï¼Œåˆå¹¶ LoRA æƒé‡ååˆ›å»º ValueHead æ¨¡å‹\n",
    "        # æ³¨æ„ï¼šåˆå¹¶åçš„æ¨¡å‹ä¼šåœ¨å†…å­˜ä¸­ï¼Œéœ€è¦ä¿å­˜åˆ°ä¸´æ—¶è·¯å¾„æˆ–ç›´æ¥ä½¿ç”¨åŸºç¡€æ¨¡å‹\n",
    "        print(\"[PPO] æ£€æµ‹åˆ° PeftModelï¼Œä»åŸºç¡€æ¨¡å‹åˆ›å»º ValueHead æ¨¡å‹ï¼ˆå°†ä½¿ç”¨ SFT æƒé‡ï¼‰\")\n",
    "        # ç®€åŒ–å¤„ç†ï¼šç›´æ¥ä»åŸºç¡€æ¨¡å‹åˆ›å»ºï¼ŒSFT æƒé‡å°†åœ¨åç»­åŠ è½½\n",
    "        ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "            base_model_or_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "    else:\n",
    "        # å¦‚æœ policy_model æ˜¯æ™®é€šæ¨¡å‹ï¼Œç›´æ¥ä»åŸºç¡€æ¨¡å‹åˆ›å»º\n",
    "        ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "            base_model_or_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "    print(\"[PPO] ç­–ç•¥æ¨¡å‹ï¼ˆå¸¦å€¼å‡½æ•°å¤´ï¼‰åŠ è½½å®Œæˆ\")\n",
    "except Exception as e:\n",
    "    print(f\"[PPO] åˆ›å»º ValueHead æ¨¡å‹å¤±è´¥ï¼š{e}\")\n",
    "    print(\"[PPO] å°è¯•ç›´æ¥ä»åŸºç¡€æ¨¡å‹åˆ›å»º...\")\n",
    "    ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "        base_model_or_dir,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "# ========== 2. åŠ è½½å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ä½œä¸ºå¥–åŠ±ä¿¡å· ==========\n",
    "if 'rm_model_dir' in globals() and os.path.exists(rm_model_dir):\n",
    "    try:\n",
    "        reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            rm_model_dir,\n",
    "            trust_remote_code=True,\n",
    "            num_labels=1,  # å¥–åŠ±åˆ†æ•°æ˜¯æ ‡é‡\n",
    "        )\n",
    "        print(f\"[PPO] å¥–åŠ±æ¨¡å‹ï¼šä» {rm_model_dir} åŠ è½½æˆåŠŸ\")\n",
    "    except Exception as e:\n",
    "        print(f\"[PPO] æ— æ³•åŠ è½½å¥–åŠ±æ¨¡å‹ï¼š{e}\")\n",
    "        reward_model = None\n",
    "else:\n",
    "    print(\"[PPO] æœªæ‰¾åˆ°å¥–åŠ±æ¨¡å‹ï¼Œéœ€è¦åœ¨è®­ç»ƒå‰å…ˆå®Œæˆ RM è®­ç»ƒ\")\n",
    "    reward_model = None\n",
    "\n",
    "# Tokenizer å¤ç”¨ï¼ˆå¦‚æœå·²å®šä¹‰ï¼‰\n",
    "if 'tokenizer' not in globals() or tokenizer is None:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        base_model_or_dir,\n",
    "        use_fast=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"[PPO] æ¨¡å‹åŠ è½½å®Œæˆ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5960d724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã€PPO æ•°æ®å‡†å¤‡ï¼šæç¤ºæ•°æ®é›†ã€‘\n",
    "# PPO è®­ç»ƒéœ€è¦æç¤ºï¼ˆpromptsï¼‰æ•°æ®é›†ï¼Œæ¨¡å‹ä¼šæ ¹æ®è¿™äº›æç¤ºç”Ÿæˆå›ç­”\n",
    "# ç„¶åå¥–åŠ±æ¨¡å‹å¯¹ç”Ÿæˆçš„å›ç­”æ‰“åˆ†ï¼ŒPPO æ ¹æ®å¥–åŠ±ä¿¡å·ä¼˜åŒ–ç­–ç•¥\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# æ–¹å¼ 1ï¼šä» SFT æ•°æ®é›†ä¸­æå– promptsï¼ˆæ¨èï¼‰\n",
    "# ä½¿ç”¨ SFT è®­ç»ƒé›†ä¸­çš„ prompts ä½œä¸º PPO çš„è¾“å…¥\n",
    "if 'train_ds' in globals() and len(train_ds) > 0:\n",
    "    # ä» SFT è®­ç»ƒé›†ä¸­æå– prompts\n",
    "    ppo_prompts = [ex[\"prompt\"] for ex in train_ds.select(range(min(1000, len(train_ds))))]\n",
    "    print(f\"[PPO] ä» SFT è®­ç»ƒé›†æå– {len(ppo_prompts)} ä¸ª prompts\")\n",
    "else:\n",
    "    # æ–¹å¼ 2ï¼šåˆ›å»ºç®€å•çš„æç¤ºç¤ºä¾‹\n",
    "    ppo_prompts = [\n",
    "        \"è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ã€‚\",\n",
    "        \"å¦‚ä½•æé«˜ç¼–ç¨‹æŠ€èƒ½ï¼Ÿ\",\n",
    "        \"ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ\",\n",
    "        \"å¦‚ä½•å­¦ä¹  Pythonï¼Ÿ\",\n",
    "        \"ä»‹ç»ä¸€ä¸‹ç¥ç»ç½‘ç»œã€‚\",\n",
    "    ]\n",
    "    print(f\"[PPO] ä½¿ç”¨ç¤ºä¾‹ promptsï¼ˆå…± {len(ppo_prompts)} ä¸ªï¼‰\")\n",
    "\n",
    "# æ„é€  PPO æç¤ºæ•°æ®é›†\n",
    "ppo_dataset = Dataset.from_dict({\"query\": ppo_prompts})\n",
    "\n",
    "print(f\"[PPO] æç¤ºæ•°æ®é›†å‡†å¤‡å®Œæˆï¼š{len(ppo_dataset)} æ¡ prompts\")\n",
    "print(f\"[PPO] ç¤ºä¾‹ prompt: {ppo_dataset[0]['query']}\")\n",
    "\n",
    "# PPO è®­ç»ƒæµç¨‹è¯´æ˜ï¼š\n",
    "# 1. ç­–ç•¥æ¨¡å‹æ ¹æ® prompts ç”Ÿæˆå›ç­”ï¼ˆå¤šä¸ªå€™é€‰å›ç­”ï¼‰\n",
    "# 2. å¥–åŠ±æ¨¡å‹å¯¹æ¯ä¸ªç”Ÿæˆçš„å›ç­”æ‰“åˆ†\n",
    "# 3. PPO ç®—æ³•æ ¹æ®å¥–åŠ±ä¿¡å·ä¼˜åŒ–ç­–ç•¥ï¼ŒåŒæ—¶ä¿æŒä¸å‚è€ƒæ¨¡å‹çš„ KL æ•£åº¦çº¦æŸ\n",
    "# 4. é‡å¤ä¸Šè¿°è¿‡ç¨‹ï¼Œç›´åˆ°ç­–ç•¥æ”¶æ•›\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9851c08b",
   "metadata": {},
   "source": [
    "## ğŸ” RLHF ä¸‰é˜¶æ®µçš„æƒé‡å­˜å‚¨è¯¦è§£\n",
    "\n",
    "### 1ï¸âƒ£ SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰é˜¶æ®µ - LoRA æƒé‡å­˜å‚¨\n",
    "\n",
    "**å­˜å‚¨æ–¹å¼**ï¼šä»…ä¿å­˜ LoRA adapter æƒé‡ï¼ˆè½»é‡çº§ï¼‰\n",
    "\n",
    "```\n",
    "SFT è®­ç»ƒåçš„æ¨¡å‹ç»“æ„ï¼š\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   åŸºç¡€æ¨¡å‹ï¼ˆQwen2.5-1.5Bï¼‰              â”‚  â† ä¸ä¿å­˜ï¼Œä¿æŒä¸å˜\n",
    "â”‚   â”œâ”€â”€ Embeddings                     â”‚\n",
    "â”‚   â”œâ”€â”€ Transformer Layers (x24)       â”‚\n",
    "â”‚   â””â”€â”€ LM Head                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â–²\n",
    "           â”‚ LoRA é€‚é…å™¨ï¼ˆå°æƒé‡çŸ©é˜µï¼‰\n",
    "           â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   LoRA Adapters                     â”‚  â† ä¿å­˜è¿™äº›\n",
    "â”‚   â”œâ”€â”€ LoRA_A (rank=16)              â”‚    å¤§å°ï¼š~å‡ MB\n",
    "â”‚   â””â”€â”€ LoRA_B (rank=16)              â”‚    å‚æ•°é‡ï¼š~0.1% of åŸºç¡€æ¨¡å‹\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "ä¿å­˜è·¯å¾„ï¼šoutputs/sft_qlora/adapter/\n",
    "â”œâ”€â”€ adapter_config.json              â† LoRA é…ç½®\n",
    "â”œâ”€â”€ adapter_model.safetensors        â† LoRA æƒé‡ï¼ˆè½»é‡çº§ï¼‰\n",
    "â””â”€â”€ tokenizer.json                   â† Tokenizer\n",
    "```\n",
    "\n",
    "**ç‰¹ç‚¹**ï¼š\n",
    "- âœ… ä»…ä¿å­˜å°‘é‡å‚æ•°ï¼ˆLoRA æƒé‡ï¼Œé€šå¸¸åªæœ‰åŸæ¨¡å‹çš„ 0.1-1%ï¼‰\n",
    "- âœ… æ–‡ä»¶å°ï¼Œä¾¿äºå­˜å‚¨å’Œå…±äº«\n",
    "- âœ… éœ€è¦åŸºç¡€æ¨¡å‹æ‰èƒ½ä½¿ç”¨ï¼ˆæ¨ç†æ—¶éœ€è¦åˆå¹¶ï¼‰\n",
    "\n",
    "**ä»£ç ç¤ºä¾‹**ï¼š\n",
    "```python\n",
    "# SFT è®­ç»ƒåä¿å­˜\n",
    "adapter_dir = os.path.join(output_dir, \"adapter\")\n",
    "trainer.model.save_pretrained(adapter_dir)  # åªä¿å­˜ LoRA æƒé‡\n",
    "\n",
    "# ä½¿ç”¨æ—¶çš„åŠ è½½\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_or_dir)\n",
    "model = PeftModel.from_pretrained(base_model, adapter_dir)  # åŠ è½½ LoRA\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ RMï¼ˆå¥–åŠ±æ¨¡å‹ï¼‰é˜¶æ®µ - å¥–åŠ±å¤´å­˜å‚¨\n",
    "\n",
    "**å­˜å‚¨æ–¹å¼**ï¼šä¿å­˜å®Œæ•´çš„å¥–åŠ±æ¨¡å‹ï¼ˆåŸºç¡€æ¨¡å‹ + å¥–åŠ±å¤´ï¼‰\n",
    "\n",
    "```\n",
    "RM è®­ç»ƒåçš„æ¨¡å‹ç»“æ„ï¼š\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   åŸºç¡€æ¨¡å‹ï¼ˆå¯èƒ½æ˜¯ SFT åˆå¹¶åçš„ï¼‰       â”‚  â† åŒ…å«åœ¨å†…\n",
    "â”‚   â”œâ”€â”€ Embeddings                     â”‚\n",
    "â”‚   â”œâ”€â”€ Transformer Layers             â”‚\n",
    "â”‚   â””â”€â”€ Hidden States                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â–²\n",
    "           â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   å¥–åŠ±å¤´ï¼ˆReward Headï¼‰                â”‚  â† è¿™æ˜¯æ–°æ·»åŠ çš„\n",
    "â”‚   â”œâ”€â”€ Linear Layer (hidden_size)     â”‚    å…³é”®ç»„ä»¶ï¼\n",
    "â”‚   â””â”€â”€ Output: 1 (å¥–åŠ±åˆ†æ•°æ ‡é‡)        â”‚    å¤§å°ï¼š~å‡ MB\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "ä¿å­˜è·¯å¾„ï¼šoutputs/rm_qlora/reward_model/\n",
    "â”œâ”€â”€ config.json                       â† æ¨¡å‹é…ç½®ï¼ˆåŒ…å« num_labels=1ï¼‰\n",
    "â”œâ”€â”€ model.safetensors                 â† å®Œæ•´æ¨¡å‹æƒé‡ï¼ˆåŸºç¡€æ¨¡å‹ + å¥–åŠ±å¤´ï¼‰\n",
    "â”‚                                       æˆ– adapter_model.safetensorsï¼ˆå¦‚æœç”¨äº† LoRAï¼‰\n",
    "â””â”€â”€ tokenizer.json                    â† Tokenizer\n",
    "```\n",
    "\n",
    "**ç‰¹ç‚¹**ï¼š\n",
    "- âœ… å¥–åŠ±å¤´æ˜¯æ–°å¢çš„ç»„ä»¶ï¼ˆLinear å±‚ï¼Œè¾“å‡ºç»´åº¦ä¸º 1ï¼‰\n",
    "- âœ… å¦‚æœä½¿ç”¨ LoRAï¼Œå¯èƒ½åªä¿å­˜ LoRA æƒé‡ï¼›å¦‚æœå…¨é‡å¾®è°ƒï¼Œä¿å­˜å®Œæ•´æ¨¡å‹\n",
    "- âœ… å¥–åŠ±å¤´æ˜¯å…³é”®ï¼šå®ƒå­¦ä¼šäº†å¯¹å›ç­”æ‰“åˆ†ï¼ˆchosen > rejectedï¼‰\n",
    "\n",
    "**ä»£ç ç¤ºä¾‹**ï¼š\n",
    "```python\n",
    "# RM è®­ç»ƒåä¿å­˜\n",
    "rm_model_dir = os.path.join(rm_output_dir, \"reward_model\")\n",
    "rm_trainer.model.save_pretrained(rm_model_dir)  # ä¿å­˜å¥–åŠ±æ¨¡å‹ï¼ˆåŒ…å«å¥–åŠ±å¤´ï¼‰\n",
    "\n",
    "# ä½¿ç”¨æ—¶çš„åŠ è½½\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    rm_model_dir,\n",
    "    num_labels=1,  # å¥–åŠ±å¤´è¾“å‡ºç»´åº¦ä¸º 1\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ PPOï¼ˆå¼ºåŒ–ä¼˜åŒ–ï¼‰é˜¶æ®µ - ç­–ç•¥æ¨¡å‹ + å€¼å‡½æ•°å¤´å­˜å‚¨\n",
    "\n",
    "**å­˜å‚¨æ–¹å¼**ï¼šä¿å­˜ç­–ç•¥æ¨¡å‹æƒé‡ + å€¼å‡½æ•°å¤´æƒé‡\n",
    "\n",
    "```\n",
    "PPO è®­ç»ƒåçš„æ¨¡å‹ç»“æ„ï¼š\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   ç­–ç•¥æ¨¡å‹ï¼ˆPolicy Modelï¼‰             â”‚  â† å¯ä»¥æ˜¯åŸºç¡€æ¨¡å‹æˆ– SFT+LoRA\n",
    "â”‚   â”œâ”€â”€ Embeddings                     â”‚\n",
    "â”‚   â”œâ”€â”€ Transformer Layers             â”‚\n",
    "â”‚   â””â”€â”€ LM Head                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â–²\n",
    "           â”‚ ä¸¤ä¸ªå¤´ï¼šç­–ç•¥å¤´ + å€¼å‡½æ•°å¤´\n",
    "           â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   ç­–ç•¥å¤´ï¼ˆPolicy Headï¼‰               â”‚  â† åŸæœ‰çš„ï¼ˆç”¨äºç”Ÿæˆï¼‰\n",
    "â”‚   â””â”€â”€ LM Head (vocab_size)          â”‚\n",
    "â”‚                                      â”‚\n",
    "â”‚   å€¼å‡½æ•°å¤´ï¼ˆValue Headï¼‰               â”‚  â† æ–°æ·»åŠ çš„ï¼ˆç”¨äº PPOï¼‰\n",
    "â”‚   â”œâ”€â”€ Linear Layer 1 (hidden_size)  â”‚    å…³é”®ç»„ä»¶ï¼\n",
    "â”‚   â””â”€â”€ Output: 1 (çŠ¶æ€ä»·å€¼æ ‡é‡)       â”‚    å¤§å°ï¼š~å‡ MB\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "ä¿å­˜è·¯å¾„ï¼šoutputs/ppo_model/\n",
    "â”œâ”€â”€ config.json                       â† æ¨¡å‹é…ç½®\n",
    "â”œâ”€â”€ pytorch_model.bin                  â† ç­–ç•¥æ¨¡å‹æƒé‡\n",
    "â”‚   æˆ– adapter_model.safetensors      â† å¦‚æœç”¨äº† LoRAï¼ˆä»… LoRA æƒé‡ï¼‰\n",
    "â”œâ”€â”€ value_head/                        â† å€¼å‡½æ•°å¤´ï¼ˆå•ç‹¬ä¿å­˜ï¼‰\n",
    "â”‚   â”œâ”€â”€ config.json\n",
    "â”‚   â””â”€â”€ pytorch_model.bin\n",
    "â””â”€â”€ tokenizer.json                     â† Tokenizer\n",
    "```\n",
    "\n",
    "**ç‰¹ç‚¹**ï¼š\n",
    "- âœ… **å€¼å‡½æ•°å¤´ï¼ˆValue Headï¼‰**æ˜¯ PPO ç‰¹æœ‰çš„ç»„ä»¶\n",
    "  - ç”¨äºä¼°è®¡çŠ¶æ€ä»·å€¼ï¼ˆstate valueï¼‰ï¼Œè®¡ç®—ä¼˜åŠ¿å‡½æ•°ï¼ˆadvantageï¼‰\n",
    "  - è¾“å‡ºç»´åº¦ä¸º 1ï¼ˆæ ‡é‡ä»·å€¼ï¼‰\n",
    "- âœ… ç­–ç•¥æ¨¡å‹å¯ä»¥æ˜¯åŸºç¡€æ¨¡å‹æˆ–å¸¦ LoRA çš„æ¨¡å‹\n",
    "- âœ… å¦‚æœä½¿ç”¨ LoRAï¼Œå¯èƒ½åªä¿å­˜ LoRA æƒé‡ï¼›å€¼å‡½æ•°å¤´é€šå¸¸å…¨é‡ä¿å­˜\n",
    "\n",
    "**ä»£ç ç¤ºä¾‹**ï¼š\n",
    "```python\n",
    "# PPO æ¨¡å‹åˆ›å»º\n",
    "ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    base_model_or_dir,  # è‡ªåŠ¨æ·»åŠ å€¼å‡½æ•°å¤´\n",
    ")\n",
    "\n",
    "# PPO è®­ç»ƒåä¿å­˜\n",
    "ppo_trainer.save_model(ppo_output_dir)  # ä¿å­˜ç­–ç•¥æ¨¡å‹ + å€¼å‡½æ•°å¤´\n",
    "\n",
    "# ä½¿ç”¨æ—¶çš„åŠ è½½\n",
    "ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(ppo_output_dir)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š ä¸‰é˜¶æ®µæƒé‡å­˜å‚¨å¯¹æ¯”\n",
    "\n",
    "| é˜¶æ®µ | å­˜å‚¨å†…å®¹ | å­˜å‚¨å¤§å° | å…³é”®ç»„ä»¶ | èƒ½å¦ç‹¬ç«‹ä½¿ç”¨ |\n",
    "|------|---------|---------|---------|------------|\n",
    "| **SFT** | LoRA Adapter æƒé‡ | ~å‡ MBï¼ˆ0.1-1% å‚æ•°é‡ï¼‰ | LoRA_A, LoRA_B | âŒ éœ€è¦åŸºç¡€æ¨¡å‹ |\n",
    "| **RM** | åŸºç¡€æ¨¡å‹ + å¥–åŠ±å¤´ | ~å®Œæ•´æ¨¡å‹ï¼ˆæˆ– LoRAï¼‰ | å¥–åŠ±å¤´ï¼ˆReward Headï¼‰ | âœ… å¯ä»¥ç‹¬ç«‹ä½¿ç”¨ |\n",
    "| **PPO** | ç­–ç•¥æ¨¡å‹ + å€¼å‡½æ•°å¤´ | ~å®Œæ•´æ¨¡å‹ï¼ˆæˆ– LoRAï¼‰ | å€¼å‡½æ•°å¤´ï¼ˆValue Headï¼‰ | âœ… å¯ä»¥ç‹¬ç«‹ä½¿ç”¨ |\n",
    "\n",
    "## ğŸ”‘ å…³é”®åŒºåˆ«\n",
    "\n",
    "### SFT vs RM vs PPO çš„å­˜å‚¨å·®å¼‚\n",
    "\n",
    "1. **SFT**ï¼š\n",
    "   - åªä¿å­˜ LoRA æƒé‡ï¼ˆè½»é‡çº§ï¼‰\n",
    "   - ä¸ä¿®æ”¹åŸºç¡€æ¨¡å‹\n",
    "   - æ¨ç†æ—¶éœ€è¦åˆå¹¶\n",
    "\n",
    "2. **RM**ï¼š\n",
    "   - ä¿å­˜å¥–åŠ±å¤´ï¼ˆæ–°å¢çš„ Linear å±‚ï¼‰\n",
    "   - å¦‚æœå…¨é‡å¾®è°ƒï¼Œä¿å­˜å®Œæ•´æ¨¡å‹ï¼›å¦‚æœ LoRAï¼Œä¿å­˜ LoRA + å¥–åŠ±å¤´é…ç½®\n",
    "   - å¯ä»¥ç‹¬ç«‹ç”¨äºæ‰“åˆ†\n",
    "\n",
    "3. **PPO**ï¼š\n",
    "   - ä¿å­˜å€¼å‡½æ•°å¤´ï¼ˆæ–°å¢çš„ç»„ä»¶ï¼Œç”¨äº RLï¼‰\n",
    "   - ç­–ç•¥æ¨¡å‹å¯ä»¥ä¿æŒ LoRA æ ¼å¼æˆ–å…¨é‡ä¿å­˜\n",
    "   - å¯ä»¥ç‹¬ç«‹ç”¨äºç”Ÿæˆï¼ˆä½†å€¼å‡½æ•°å¤´åªåœ¨è®­ç»ƒæ—¶ä½¿ç”¨ï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64e31f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã€PPO è®­ç»ƒé…ç½®ã€‘\n",
    "# PPO è®­ç»ƒå‚æ•°ï¼šæ§åˆ¶å¼ºåŒ–å­¦ä¹ è®­ç»ƒæµç¨‹\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    # è¾“å‡ºä¸ä¿å­˜\n",
    "    output_dir=ppo_output_dir,              # æ¨¡å‹/æ—¥å¿—è¾“å‡ºç›®å½•\n",
    "    save_steps=500,                         # æ¯ N æ­¥ä¿å­˜ä¸€æ¬¡ checkpoint\n",
    "    save_total_limit=2,                     # æœ€å¤šä¿ç•™ N ä¸ª checkpoint\n",
    "    \n",
    "    # ç”Ÿæˆå‚æ•°ï¼ˆç­–ç•¥æ¨¡å‹ç”Ÿæˆå›ç­”æ—¶çš„å‚æ•°ï¼‰\n",
    "    mini_batch_size=1,                     # PPO mini-batch å¤§å°ï¼ˆæ¯ä¸ª prompt çš„å¤„ç†æ‰¹æ¬¡ï¼‰\n",
    "    batch_size=8,                          # PPO batch å¤§å°ï¼ˆæ”¶é›†ç»éªŒçš„æ‰¹æ¬¡ï¼‰\n",
    "    gradient_accumulation_steps=1,         # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
    "    \n",
    "    # PPO ç®—æ³•å‚æ•°\n",
    "    ppo_epochs=4,                           # PPO æ›´æ–°è½®æ¬¡ï¼ˆæ¯æ¬¡æ”¶é›†ç»éªŒåæ›´æ–°å¤šå°‘è½®ï¼‰\n",
    "    learning_rate=1e-6,                     # PPO å­¦ä¹ ç‡ï¼ˆé€šå¸¸è¾ƒå°ï¼Œ1e-6 åˆ° 1e-5ï¼‰\n",
    "    lr_scheduler_type=\"linear\",             # å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹\n",
    "    warmup_ratio=0.1,                       # warmup æ¯”ä¾‹\n",
    "    \n",
    "    # å¥–åŠ±ä¸ KL çº¦æŸ\n",
    "    init_kl_coef=0.1,                       # åˆå§‹ KL æƒ©ç½šç³»æ•°ï¼ˆå¹³è¡¡å¥–åŠ±ä¸ KL æ•£åº¦ï¼‰\n",
    "    target=6.0,                             # KL æ•£åº¦ç›®æ ‡å€¼ï¼ˆæ§åˆ¶ä¸å‚è€ƒæ¨¡å‹çš„åç¦»ç¨‹åº¦ï¼‰\n",
    "    horizon=10000,                          # PPO horizonï¼ˆå¥–åŠ±å½’ä¸€åŒ–å‚æ•°ï¼‰\n",
    "    gamma=1.0,                              # æŠ˜æ‰£å› å­ï¼ˆRL ä¸­çš„æœªæ¥å¥–åŠ±æŠ˜æ‰£ï¼‰\n",
    "    lam=0.95,                               # GAE lambda å‚æ•°ï¼ˆä¼˜åŠ¿ä¼°è®¡ï¼‰\n",
    "    \n",
    "    # ç”Ÿæˆå‚æ•°ï¼ˆæ¯ä¸ª prompt ç”Ÿæˆå¤šå°‘ä¸ªå€™é€‰å›ç­”ï¼‰\n",
    "    num_padding_at_beginning=1,             # å¡«å……ä½ç½®ï¼ˆæŸäº›æ¨¡å‹éœ€è¦ï¼‰\n",
    "    \n",
    "    # è®­ç»ƒæ§åˆ¶\n",
    "    max_grad_norm=1.0,                      # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰\n",
    "    report_to=[\"none\"],                     # ä¸å‘å¤–éƒ¨ä¸ŠæŠ¥ï¼ˆå¯æ”¹ä¸º [\"wandb\"] ç­‰ï¼‰\n",
    "    \n",
    "    # è¯„ä¼°ä¸æ—¥å¿—\n",
    "    log_with=\"none\",                        # æ—¥å¿—è®°å½•å·¥å…·ï¼ˆ\"wandb\", \"tensorboard\" ç­‰ï¼‰\n",
    "    logging_steps=10,                       # æ¯ N æ­¥æ‰“å°ä¸€æ¬¡æ—¥å¿—\n",
    "    \n",
    "    # æ€§èƒ½ä¼˜åŒ–\n",
    "    bf16=use_cuda,                          # CUDA ä¸Šç”¨ bfloat16\n",
    "    fp16=False,                             # ç¦ç”¨ FP16\n",
    "    \n",
    "    # åºåˆ—é•¿åº¦\n",
    "    max_length=max_seq_length,              # æœ€å¤§åºåˆ—é•¿åº¦\n",
    "    max_new_tokens=512,                    # æ¯æ¬¡ç”Ÿæˆçš„æœ€å¤§æ–° token æ•°\n",
    ")\n",
    "\n",
    "print(\"[PPO] è®­ç»ƒé…ç½®å®Œæˆ\")\n",
    "print(f\"[PPO] å…³é”®å‚æ•°ï¼š\")\n",
    "print(f\"  - PPO epochs: {ppo_config.ppo_epochs}ï¼ˆæ¯æ¬¡æ›´æ–° {ppo_config.ppo_epochs} è½®ï¼‰\")\n",
    "print(f\"  - Batch size: {ppo_config.batch_size}ï¼ˆæ”¶é›†ç»éªŒçš„æ‰¹æ¬¡å¤§å°ï¼‰\")\n",
    "print(f\"  - Mini batch size: {ppo_config.mini_batch_size}ï¼ˆPPO æ›´æ–°çš„å°æ‰¹æ¬¡ï¼‰\")\n",
    "print(f\"  - Learning rate: {ppo_config.learning_rate}ï¼ˆPPO å­¦ä¹ ç‡ï¼‰\")\n",
    "print(f\"  - KL coefficient: {ppo_config.init_kl_coef}ï¼ˆKL æƒ©ç½šç³»æ•°ï¼‰\")\n",
    "print(f\"  - Target KL: {ppo_config.target}ï¼ˆç›®æ ‡ KL æ•£åº¦ï¼‰\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1755d942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã€PPO è®­ç»ƒå™¨æ„é€ ä¸è®­ç»ƒæµç¨‹è¯´æ˜ã€‘\n",
    "from trl import PPOTrainer\n",
    "\n",
    "print(\"[PPO] æ„é€  PPO è®­ç»ƒå™¨...\")\n",
    "\n",
    "# æ³¨æ„ï¼šåœ¨æ„é€ è®­ç»ƒå™¨ä¹‹å‰ï¼Œéœ€è¦ç¡®ä¿å¥–åŠ±æ¨¡å‹å·²åŠ è½½\n",
    "if reward_model is None:\n",
    "    print(\"[PPO] âš ï¸  è­¦å‘Šï¼šå¥–åŠ±æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•è¿›è¡Œ PPO è®­ç»ƒ\")\n",
    "    print(\"[PPO] è¯·å…ˆå®Œæˆ RM è®­ç»ƒï¼Œç¡®ä¿ rm_model_dir æŒ‡å‘æœ‰æ•ˆçš„å¥–åŠ±æ¨¡å‹è·¯å¾„\")\n",
    "else:\n",
    "    # æ„é€  PPO è®­ç»ƒå™¨\n",
    "    ppo_trainer = PPOTrainer(\n",
    "        config=ppo_config,\n",
    "        model=ppo_model,                    # ç­–ç•¥æ¨¡å‹ï¼ˆå¸¦å€¼å‡½æ•°å¤´ï¼‰\n",
    "        ref_model=reference_model,          # å‚è€ƒæ¨¡å‹ï¼ˆç”¨äº KL çº¦æŸï¼‰\n",
    "        tokenizer=tokenizer,\n",
    "        dataset=ppo_dataset,                # æç¤ºæ•°æ®é›†\n",
    "    )\n",
    "    \n",
    "    print(\"[PPO] è®­ç»ƒå™¨æ„é€ å®Œæˆ\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"[PPO] PPO è®­ç»ƒæµç¨‹è¯´æ˜ï¼š\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\"\"\n",
    "    PPOï¼ˆProximal Policy Optimizationï¼‰çš„è®­ç»ƒæµç¨‹ï¼š\n",
    "    \n",
    "    1ï¸âƒ£ ã€æ”¶é›†ç»éªŒé˜¶æ®µã€‘ï¼ˆExperience Collectionï¼‰\n",
    "       - ç­–ç•¥æ¨¡å‹æ ¹æ® prompts ç”Ÿæˆå›ç­”\n",
    "       - å¥–åŠ±æ¨¡å‹å¯¹ç”Ÿæˆçš„å›ç­”æ‰“åˆ†ï¼ˆrewardï¼‰\n",
    "       - è®¡ç®—æ¯ä¸ªå›ç­”çš„ä¼˜åŠ¿ï¼ˆadvantageï¼‰å’Œä»·å€¼ï¼ˆvalueï¼‰\n",
    "       - æ”¶é›†ä¸€ä¸ª batch çš„ç»éªŒï¼ˆprompts, responses, rewards, advantagesï¼‰\n",
    "    \n",
    "    2ï¸âƒ£ ã€PPO æ›´æ–°é˜¶æ®µã€‘ï¼ˆPolicy Updateï¼‰\n",
    "       - å¯¹æ”¶é›†çš„ç»éªŒè¿›è¡Œå¤šè½® PPO æ›´æ–°ï¼ˆppo_epochs è½®ï¼‰\n",
    "       - è®¡ç®—ç­–ç•¥æŸå¤±ï¼ˆpolicy lossï¼‰ï¼šæœ€å¤§åŒ–æœŸæœ›å¥–åŠ±\n",
    "       - è®¡ç®—ä»·å€¼æŸå¤±ï¼ˆvalue lossï¼‰ï¼šä»·å€¼å‡½æ•°çš„å›å½’æŸå¤±\n",
    "       - è®¡ç®— KL æ•£åº¦æŸå¤±ï¼ˆKL penaltyï¼‰ï¼šä¿æŒä¸å‚è€ƒç­–ç•¥çš„æ¥è¿‘\n",
    "       - æ€»æŸå¤± = policy_loss - value_loss + kl_penalty\n",
    "       - ä½¿ç”¨æ¢¯åº¦ä¸Šå‡ä¼˜åŒ–ç­–ç•¥å‚æ•°\n",
    "    \n",
    "    3ï¸âƒ£ ã€KL çº¦æŸæ§åˆ¶ã€‘ï¼ˆKL Divergence Controlï¼‰\n",
    "       - åŠ¨æ€è°ƒæ•´ KL æƒ©ç½šç³»æ•°ï¼ˆkl_coefï¼‰\n",
    "       - å¦‚æœ KL æ•£åº¦è¶…è¿‡ç›®æ ‡å€¼ï¼Œå¢åŠ æƒ©ç½š\n",
    "       - å¦‚æœ KL æ•£åº¦ä½äºç›®æ ‡å€¼ï¼Œå‡å°‘æƒ©ç½š\n",
    "       - ç¡®ä¿ç­–ç•¥ä¸ä¼šåç¦»å‚è€ƒæ¨¡å‹å¤ªè¿œï¼ˆé¿å…æ¨¡å¼åç¼©ï¼‰\n",
    "    \n",
    "    4ï¸âƒ£ ã€é‡å¤è¿­ä»£ã€‘\n",
    "       - é‡å¤æ­¥éª¤ 1-3ï¼Œç›´åˆ°ç­–ç•¥æ”¶æ•›æˆ–è¾¾åˆ°æœ€å¤§æ­¥æ•°\n",
    "       - å®šæœŸä¿å­˜ checkpoint\n",
    "       - ç›‘æ§å¥–åŠ±ã€KL æ•£åº¦ã€ç”Ÿæˆè´¨é‡ç­‰æŒ‡æ ‡\n",
    "    \n",
    "    å…³é”®ä¼˜åŠ¿ï¼š\n",
    "    - âœ… ç¨³å®šçš„ç­–ç•¥æ›´æ–°ï¼ˆå‰ªåˆ‡ç­–ç•¥æ¢¯åº¦ï¼Œé¿å…å¤§å¹…æ³¢åŠ¨ï¼‰\n",
    "    - âœ… KL çº¦æŸé˜²æ­¢æ¨¡å¼åç¼©\n",
    "    - âœ… æ”¯æŒåœ¨çº¿å­¦ä¹ ï¼ˆè¾¹ç”Ÿæˆè¾¹ä¼˜åŒ–ï¼‰\n",
    "    - âœ… é€‚ç”¨äºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹\n",
    "    \n",
    "    æ³¨æ„äº‹é¡¹ï¼š\n",
    "    - âš ï¸  éœ€è¦é«˜è´¨é‡å¥–åŠ±æ¨¡å‹ï¼ˆRM çš„è´¨é‡ç›´æ¥å½±å“ PPO æ•ˆæœï¼‰\n",
    "    - âš ï¸  KL ç³»æ•°éœ€è¦ä»”ç»†è°ƒä¼˜ï¼ˆè¿‡å¤§å¯¼è‡´æ›´æ–°æ…¢ï¼Œè¿‡å°å¯¼è‡´åç¦»ï¼‰\n",
    "    - âš ï¸  ç›‘æ§é•¿åº¦åç½®ï¼ˆæ¨¡å‹å¯èƒ½å€¾å‘äºç”Ÿæˆæ›´é•¿çš„å›ç­”ä»¥è·å¾—æ›´é«˜å¥–åŠ±ï¼‰\n",
    "    - âš ï¸  é˜²æ­¢è¿‡æ‹Ÿåˆï¼ˆéœ€è¦å®šæœŸè¯„ä¼°ç”Ÿæˆè´¨é‡ï¼‰\n",
    "    \"\"\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # æ„é€ å¥–åŠ±å‡½æ•°ï¼ˆç”¨äº PPO è®­ç»ƒï¼‰\n",
    "    def reward_function(samples):\n",
    "        \"\"\"å¥–åŠ±å‡½æ•°ï¼šä½¿ç”¨å¥–åŠ±æ¨¡å‹å¯¹ç”Ÿæˆçš„å›ç­”æ‰“åˆ†\"\"\"\n",
    "        # å¯¹æ¯ä¸ªç”Ÿæˆçš„å›ç­”ä½¿ç”¨å¥–åŠ±æ¨¡å‹æ‰“åˆ†\n",
    "        inputs = tokenizer(\n",
    "            samples,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "        ).to(reward_model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # å¥–åŠ±æ¨¡å‹è¿”å› logitsï¼ˆæ ‡é‡å¥–åŠ±åˆ†æ•°ï¼‰\n",
    "            rewards = reward_model(**inputs).logits.squeeze(-1)\n",
    "        \n",
    "        return rewards\n",
    "    \n",
    "    print(\"\\n[PPO] å¥–åŠ±å‡½æ•°å·²å®šä¹‰ï¼ˆä½¿ç”¨å¥–åŠ±æ¨¡å‹æ‰“åˆ†ï¼‰\")\n",
    "    print(\"[PPO] è®­ç»ƒå™¨å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒ\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"[PPO] è®­ç»ƒç¤ºä¾‹ä»£ç ï¼ˆæ³¨é‡Šæ‰ï¼Œä¸å®é™…æ‰§è¡Œï¼‰ï¼š\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    example_code = '''\n",
    "    # å¼€å§‹ PPO è®­ç»ƒå¾ªç¯\n",
    "    generation_kwargs = {\n",
    "        \"max_new_tokens\": 512,              # ç”Ÿæˆçš„æœ€å¤§æ–° token æ•°\n",
    "        \"temperature\": 1.0,                # ç”Ÿæˆæ¸©åº¦ï¼ˆæ§åˆ¶éšæœºæ€§ï¼‰\n",
    "        \"do_sample\": True,                  # æ˜¯å¦é‡‡æ ·\n",
    "        \"top_p\": 0.95,                      # nucleus sampling\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    }\n",
    "    \n",
    "    # PPO è®­ç»ƒå¾ªç¯\n",
    "    for epoch in range(1):  # å¯ä»¥è®¾ç½®å¤šä¸ª epoch\n",
    "        for batch in ppo_trainer.dataloader:\n",
    "            # 1. ç­–ç•¥æ¨¡å‹ç”Ÿæˆå›ç­”\n",
    "            query_tensors = batch[\"input_ids\"]\n",
    "            response_tensors = ppo_trainer.generate(\n",
    "                query_tensors,\n",
    "                return_prompt=False,\n",
    "                length_sampler=None,\n",
    "                batch_size=ppo_config.batch_size,\n",
    "                **generation_kwargs,\n",
    "            )\n",
    "            \n",
    "            # 2. æå–ç”Ÿæˆçš„å›ç­”æ–‡æœ¬\n",
    "            batch[\"response\"] = [\n",
    "                tokenizer.decode(r.squeeze(), skip_special_tokens=True)\n",
    "                for r in response_tensors\n",
    "            ]\n",
    "            \n",
    "            # 3. è®¡ç®—å¥–åŠ±ï¼ˆä½¿ç”¨å¥–åŠ±æ¨¡å‹ï¼‰\n",
    "            texts = [\n",
    "                q + r for q, r in zip(batch[\"query\"], batch[\"response\"])\n",
    "            ]\n",
    "            rewards = reward_function(texts)\n",
    "            \n",
    "            # 4. PPO æ›´æ–°\n",
    "            stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "            ppo_trainer.log_stats(stats, batch, rewards)\n",
    "    \n",
    "    # ä¿å­˜æœ€ç»ˆæ¨¡å‹\n",
    "    ppo_trainer.save_model(ppo_output_dir)\n",
    "    tokenizer.save_pretrained(ppo_output_dir)\n",
    "    print(f\"[Done] PPO è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜è‡³: {ppo_output_dir}\")\n",
    "    '''\n",
    "    \n",
    "    print(example_code)\n",
    "    print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336dc170",
   "metadata": {},
   "source": [
    "## DPOï¼ˆDirect Preference Optimizationï¼‰\n",
    "- **å®šä½**ï¼šä½œä¸ºï¼ˆRM+PPOï¼‰çš„å¸¸è§æ›¿ä»£æ–¹æ¡ˆï¼Œç”¨åå¥½å¯¹ç›´æ¥ä¼˜åŒ–ç­–ç•¥ã€‚\n",
    "- **æ ¸å¿ƒ**ï¼šåŸºäº \\((x, y_{pos}, y_{neg})\\) æé«˜ \\(y_{pos}\\) æ¦‚ç‡ã€é™ä½ \\(y_{neg}\\)ï¼Œå¹¶ä»¥å‚è€ƒç­–ç•¥ \\(\\pi_{ref}\\) çš„å¯¹æ•°æ¦‚ç‡å·®ä½œéšå¼ KL çº¦æŸã€‚\n",
    "- **ç›´è§‚ç›®æ ‡**ï¼šæœ€å°åŒ– \\(-\\log \\sigma\\big(\\beta[(\\log \\pi_\\theta(y_{pos}|x) - \\log \\pi_\\theta(y_{neg}|x)) - (\\log \\pi_{ref}(y_{pos}|x) - \\log \\pi_{ref}(y_{neg}|x))]\\big)\\)\n",
    "- **ä¼˜ç‚¹**ï¼šæµç¨‹ç®€å•ã€æ— å¥–åŠ±æ¨¡å‹ä¸ RL å›è·¯ã€ç¨³å®šæ˜“å¤ç°ã€ååé«˜ã€‚\n",
    "- **å±€é™**ï¼šä¾èµ–é«˜è´¨é‡åå¥½æ•°æ®ï¼›æç«¯åˆ†å¸ƒè¿ç§»ä¸‹å¯æ§æ€§è¾ƒå¼±ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcb38e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã€DPO æ•°æ®å‡†å¤‡ï¼šå¤ç”¨åå¥½æ•°æ®é›†ã€‘\n",
    "# DPO ä½¿ç”¨ä¸ RM ç›¸åŒçš„åå¥½æ•°æ®æ ¼å¼ï¼ˆprompt, chosen, rejectedï¼‰\n",
    "# å¯ä»¥ç›´æ¥å¤ç”¨ä¹‹å‰åŠ è½½çš„ RM æ•°æ®é›†\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "print(\"[DPO] å‡†å¤‡åå¥½æ•°æ®...\")\n",
    "\n",
    "# æ–¹å¼ 1ï¼šç›´æ¥å¤ç”¨ RM è®­ç»ƒæ•°æ®ï¼ˆæ¨èï¼‰\n",
    "if 'rm_train_ds' in globals() and len(rm_train_ds) > 0:\n",
    "    # å¤ç”¨ RM çš„åå¥½æ•°æ®é›†ï¼ˆå·²ç»æ˜¯ prompt, chosen, rejected æ ¼å¼ï¼‰\n",
    "    dpo_train_ds = rm_train_ds.select(range(min(5000, len(rm_train_ds))))  # å¯ä»¥é€‰æ‹©éƒ¨åˆ†æ•°æ®\n",
    "    dpo_eval_ds = rm_eval_ds.select(range(min(200, len(rm_eval_ds))))\n",
    "    print(f\"[DPO] å¤ç”¨ RM è®­ç»ƒæ•°æ®ï¼šè®­ç»ƒé›† {len(dpo_train_ds)} æ¡ï¼ŒéªŒè¯é›† {len(dpo_eval_ds)} æ¡\")\n",
    "else:\n",
    "    # æ–¹å¼ 2ï¼šé‡æ–°åŠ è½½åå¥½æ•°æ®ï¼ˆå¦‚æœ RM æ•°æ®ä¸å¯ç”¨ï¼‰\n",
    "    print(\"[DPO] RM æ•°æ®ä¸å¯ç”¨ï¼Œå°è¯•é‡æ–°åŠ è½½åå¥½æ•°æ®...\")\n",
    "    if 'rm_ds' in globals():\n",
    "        dpo_split = rm_ds.train_test_split(test_size=0.02, seed=42)\n",
    "        dpo_train_ds = dpo_split[\"train\"].select(range(min(5000, len(dpo_split[\"train\"]))))\n",
    "        dpo_eval_ds = dpo_split[\"test\"].select(range(min(200, len(dpo_split[\"test\"]))))\n",
    "        print(f\"[DPO] ä» rm_ds åŠ è½½ï¼šè®­ç»ƒé›† {len(dpo_train_ds)} æ¡ï¼ŒéªŒè¯é›† {len(dpo_eval_ds)} æ¡\")\n",
    "    else:\n",
    "        # æ–¹å¼ 3ï¼šä½¿ç”¨ç¤ºä¾‹æ•°æ®ï¼ˆä»…ç”¨äºæ¼”ç¤ºï¼‰\n",
    "        print(\"[DPO] âš ï¸  è­¦å‘Šï¼šä½¿ç”¨ç¤ºä¾‹æ•°æ®ï¼Œå®é™…è®­ç»ƒåº”ä½¿ç”¨çœŸå®åå¥½æ•°æ®\")\n",
    "        dpo_examples = [\n",
    "            {\n",
    "                \"prompt\": \"è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ã€‚\",\n",
    "                \"chosen\": \"æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ è§„å¾‹ï¼Œæ— éœ€æ˜ç¡®ç¼–ç¨‹å°±èƒ½åšå‡ºé¢„æµ‹æˆ–å†³ç­–ã€‚\",\n",
    "                \"rejected\": \"æœºå™¨å­¦ä¹ å°±æ˜¯ä¸€ç§ç¼–ç¨‹æ–¹å¼ã€‚\"\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": \"å¦‚ä½•å­¦ä¹  Pythonï¼Ÿ\",\n",
    "                \"chosen\": \"å­¦ä¹  Python å¯ä»¥ä»åŸºç¡€è¯­æ³•å¼€å§‹ï¼Œç„¶åé€æ­¥å­¦ä¹ æ•°æ®ç»“æ„ã€é¢å‘å¯¹è±¡ç¼–ç¨‹ï¼Œå¹¶é€šè¿‡å®é™…é¡¹ç›®ç»ƒä¹ æå‡ã€‚\",\n",
    "                \"rejected\": \"Python å¾ˆç®€å•ï¼Œéšä¾¿å­¦å­¦å°±è¡Œã€‚\"\n",
    "            },\n",
    "        ]\n",
    "        dpo_train_ds = Dataset.from_list(dpo_examples)\n",
    "        dpo_eval_ds = Dataset.from_list(dpo_examples[:1])\n",
    "\n",
    "print(f\"\\n[DPO] æ•°æ®å‡†å¤‡å®Œæˆï¼š\")\n",
    "print(f\"  è®­ç»ƒé›†ï¼š{len(dpo_train_ds)} æ¡\")\n",
    "print(f\"  éªŒè¯é›†ï¼š{len(dpo_eval_ds)} æ¡\")\n",
    "print(f\"\\n[DPO] ç¤ºä¾‹æ•°æ®ï¼š\")\n",
    "sample = dpo_train_ds[0]\n",
    "for k, v in sample.items():\n",
    "    if isinstance(v, str) and len(v) > 100:\n",
    "        print(f\"  {k}: {v[:100]}...\")\n",
    "    else:\n",
    "        print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6a0c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã€DPO æ¨¡å‹åŠ è½½ï¼šç­–ç•¥æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹ã€‘\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "import torch\n",
    "\n",
    "# DPO è¾“å‡ºé…ç½®\n",
    "dpo_output_dir = \"outputs/dpo_qlora\"\n",
    "max_seq_length = 2048\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_mps = torch.backends.mps.is_available()\n",
    "\n",
    "print(\"[DPO] å¼€å§‹åŠ è½½æ¨¡å‹...\")\n",
    "\n",
    "# ========== 1. åŠ è½½ç­–ç•¥æ¨¡å‹ï¼ˆé€šå¸¸æ˜¯ SFT æ¨¡å‹ï¼‰==========\n",
    "# DPO éœ€è¦ç­–ç•¥æ¨¡å‹ï¼ˆè®­ç»ƒä¸­çš„æ¨¡å‹ï¼‰å’Œå‚è€ƒæ¨¡å‹ï¼ˆç”¨äº KL çº¦æŸï¼Œå†»ç»“å‚æ•°ï¼‰\n",
    "if 'adapter_dir' in globals() and os.path.exists(adapter_dir):\n",
    "    try:\n",
    "        # ä» SFT æ¨¡å‹åŠ è½½ï¼ˆå¸¦ LoRA æƒé‡ï¼‰\n",
    "        base_policy = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_or_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        dpo_policy_model = PeftModel.from_pretrained(base_policy, adapter_dir)\n",
    "        print(\"[DPO] ç­–ç•¥æ¨¡å‹ï¼šä½¿ç”¨ SFT æ¨¡å‹ï¼ˆå¸¦ LoRA æƒé‡ï¼‰\")\n",
    "    except Exception as e:\n",
    "        print(f\"[DPO] æ— æ³•åŠ è½½ SFT æ¨¡å‹ï¼š{e}ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹\")\n",
    "        dpo_policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_or_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "else:\n",
    "    # å¦‚æœæœªæ‰¾åˆ° SFT æ¨¡å‹ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹\n",
    "    dpo_policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_or_dir,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    print(\"[DPO] ç­–ç•¥æ¨¡å‹ï¼šä½¿ç”¨åŸºç¡€æ¨¡å‹ï¼ˆæœªæ‰¾åˆ° SFT æ¨¡å‹ï¼‰\")\n",
    "\n",
    "# ========== 2. åŠ è½½å‚è€ƒæ¨¡å‹ï¼ˆç”¨äº KL çº¦æŸï¼Œå†»ç»“å‚æ•°ï¼‰==========\n",
    "# å‚è€ƒæ¨¡å‹é€šå¸¸æ˜¯ SFT æ¨¡å‹çš„å‰¯æœ¬ï¼Œä½†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å†»ç»“å‚æ•°\n",
    "if 'adapter_dir' in globals() and os.path.exists(adapter_dir):\n",
    "    try:\n",
    "        # åŠ è½½å‚è€ƒæ¨¡å‹ï¼ˆä¸ç­–ç•¥æ¨¡å‹ç›¸åŒï¼Œä½†å†»ç»“å‚æ•°ï¼‰\n",
    "        base_ref = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_or_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        # å¯¹äº LoRA æ¨¡å‹ï¼Œå¯ä»¥åŠ è½½ç›¸åŒçš„ adapterï¼Œç„¶ååœ¨è®­ç»ƒå™¨ä¸­å†»ç»“\n",
    "        dpo_ref_model = PeftModel.from_pretrained(base_ref, adapter_dir)\n",
    "        # å†»ç»“å‚è€ƒæ¨¡å‹çš„å‚æ•°ï¼ˆDPOTrainer ä¼šè‡ªåŠ¨å¤„ç†ï¼‰\n",
    "        print(\"[DPO] å‚è€ƒæ¨¡å‹ï¼šä½¿ç”¨ SFT æ¨¡å‹ï¼ˆå¸¦ LoRA æƒé‡ï¼Œè®­ç»ƒæ—¶å†»ç»“ï¼‰\")\n",
    "    except Exception as e:\n",
    "        print(f\"[DPO] æ— æ³•åŠ è½½ SFT æ¨¡å‹ä½œä¸ºå‚è€ƒï¼š{e}ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹\")\n",
    "        dpo_ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_or_dir,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "else:\n",
    "    # å¦‚æœæœªæ‰¾åˆ° SFT æ¨¡å‹ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹\n",
    "    dpo_ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_or_dir,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    print(\"[DPO] å‚è€ƒæ¨¡å‹ï¼šä½¿ç”¨åŸºç¡€æ¨¡å‹ï¼ˆæœªæ‰¾åˆ° SFT æ¨¡å‹ï¼‰\")\n",
    "\n",
    "# Tokenizer åˆå§‹åŒ–ï¼ˆå¦‚æœä¹‹å‰æœªå®šä¹‰ï¼‰\n",
    "if 'tokenizer' not in globals() or tokenizer is None:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        base_model_or_dir,\n",
    "        use_fast=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"[DPO] æ¨¡å‹åŠ è½½å®Œæˆ\")\n",
    "print(f\"[DPO] ç­–ç•¥æ¨¡å‹ï¼š{type(dpo_policy_model).__name__}\")\n",
    "print(f\"[DPO] å‚è€ƒæ¨¡å‹ï¼š{type(dpo_ref_model).__name__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef44db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã€DPO LoRA é…ç½®ä¸è®­ç»ƒå‚æ•°ã€‘\n",
    "# DPO å¯ä»¥ä½¿ç”¨ LoRA å¾®è°ƒï¼Œä¹Ÿå¯ä»¥å…¨é‡å¾®è°ƒ\n",
    "\n",
    "# LoRA é…ç½®ï¼ˆå¦‚æœéœ€è¦ä½¿ç”¨ LoRAï¼‰\n",
    "dpo_lora_config = None\n",
    "if True:  # é»˜è®¤ä½¿ç”¨ LoRAï¼ˆæ›´èŠ‚çœæ˜¾å­˜ï¼‰\n",
    "    dpo_lora_config = LoraConfig(\n",
    "        r=16,                    # ç§©ï¼ˆrankï¼‰\n",
    "        lora_alpha=32,          # ç¼©æ”¾ç³»æ•°ï¼ˆé€šå¸¸ alpha=2*rï¼‰\n",
    "        lora_dropout=0.05,      # é€‚é…å™¨å±‚ dropout\n",
    "        bias=\"none\",            # ä¸è®­ç»ƒ bias\n",
    "        task_type=\"CAUSAL_LM\",  # ä»»åŠ¡ç±»å‹ï¼šå› æœè¯­è¨€æ¨¡å‹\n",
    "    )\n",
    "    print(\"[DPO] ä½¿ç”¨ LoRA é…ç½®ï¼ˆr=16, alpha=32ï¼‰\")\n",
    "else:\n",
    "    print(\"[DPO] ä½¿ç”¨å…¨é‡å¾®è°ƒï¼ˆä¸ä½¿ç”¨ LoRAï¼‰\")\n",
    "\n",
    "# bitsandbytes QLoRA é…ç½®ï¼ˆå¯é€‰ï¼‰\n",
    "dpo_quantization_config = None\n",
    "if use_cuda:\n",
    "    try:\n",
    "        dpo_quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "        print(\"[DPO] ä½¿ç”¨ bitsandbytes 4-bit é‡åŒ–åŠ è½½æ¨¡å‹\")\n",
    "    except Exception as e:\n",
    "        print(f\"[DPO] æœªå¯ç”¨4bité‡åŒ–ï¼š{e}\")\n",
    "else:\n",
    "    print(\"[DPO] å½“å‰ä¸ºé CUDA ç¯å¢ƒï¼Œä½¿ç”¨å¸¸è§„ç²¾åº¦åŠ è½½\")\n",
    "\n",
    "# DPO è®­ç»ƒå‚æ•°ï¼šæ§åˆ¶è®­ç»ƒæµç¨‹ä¸ä¼˜åŒ–\n",
    "dpo_args = TrainingArguments(\n",
    "    # è¾“å‡ºä¸ä¿å­˜\n",
    "    output_dir=dpo_output_dir,              # æ¨¡å‹/æ—¥å¿—è¾“å‡ºç›®å½•\n",
    "    save_steps=500,                          # æ¯ N æ­¥ä¿å­˜ä¸€æ¬¡ checkpoint\n",
    "    save_total_limit=2,                      # æœ€å¤šä¿ç•™ N ä¸ª checkpoint\n",
    "    save_safetensors=True,                   # ä½¿ç”¨ safetensors æ ¼å¼ä¿å­˜\n",
    "    \n",
    "    # æ‰¹å¤§å°ä¸æ¢¯åº¦\n",
    "    per_device_train_batch_size=2,          # æ¯è®¾å¤‡è®­ç»ƒ batch å¤§å°ï¼ˆDPO éœ€è¦å¤„ç† chosen/rejected å¯¹ï¼‰\n",
    "    per_device_eval_batch_size=2,           # æ¯è®¾å¤‡éªŒè¯ batch å¤§å°\n",
    "    gradient_accumulation_steps=4,          # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆç­‰æ•ˆ batch = 2 * 4 = 8ï¼‰\n",
    "    \n",
    "    # è®­ç»ƒè½®æ¬¡ä¸å­¦ä¹ ç‡\n",
    "    num_train_epochs=1,                      # è®­ç»ƒè½®æ¬¡æ•°\n",
    "    learning_rate=1e-5,                     # åˆå§‹å­¦ä¹ ç‡ï¼ˆDPO å¸¸ç”¨ 1e-5 åˆ° 5e-5ï¼‰\n",
    "    lr_scheduler_type=\"cosine\",             # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆä½™å¼¦é€€ç«ï¼‰\n",
    "    warmup_ratio=0.1,                       # warmup æ¯”ä¾‹ï¼ˆå‰ 10% æ­¥æ•°çº¿æ€§å¢é•¿ LRï¼‰\n",
    "    \n",
    "    # è¯„ä¼°ä¸æ—¥å¿—\n",
    "    eval_strategy=\"steps\",                  # è¯„ä¼°ç­–ç•¥ï¼ˆ\"steps\"/\"epoch\"/\"no\"ï¼‰\n",
    "    eval_steps=250,                         # æ¯ N æ­¥è¯„ä¼°ä¸€æ¬¡\n",
    "    logging_steps=50,                       # æ¯ N æ­¥æ‰“å°ä¸€æ¬¡æ—¥å¿—\n",
    "    report_to=[\"none\"],                     # ä¸å‘å¤–éƒ¨ä¸ŠæŠ¥ï¼ˆå¯æ”¹ä¸º [\"wandb\"] ç­‰ï¼‰\n",
    "    \n",
    "    # æ¨¡å‹æ£€æŸ¥ç‚¹\n",
    "    load_best_model_at_end=True,            # è®­ç»ƒç»“æŸåŠ è½½æœ€ä½³æ¨¡å‹\n",
    "    metric_for_best_model=\"eval_loss\",     # æœ€ä½³æ¨¡å‹æŒ‡æ ‡\n",
    "    greater_is_better=False,                # è¯¥æŒ‡æ ‡è¶Šå°è¶Šå¥½\n",
    "    \n",
    "    # æ€§èƒ½ä¼˜åŒ–\n",
    "    bf16=use_cuda,                          # CUDA ä¸Šç”¨ bfloat16\n",
    "    fp16=False,                             # ç¦ç”¨ FP16\n",
    "    gradient_checkpointing=True,           # æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆç‰ºç‰²æ—¶é—´æ¢æ˜¾å­˜ï¼‰\n",
    ")\n",
    "\n",
    "# DPO ç‰¹å®šé…ç½®ï¼ˆbeta æ¸©åº¦å‚æ•°ï¼‰\n",
    "dpo_config = DPOConfig(\n",
    "    beta=0.1,                               # æ¸©åº¦å‚æ•°ï¼ˆæ§åˆ¶ KL çº¦æŸå¼ºåº¦ï¼Œå¸¸ç”¨ 0.1-0.5ï¼‰\n",
    "    loss_type=\"sigmoid\",                    # æŸå¤±ç±»å‹ï¼ˆ\"sigmoid\" æˆ– \"hinge\"ï¼‰\n",
    "    label_smoothing=0.0,                    # æ ‡ç­¾å¹³æ»‘ï¼ˆ0.0 è¡¨ç¤ºä¸å¹³æ»‘ï¼‰\n",
    "    reference_free=False,                   # æ˜¯å¦ä¸ä½¿ç”¨å‚è€ƒæ¨¡å‹ï¼ˆFalse è¡¨ç¤ºä½¿ç”¨ï¼‰\n",
    ")\n",
    "\n",
    "print(\"[DPO] è®­ç»ƒå‚æ•°é…ç½®å®Œæˆ\")\n",
    "print(f\"[DPO] å…³é”®å‚æ•°ï¼š\")\n",
    "print(f\"  - Learning rate: {dpo_args.learning_rate}ï¼ˆDPO å­¦ä¹ ç‡ï¼‰\")\n",
    "print(f\"  - Beta (Î²): {dpo_config.beta}ï¼ˆKL çº¦æŸå¼ºåº¦ï¼Œè¶Šå¤§è¶Šæ¥è¿‘å‚è€ƒæ¨¡å‹ï¼‰\")\n",
    "print(f\"  - Batch size: {dpo_args.per_device_train_batch_size}ï¼ˆæ¯è®¾å¤‡æ‰¹æ¬¡å¤§å°ï¼‰\")\n",
    "print(f\"  - Gradient accumulation: {dpo_args.gradient_accumulation_steps}ï¼ˆæ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼‰\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ab23bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã€DPO è®­ç»ƒå™¨æ„é€ ä¸è®­ç»ƒã€‘\n",
    "from trl import DPOTrainer\n",
    "\n",
    "print(\"[DPO] æ„é€  DPO è®­ç»ƒå™¨...\")\n",
    "\n",
    "# å¦‚æœç­–ç•¥æ¨¡å‹ä½¿ç”¨äº†é‡åŒ–ï¼Œéœ€è¦åº”ç”¨ LoRAï¼ˆå¦‚æœä½¿ç”¨ï¼‰\n",
    "if dpo_quantization_config is not None or use_cuda:\n",
    "    # å¦‚æœå·²ç»é‡åŒ–æˆ–ä½¿ç”¨ CUDAï¼Œå°è¯•åº”ç”¨ LoRA\n",
    "    try:\n",
    "        if dpo_lora_config is not None:\n",
    "            from peft import get_peft_model\n",
    "            # æ³¨æ„ï¼šå¦‚æœç­–ç•¥æ¨¡å‹å·²ç»æ˜¯ PeftModelï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†\n",
    "            if not isinstance(dpo_policy_model, PeftModel):\n",
    "                dpo_policy_model = get_peft_model(dpo_policy_model, dpo_lora_config)\n",
    "                print(\"[DPO] LoRA é…ç½®å·²åº”ç”¨åˆ°ç­–ç•¥æ¨¡å‹\")\n",
    "    except Exception as e:\n",
    "        print(f\"[DPO] åº”ç”¨ LoRA æ—¶å‡ºé”™ï¼š{e}ï¼Œç»§ç»­ä½¿ç”¨å½“å‰é…ç½®\")\n",
    "\n",
    "# æ„é€  DPO è®­ç»ƒå™¨\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=dpo_policy_model,                  # ç­–ç•¥æ¨¡å‹ï¼ˆè¦ä¼˜åŒ–çš„æ¨¡å‹ï¼‰\n",
    "    ref_model=dpo_ref_model,                 # å‚è€ƒæ¨¡å‹ï¼ˆç”¨äº KL çº¦æŸï¼Œå†»ç»“å‚æ•°ï¼‰\n",
    "    args=dpo_args,                           # è®­ç»ƒå‚æ•°\n",
    "    beta=dpo_config.beta,                    # æ¸©åº¦å‚æ•°ï¼ˆKL çº¦æŸå¼ºåº¦ï¼‰\n",
    "    train_dataset=dpo_train_ds,             # è®­ç»ƒæ•°æ®é›†ï¼ˆprompt, chosen, rejectedï¼‰\n",
    "    eval_dataset=dpo_eval_ds,                # éªŒè¯æ•°æ®é›†\n",
    "    tokenizer=tokenizer,                     # Tokenizer\n",
    "    peft_config=dpo_lora_config if dpo_quantization_config is None and not use_cuda else None,  # LoRA é…ç½®\n",
    "    max_length=max_seq_length,              # æœ€å¤§åºåˆ—é•¿åº¦\n",
    "    max_target_length=max_seq_length,        # æœ€å¤§ç›®æ ‡é•¿åº¦\n",
    "    loss_type=dpo_config.loss_type,          # æŸå¤±ç±»å‹ï¼ˆ\"sigmoid\" æˆ– \"hinge\"ï¼‰\n",
    "    label_smoothing=dpo_config.label_smoothing,  # æ ‡ç­¾å¹³æ»‘\n",
    ")\n",
    "\n",
    "print(\"[DPO] è®­ç»ƒå™¨æ„é€ å®Œæˆ\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"[DPO] DPO è®­ç»ƒæµç¨‹è¯´æ˜ï¼š\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "DPOï¼ˆDirect Preference Optimizationï¼‰çš„è®­ç»ƒæµç¨‹ï¼š\n",
    "\n",
    "1ï¸âƒ£ ã€æ•°æ®å‡†å¤‡ã€‘\n",
    "   - å‡†å¤‡åå¥½æ•°æ®ï¼ˆprompt, chosen, rejectedï¼‰\n",
    "   - chosenï¼šæ›´å¥½çš„å›ç­”\n",
    "   - rejectedï¼šæ›´å·®çš„å›ç­”\n",
    "\n",
    "2ï¸âƒ£ ã€æ¨¡å‹å‡†å¤‡ã€‘\n",
    "   - ç­–ç•¥æ¨¡å‹ï¼ˆÏ€_Î¸ï¼‰ï¼šè¦ä¼˜åŒ–çš„æ¨¡å‹ï¼ˆé€šå¸¸æ˜¯ SFT æ¨¡å‹ï¼‰\n",
    "   - å‚è€ƒæ¨¡å‹ï¼ˆÏ€_refï¼‰ï¼šç”¨äº KL çº¦æŸçš„åŸºå‡†æ¨¡å‹ï¼ˆå†»ç»“å‚æ•°ï¼‰\n",
    "\n",
    "3ï¸âƒ£ ã€DPO è®­ç»ƒã€‘\n",
    "   - è®¡ç®—ç­–ç•¥æ¨¡å‹å¯¹ chosen/rejected çš„å¯¹æ•°æ¦‚ç‡\n",
    "   - è®¡ç®—å‚è€ƒæ¨¡å‹å¯¹ chosen/rejected çš„å¯¹æ•°æ¦‚ç‡\n",
    "   - è®¡ç®— DPO æŸå¤±ï¼š\n",
    "     L = -log Ïƒ(Î²[(log Ï€_Î¸(chosen|x) - log Ï€_Î¸(rejected|x)) - \n",
    "                  (log Ï€_ref(chosen|x) - log Ï€_ref(rejected|x))])\n",
    "   - ä¼˜åŒ–ç­–ç•¥æ¨¡å‹ï¼Œæœ€å¤§åŒ– chosen æ¦‚ç‡ï¼Œæœ€å°åŒ– rejected æ¦‚ç‡\n",
    "   - é€šè¿‡éšå¼ KL çº¦æŸé˜²æ­¢åç¦»å‚è€ƒæ¨¡å‹\n",
    "\n",
    "4ï¸âƒ£ ã€ä¿å­˜æ¨¡å‹ã€‘\n",
    "   - ä¿å­˜ä¼˜åŒ–åçš„ç­–ç•¥æ¨¡å‹ï¼ˆå¯ä»¥æ˜¯ LoRA æƒé‡ï¼‰\n",
    "\n",
    "å…³é”®ä¼˜åŠ¿ï¼š\n",
    "- âœ… æ— éœ€è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰\n",
    "- âœ… æ— éœ€å¼ºåŒ–å­¦ä¹ ï¼ˆPPOï¼‰\n",
    "- âœ… è®­ç»ƒç®€å•ç¨³å®šï¼ˆç›‘ç£å­¦ä¹ ï¼‰\n",
    "- âœ… è®­ç»ƒé€Ÿåº¦å¿«ï¼ˆç›´æ¥ä¼˜åŒ–ï¼‰\n",
    "- âœ… æ˜“å¤ç°ï¼ˆç¡®å®šæ€§è®­ç»ƒï¼‰\n",
    "\n",
    "å…³é”®å‚æ•°ï¼š\n",
    "- Î² (beta)ï¼šæ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶ KL çº¦æŸå¼ºåº¦ï¼ˆå¸¸ç”¨ 0.1-0.5ï¼‰\n",
    "  - Î² è¶Šå¤§ â†’ æ›´æ¥è¿‘å‚è€ƒæ¨¡å‹ï¼ˆä¿å®ˆï¼‰\n",
    "  - Î² è¶Šå° â†’ æ›´åç¦»å‚è€ƒæ¨¡å‹ï¼ˆæ¿€è¿›ï¼‰\n",
    "\"\"\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# å¼€å§‹è®­ç»ƒï¼ˆæ³¨é‡Šæ‰ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©æ˜¯å¦æ‰§è¡Œï¼‰\n",
    "print(\"\\n[DPO] å‡†å¤‡å¼€å§‹è®­ç»ƒ...\")\n",
    "print(\"[DPO] è¦å¼€å§‹è®­ç»ƒï¼Œè¯·å–æ¶ˆä¸‹é¢ä»£ç çš„æ³¨é‡Š\")\n",
    "\n",
    "# è®­ç»ƒä»£ç ï¼ˆæ³¨é‡Šæ‰ï¼Œä¸å®é™…æ‰§è¡Œï¼‰\n",
    "# dpo_trainer.train()\n",
    "\n",
    "# ä¿å­˜æ¨¡å‹ï¼ˆæ³¨é‡Šæ‰ï¼‰\n",
    "# dpo_model_dir = os.path.join(dpo_output_dir, \"dpo_model\")\n",
    "# dpo_trainer.model.save_pretrained(dpo_model_dir)\n",
    "# tokenizer.save_pretrained(dpo_model_dir)\n",
    "# print(f\"[Done] DPO è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜è‡³: {dpo_model_dir}\")\n",
    "\n",
    "print(\"\\n[DPO] è®­ç»ƒä»£ç å·²å‡†å¤‡ï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e1dc02",
   "metadata": {},
   "source": [
    "## ğŸ” KL çº¦æŸè¯¦è§£ï¼šä»€ä¹ˆæ˜¯ KL æ•£åº¦å’Œ KL çº¦æŸï¼Ÿ\n",
    "\n",
    "### ğŸ“š KL æ•£åº¦ï¼ˆKullback-Leibler Divergenceï¼‰\n",
    "\n",
    "**KL æ•£åº¦**ï¼ˆä¹Ÿç§°ä¸ºç›¸å¯¹ç†µï¼‰æ˜¯è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒå·®å¼‚çš„æŒ‡æ ‡ã€‚\n",
    "\n",
    "#### æ•°å­¦å®šä¹‰\n",
    "\n",
    "å¯¹äºä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒ P å’Œ Qï¼ŒKL æ•£åº¦å®šä¹‰ä¸ºï¼š\n",
    "\n",
    "\\[\n",
    "D_{KL}(P || Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)}\n",
    "\\]\n",
    "\n",
    "åœ¨è¿ç»­æƒ…å†µä¸‹ï¼š\n",
    "\n",
    "\\[\n",
    "D_{KL}(P || Q) = \\int_{-\\infty}^{\\infty} p(x) \\log \\frac{p(x)}{q(x)} dx\n",
    "\\]\n",
    "\n",
    "#### ç›´è§‚ç†è§£\n",
    "\n",
    "- **KL æ•£åº¦ = 0**ï¼šä¸¤ä¸ªåˆ†å¸ƒå®Œå…¨ç›¸åŒ\n",
    "- **KL æ•£åº¦ > 0**ï¼šä¸¤ä¸ªåˆ†å¸ƒä¸åŒï¼Œå€¼è¶Šå¤§å·®å¼‚è¶Šå¤§\n",
    "- **KL æ•£åº¦ â‰  å¯¹ç§°**ï¼š\\(D_{KL}(P||Q) \\neq D_{KL}(Q||P)\\)\n",
    "\n",
    "#### åœ¨è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨\n",
    "\n",
    "åœ¨ RLHF ä¸­ï¼ŒKL æ•£åº¦è¡¡é‡çš„æ˜¯ï¼š\n",
    "- **ç­–ç•¥æ¨¡å‹** \\(\\pi_\\theta\\)ï¼ˆè®­ç»ƒä¸­çš„æ¨¡å‹ï¼‰\n",
    "- **å‚è€ƒæ¨¡å‹** \\(\\pi_{ref}\\)ï¼ˆåŸºå‡†æ¨¡å‹ï¼Œé€šå¸¸æ˜¯ SFT æ¨¡å‹ï¼‰\n",
    "\n",
    "çš„æ¦‚ç‡åˆ†å¸ƒå·®å¼‚ï¼š\n",
    "\n",
    "\\[\n",
    "D_{KL}(\\pi_\\theta || \\pi_{ref}) = \\mathbb{E}_{x \\sim \\pi_\\theta} \\left[ \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)} \\right]\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ KL çº¦æŸçš„ç›®çš„\n",
    "\n",
    "#### ä¸ºä»€ä¹ˆéœ€è¦ KL çº¦æŸï¼Ÿ\n",
    "\n",
    "åœ¨ RLHF è®­ç»ƒä¸­ï¼Œå¦‚æœ**åªæœ€å¤§åŒ–å¥–åŠ±**ï¼Œæ¨¡å‹å¯èƒ½ä¼šï¼š\n",
    "\n",
    "1. **è¿‡åº¦ä¼˜åŒ–**ï¼šä¸ºäº†è·å¾—æ›´é«˜å¥–åŠ±ï¼Œç”Ÿæˆä¸è‡ªç„¶çš„å›ç­”\n",
    "2. **æ¨¡å¼åç¼©**ï¼šåªç”Ÿæˆå°‘æ•°å‡ ç§é«˜åˆ†å›ç­”ï¼Œå¤±å»å¤šæ ·æ€§\n",
    "3. **åç¦»åŸºç¡€æ¨¡å‹**ï¼šå®Œå…¨æ”¹å˜æ¨¡å‹è¡Œä¸ºï¼Œä¸¢å¤±åŸæœ‰èƒ½åŠ›\n",
    "4. **é•¿åº¦åç½®**ï¼šç”Ÿæˆè¶…é•¿å›ç­”ï¼ˆå› ä¸ºæ›´é•¿å¯èƒ½è·å¾—æ›´é«˜å¥–åŠ±ï¼‰\n",
    "\n",
    "**KL çº¦æŸçš„ä½œç”¨**ï¼šé˜²æ­¢ç­–ç•¥æ¨¡å‹åç¦»å‚è€ƒæ¨¡å‹å¤ªè¿œï¼Œä¿æŒæ¨¡å‹çš„ï¼š\n",
    "- âœ… **ç¨³å®šæ€§**ï¼šä¸ä¼šäº§ç”Ÿæç«¯å˜åŒ–\n",
    "- âœ… **å¤šæ ·æ€§**ï¼šä¿æŒç”Ÿæˆå¤šæ ·æ€§\n",
    "- âœ… **åŸºç¡€èƒ½åŠ›**ï¼šä¿ç•™å‚è€ƒæ¨¡å‹ï¼ˆé€šå¸¸æ˜¯ SFTï¼‰çš„èƒ½åŠ›\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”§ KL çº¦æŸçš„å®ç°æ–¹å¼\n",
    "\n",
    "#### 1ï¸âƒ£ PPO ä¸­çš„æ˜¾å¼ KL çº¦æŸ\n",
    "\n",
    "åœ¨ PPO è®­ç»ƒä¸­ï¼ŒKL çº¦æŸé€šè¿‡**æ˜¾å¼ KL æƒ©ç½š**å®ç°ï¼š\n",
    "\n",
    "**æŸå¤±å‡½æ•°**ï¼š\n",
    "\\[\n",
    "\\mathcal{L}_{PPO} = -\\mathcal{L}_{policy} + \\lambda_{KL} \\cdot D_{KL}(\\pi_\\theta || \\pi_{ref})\n",
    "\\]\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- \\(\\mathcal{L}_{policy}\\)ï¼šç­–ç•¥æŸå¤±ï¼ˆæœ€å¤§åŒ–å¥–åŠ±ï¼‰\n",
    "- \\(D_{KL}(\\pi_\\theta || \\pi_{ref})\\)ï¼šKL æ•£åº¦æƒ©ç½šé¡¹\n",
    "- \\(\\lambda_{KL}\\)ï¼šKL æƒ©ç½šç³»æ•°ï¼ˆ`kl_coef`ï¼‰\n",
    "\n",
    "**ä»£ç ä¸­çš„ä½“ç°**ï¼š\n",
    "```python\n",
    "# PPO é…ç½®ä¸­çš„ KL çº¦æŸå‚æ•°\n",
    "ppo_config = PPOConfig(\n",
    "    init_kl_coef=0.1,      # KL æƒ©ç½šç³»æ•°ï¼ˆÎ»ï¼‰\n",
    "    target=6.0,            # ç›®æ ‡ KL æ•£åº¦å€¼\n",
    "    # ...\n",
    ")\n",
    "```\n",
    "\n",
    "**åŠ¨æ€è°ƒæ•´**ï¼š\n",
    "- å¦‚æœ KL æ•£åº¦ > ç›®æ ‡å€¼ â†’ å¢åŠ æƒ©ç½šï¼ˆå¢åŠ  `kl_coef`ï¼‰\n",
    "- å¦‚æœ KL æ•£åº¦ < ç›®æ ‡å€¼ â†’ å‡å°‘æƒ©ç½šï¼ˆå‡å°‘ `kl_coef`ï¼‰\n",
    "- ç¡®ä¿ç­–ç•¥ä¸ä¼šåç¦»å‚è€ƒæ¨¡å‹å¤ªè¿œ\n",
    "\n",
    "---\n",
    "\n",
    "#### 2ï¸âƒ£ DPO ä¸­çš„éšå¼ KL çº¦æŸ\n",
    "\n",
    "åœ¨ DPO è®­ç»ƒä¸­ï¼ŒKL çº¦æŸé€šè¿‡**å‚è€ƒæ¨¡å‹çš„å¯¹æ•°æ¦‚ç‡å·®**éšå¼å®ç°ï¼š\n",
    "\n",
    "**æŸå¤±å‡½æ•°**ï¼š\n",
    "\\[\n",
    "\\mathcal{L}_{DPO} = -\\log \\sigma\\left(\\beta\\left[\n",
    "  (\\log \\pi_\\theta(y_{pos}|x) - \\log \\pi_\\theta(y_{neg}|x)) - \n",
    "  (\\log \\pi_{ref}(y_{pos}|x) - \\log \\pi_{ref}(y_{neg}|x))\n",
    "\\right]\\right)\n",
    "\\]\n",
    "\n",
    "**éšå¼ KL çº¦æŸ**ï¼š\n",
    "- é€šè¿‡å‚è€ƒæ¨¡å‹çš„å¯¹æ•°æ¦‚ç‡å·® \\(\\log \\pi_{ref}(y_{pos}|x) - \\log \\pi_{ref}(y_{neg}|x)\\) å®ç°\n",
    "- **ä¸éœ€è¦æ˜¾å¼è®¡ç®— KL æ•£åº¦**\n",
    "- **beta (Î²)** å‚æ•°æ§åˆ¶çº¦æŸå¼ºåº¦ï¼ˆç›¸å½“äº PPO ä¸­çš„ `kl_coef`ï¼‰\n",
    "\n",
    "**ä»£ç ä¸­çš„ä½“ç°**ï¼š\n",
    "```python\n",
    "# DPO é…ç½®ä¸­çš„ KL çº¦æŸå‚æ•°ï¼ˆéšå¼ï¼‰\n",
    "dpo_config = DPOConfig(\n",
    "    beta=0.1,              # æ¸©åº¦å‚æ•°ï¼ˆæ§åˆ¶ KL çº¦æŸå¼ºåº¦ï¼‰\n",
    "    # ...\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š PPO vs DPO çš„ KL çº¦æŸå¯¹æ¯”\n",
    "\n",
    "| ç‰¹æ€§ | **PPOï¼ˆæ˜¾å¼ KL çº¦æŸï¼‰** | **DPOï¼ˆéšå¼ KL çº¦æŸï¼‰** |\n",
    "|------|----------------------|----------------------|\n",
    "| **è®¡ç®—æ–¹å¼** | æ˜¾å¼è®¡ç®— KL æ•£åº¦ | é€šè¿‡å‚è€ƒæ¨¡å‹å¯¹æ•°æ¦‚ç‡å·®éšå¼å®ç° |\n",
    "| **çº¦æŸå‚æ•°** | `kl_coef`ï¼ˆKL æƒ©ç½šç³»æ•°ï¼‰ | `beta`ï¼ˆæ¸©åº¦å‚æ•°ï¼‰ |\n",
    "| **å®ç°å¤æ‚åº¦** | éœ€è¦è®¡ç®—å®Œæ•´çš„ KL æ•£åº¦ | åªéœ€è®¡ç®—å¯¹æ•°æ¦‚ç‡å·® |\n",
    "| **åŠ¨æ€è°ƒæ•´** | æ ¹æ® KL æ•£åº¦åŠ¨æ€è°ƒæ•´æƒ©ç½š | å›ºå®š betaï¼Œæ— éœ€åŠ¨æ€è°ƒæ•´ |\n",
    "| **è®¡ç®—æˆæœ¬** | è¾ƒé«˜ï¼ˆéœ€è¦è®¡ç®— KL æ•£åº¦ï¼‰ | è¾ƒä½ï¼ˆåªéœ€è®¡ç®—å¯¹æ•°æ¦‚ç‡ï¼‰ |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”‘ å…³é”®å‚æ•°ç†è§£\n",
    "\n",
    "#### PPO ä¸­çš„ KL å‚æ•°\n",
    "\n",
    "1. **`init_kl_coef`ï¼ˆåˆå§‹ KL æƒ©ç½šç³»æ•°ï¼‰**\n",
    "   - **å«ä¹‰**ï¼šåˆå§‹çš„ KL æƒ©ç½šæƒé‡ \\(\\lambda_{KL}\\)\n",
    "   - **èŒƒå›´**ï¼šé€šå¸¸ 0.01 - 0.5\n",
    "   - **ä½œç”¨**ï¼šå¹³è¡¡å¥–åŠ±æœ€å¤§åŒ–ä¸ KL çº¦æŸ\n",
    "   - **å½±å“**ï¼š\n",
    "     - å€¼è¶Šå¤§ â†’ æ›´ä¿å®ˆï¼Œæ›´æ¥è¿‘å‚è€ƒæ¨¡å‹ï¼ˆä½†å¯èƒ½ç‰ºç‰²å¥–åŠ±ï¼‰\n",
    "     - å€¼è¶Šå° â†’ æ›´æ¿€è¿›ï¼Œæ›´å®¹æ˜“åç¦»å‚è€ƒæ¨¡å‹ï¼ˆä½†å¯èƒ½æ¨¡å¼åç¼©ï¼‰\n",
    "\n",
    "2. **`target`ï¼ˆç›®æ ‡ KL æ•£åº¦ï¼‰**\n",
    "   - **å«ä¹‰**ï¼šæœŸæœ›çš„ KL æ•£åº¦ç›®æ ‡å€¼\n",
    "   - **èŒƒå›´**ï¼šé€šå¸¸ 2.0 - 10.0\n",
    "   - **ä½œç”¨**ï¼šæ§åˆ¶ç­–ç•¥ä¸å‚è€ƒæ¨¡å‹çš„åç¦»ç¨‹åº¦\n",
    "   - **å½±å“**ï¼š\n",
    "     - å€¼è¶Šå¤§ â†’ å…è®¸æ›´å¤§åç¦»ï¼ˆä½†å¯èƒ½è¿‡åº¦ä¼˜åŒ–ï¼‰\n",
    "     - å€¼è¶Šå° â†’ è¦æ±‚æ›´æ¥è¿‘å‚è€ƒæ¨¡å‹ï¼ˆä½†å¯èƒ½æ›´æ–°å¤ªæ…¢ï¼‰\n",
    "\n",
    "#### DPO ä¸­çš„ KL å‚æ•°\n",
    "\n",
    "1. **`beta`ï¼ˆæ¸©åº¦å‚æ•°ï¼‰**\n",
    "   - **å«ä¹‰**ï¼šæ§åˆ¶ KL çº¦æŸå¼ºåº¦çš„æ¸©åº¦å‚æ•°\n",
    "   - **èŒƒå›´**ï¼šé€šå¸¸ 0.1 - 0.5\n",
    "   - **ä½œç”¨**ï¼šéšå¼æ§åˆ¶ç­–ç•¥ä¸å‚è€ƒæ¨¡å‹çš„åç¦»ç¨‹åº¦\n",
    "   - **å½±å“**ï¼š\n",
    "     - å€¼è¶Šå¤§ â†’ æ›´ä¿å®ˆï¼Œæ›´æ¥è¿‘å‚è€ƒæ¨¡å‹\n",
    "     - å€¼è¶Šå° â†’ æ›´æ¿€è¿›ï¼Œæ›´å®¹æ˜“åç¦»å‚è€ƒæ¨¡å‹\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¡ å®é™…åº”ç”¨å»ºè®®\n",
    "\n",
    "#### å¦‚ä½•é€‰æ‹© KL å‚æ•°ï¼Ÿ\n",
    "\n",
    "1. **PPO ä¸­çš„ `kl_coef`**ï¼š\n",
    "   - **åˆå§‹å€¼**ï¼šä» 0.1 å¼€å§‹\n",
    "   - **è§‚å¯Ÿ**ï¼šç›‘æ§ KL æ•£åº¦æ˜¯å¦ç¨³å®š\n",
    "   - **è°ƒæ•´**ï¼š\n",
    "     - KL æ•£åº¦å¤ªå¤§ â†’ å¢åŠ  `kl_coef`\n",
    "     - KL æ•£åº¦å¤ªå° â†’ å‡å°‘ `kl_coef`\n",
    "   - **å¹³è¡¡**ï¼šåœ¨å¥–åŠ±æœ€å¤§åŒ–ä¸ KL çº¦æŸä¹‹é—´æ‰¾åˆ°å¹³è¡¡\n",
    "\n",
    "2. **DPO ä¸­çš„ `beta`**ï¼š\n",
    "   - **åˆå§‹å€¼**ï¼šä» 0.1 å¼€å§‹\n",
    "   - **è§‚å¯Ÿ**ï¼šç›‘æ§ chosen/rejected çš„æ¦‚ç‡å·®å¼‚\n",
    "   - **è°ƒæ•´**ï¼š\n",
    "     - æ¨¡å‹å¤ªä¿å®ˆ â†’ å‡å° `beta`\n",
    "     - æ¨¡å‹åç¦»å¤ªè¿œ â†’ å¢å¤§ `beta`\n",
    "   - **å¹³è¡¡**ï¼šåœ¨åå¥½ä¼˜åŒ–ä¸æ¨¡å‹ç¨³å®šæ€§ä¹‹é—´æ‰¾åˆ°å¹³è¡¡\n",
    "\n",
    "#### å¸¸è§é—®é¢˜\n",
    "\n",
    "1. **KL æ•£åº¦å¤ªå¤§æ€ä¹ˆåŠï¼Ÿ**\n",
    "   - å¢åŠ  `kl_coef`ï¼ˆPPOï¼‰æˆ– `beta`ï¼ˆDPOï¼‰\n",
    "   - é™ä½å­¦ä¹ ç‡\n",
    "   - å¢åŠ ç›®æ ‡ KL æ•£åº¦å€¼ï¼ˆPPOï¼‰\n",
    "\n",
    "2. **KL æ•£åº¦å¤ªå°æ€ä¹ˆåŠï¼Ÿ**\n",
    "   - å‡å°‘ `kl_coef`ï¼ˆPPOï¼‰æˆ– `beta`ï¼ˆDPOï¼‰\n",
    "   - å¢åŠ å­¦ä¹ ç‡\n",
    "   - å‡å°‘ç›®æ ‡ KL æ•£åº¦å€¼ï¼ˆPPOï¼‰\n",
    "\n",
    "3. **å¦‚ä½•ç›‘æ§ KL æ•£åº¦ï¼Ÿ**\n",
    "   - åœ¨è®­ç»ƒæ—¥å¿—ä¸­è§‚å¯Ÿ KL æ•£åº¦å€¼\n",
    "   - ç¡®ä¿ KL æ•£åº¦ç¨³å®šï¼ˆä¸è¦å‰§çƒˆæ³¢åŠ¨ï¼‰\n",
    "   - å¦‚æœ KL æ•£åº¦æŒç»­å¢å¤§ï¼Œè¯´æ˜æ¨¡å‹åœ¨åç¦»å‚è€ƒæ¨¡å‹\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ æ€»ç»“\n",
    "\n",
    "**KL çº¦æŸçš„æ ¸å¿ƒæ€æƒ³**ï¼š\n",
    "- **ç›®æ ‡**ï¼šåœ¨ä¼˜åŒ–å¥–åŠ±çš„åŒæ—¶ï¼Œé˜²æ­¢æ¨¡å‹åç¦»å‚è€ƒæ¨¡å‹å¤ªè¿œ\n",
    "- **æ–¹å¼**ï¼šé€šè¿‡ KL æ•£åº¦æƒ©ç½šï¼ˆPPOï¼‰æˆ–éšå¼çº¦æŸï¼ˆDPOï¼‰å®ç°\n",
    "- **ä½œç”¨**ï¼šä¿æŒæ¨¡å‹ç¨³å®šæ€§ã€å¤šæ ·æ€§ï¼Œä¿ç•™åŸºç¡€èƒ½åŠ›\n",
    "- **å‚æ•°**ï¼š`kl_coef`ï¼ˆPPOï¼‰æˆ– `beta`ï¼ˆDPOï¼‰æ§åˆ¶çº¦æŸå¼ºåº¦\n",
    "\n",
    "**å…³é”®è¦ç‚¹**ï¼š\n",
    "- âœ… KL çº¦æŸæ˜¯ RLHF è®­ç»ƒä¸­çš„å…³é”®æœºåˆ¶\n",
    "- âœ… PPO ä½¿ç”¨æ˜¾å¼ KL çº¦æŸï¼ˆè®¡ç®— KL æ•£åº¦ï¼‰\n",
    "- âœ… DPO ä½¿ç”¨éšå¼ KL çº¦æŸï¼ˆé€šè¿‡å‚è€ƒæ¨¡å‹å¯¹æ•°æ¦‚ç‡å·®ï¼‰\n",
    "- âœ… å‚æ•°è°ƒä¼˜å¾ˆé‡è¦ï¼Œéœ€è¦å¹³è¡¡å¥–åŠ±ä¸çº¦æŸ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74162724",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86705062",
   "metadata": {},
   "source": [
    "## ğŸ“Š DPO vs RM+PPO å®Œæ•´å¯¹æ¯”\n",
    "\n",
    "### ğŸ”„ æµç¨‹å¯¹æ¯”\n",
    "\n",
    "```\n",
    "ä¼ ç»Ÿ RLHFï¼ˆRM + PPOï¼‰æµç¨‹ï¼š\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  1ï¸âƒ£ SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰                        â”‚\n",
    "â”‚     â†“                                    â”‚\n",
    "â”‚  2ï¸âƒ£ RMï¼ˆå¥–åŠ±æ¨¡å‹è®­ç»ƒï¼‰                     â”‚\n",
    "â”‚     â”œâ”€â”€ è®­ç»ƒå¥–åŠ±å¤´                        â”‚\n",
    "â”‚     â””â”€â”€ å­¦ä¼šæ‰“åˆ†ï¼ˆchosen > rejectedï¼‰      â”‚\n",
    "â”‚     â†“                                    â”‚\n",
    "â”‚  3ï¸âƒ£ PPOï¼ˆå¼ºåŒ–ä¼˜åŒ–ï¼‰                        â”‚\n",
    "â”‚     â”œâ”€â”€ ç­–ç•¥æ¨¡å‹ç”Ÿæˆå›ç­”                   â”‚\n",
    "â”‚     â”œâ”€â”€ å¥–åŠ±æ¨¡å‹æ‰“åˆ†                       â”‚\n",
    "â”‚     â”œâ”€â”€ PPO ä¼˜åŒ–ç­–ç•¥                      â”‚\n",
    "â”‚     â””â”€â”€ å€¼å‡½æ•°å¤´ï¼ˆç”¨äºä¼˜åŠ¿ä¼°è®¡ï¼‰            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "DPO æµç¨‹ï¼š\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  1ï¸âƒ£ SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰                        â”‚\n",
    "â”‚     â†“                                    â”‚\n",
    "â”‚  2ï¸âƒ£ DPOï¼ˆç›´æ¥åå¥½ä¼˜åŒ–ï¼‰                    â”‚\n",
    "â”‚     â”œâ”€â”€ ç›´æ¥ç”¨åå¥½æ•°æ®è®­ç»ƒ                â”‚\n",
    "â”‚     â”œâ”€â”€ æœ€å¤§åŒ– chosen æ¦‚ç‡               â”‚\n",
    "â”‚     â”œâ”€â”€ æœ€å°åŒ– rejected æ¦‚ç‡              â”‚\n",
    "â”‚     â””â”€â”€ éšå¼ KL çº¦æŸï¼ˆé€šè¿‡å‚è€ƒæ¨¡å‹ï¼‰       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ğŸ¯ å…³é”®åŒºåˆ«\n",
    "\n",
    "| ç‰¹æ€§ | **RM + PPO** | **DPO** |\n",
    "|------|------------|---------|\n",
    "| **è®­ç»ƒé˜¶æ®µ** | 3 é˜¶æ®µï¼ˆSFT â†’ RM â†’ PPOï¼‰ | 2 é˜¶æ®µï¼ˆSFT â†’ DPOï¼‰ |\n",
    "| **éœ€è¦å¥–åŠ±æ¨¡å‹** | âœ… æ˜¯ | âŒ å¦ |\n",
    "| **éœ€è¦å¼ºåŒ–å­¦ä¹ ** | âœ… æ˜¯ï¼ˆPPOï¼‰ | âŒ å¦ |\n",
    "| **éœ€è¦å€¼å‡½æ•°å¤´** | âœ… æ˜¯ï¼ˆç”¨äº PPOï¼‰ | âŒ å¦ |\n",
    "| **KL çº¦æŸæ–¹å¼** | æ˜¾å¼ KL æƒ©ç½š | éšå¼ KL çº¦æŸï¼ˆé€šè¿‡å‚è€ƒæ¨¡å‹ï¼‰ |\n",
    "| **è®­ç»ƒå¤æ‚åº¦** | é«˜ï¼ˆRL å›è·¯ï¼‰ | ä½ï¼ˆç›‘ç£å­¦ä¹ ï¼‰ |\n",
    "| **è®­ç»ƒç¨³å®šæ€§** | ä¸­ç­‰ï¼ˆRL ä¸ç¨³å®šï¼‰ | é«˜ï¼ˆç›‘ç£å­¦ä¹ ï¼‰ |\n",
    "| **è®­ç»ƒé€Ÿåº¦** | æ…¢ï¼ˆç”Ÿæˆ+æ‰“åˆ†å¾ªç¯ï¼‰ | å¿«ï¼ˆç›´æ¥ä¼˜åŒ–ï¼‰ |\n",
    "| **æ•°æ®éœ€æ±‚** | Promptsï¼ˆç”Ÿæˆæ—¶ç”¨ï¼‰ | åå¥½å¯¹ï¼ˆchosen/rejectedï¼‰ |\n",
    "| **èµ„æºéœ€æ±‚** | é«˜ï¼ˆéœ€è¦ RMï¼‰ | ä½ï¼ˆæ— éœ€ RMï¼‰ |\n",
    "| **é€‚ç”¨åœºæ™¯** | éœ€è¦çµæ´»å¥–åŠ±æ§åˆ¶ | å›ºå®šåå¥½æ•°æ® |\n",
    "\n",
    "### ğŸ“¦ æƒé‡å­˜å‚¨å¯¹æ¯”\n",
    "\n",
    "| é˜¶æ®µ | **RM + PPO** | **DPO** |\n",
    "|------|------------|---------|\n",
    "| **SFT** | LoRA æƒé‡ | LoRA æƒé‡ |\n",
    "| **RM** | åŸºç¡€æ¨¡å‹ + å¥–åŠ±å¤´ | âŒ ä¸éœ€è¦ |\n",
    "| **PPO** | ç­–ç•¥æ¨¡å‹ + å€¼å‡½æ•°å¤´ | âŒ ä¸éœ€è¦ |\n",
    "| **DPO** | âŒ ä¸éœ€è¦ | ç­–ç•¥æ¨¡å‹ï¼ˆå¯ä»¥æ˜¯ LoRAï¼‰ |\n",
    "\n",
    "### âœ… ä½•æ—¶ä½¿ç”¨ DPO\n",
    "\n",
    "**é€‚åˆä½¿ç”¨ DPO çš„åœºæ™¯**ï¼š\n",
    "- âœ… æœ‰é«˜è´¨é‡åå¥½æ•°æ®ï¼ˆchosen/rejected å¯¹ï¼‰\n",
    "- âœ… ä¸éœ€è¦åŠ¨æ€å¥–åŠ±è°ƒæ•´\n",
    "- âœ… è¿½æ±‚è®­ç»ƒç®€å•æ€§å’Œç¨³å®šæ€§\n",
    "- âœ… èµ„æºå—é™ï¼ˆä¸æƒ³è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼‰\n",
    "- âœ… éœ€è¦å¿«é€Ÿè¿­ä»£\n",
    "\n",
    "**é€‚åˆä½¿ç”¨ RM+PPO çš„åœºæ™¯**ï¼š\n",
    "- âœ… éœ€è¦åŠ¨æ€å¥–åŠ±è°ƒæ•´ï¼ˆåœ¨çº¿å­¦ä¹ ï¼‰\n",
    "- âœ… éœ€è¦çµæ´»çš„å¥–åŠ±æ§åˆ¶\n",
    "- âœ… åå¥½æ•°æ®è¾ƒå°‘ï¼Œä½†æœ‰å¾ˆå¤š prompts\n",
    "- âœ… éœ€è¦ç²¾ç»†çš„ RL æ§åˆ¶\n",
    "\n",
    "### ğŸ”‘ DPO çš„å…³é”®å‚æ•°\n",
    "\n",
    "1. **beta (Î²)**ï¼šæ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶ KL çº¦æŸå¼ºåº¦\n",
    "   - **èŒƒå›´**ï¼š0.1 - 0.5ï¼ˆå¸¸ç”¨ï¼‰\n",
    "   - **Î² è¶Šå¤§**ï¼šæ›´ä¿å®ˆï¼Œæ›´æ¥è¿‘å‚è€ƒæ¨¡å‹\n",
    "   - **Î² è¶Šå°**ï¼šæ›´æ¿€è¿›ï¼Œæ›´å®¹æ˜“åç¦»å‚è€ƒæ¨¡å‹\n",
    "   - **æ¨èå€¼**ï¼š0.1ï¼ˆé»˜è®¤ï¼‰ï¼Œæ ¹æ®å®é™…æ•ˆæœè°ƒæ•´\n",
    "\n",
    "2. **å‚è€ƒæ¨¡å‹**ï¼šç”¨äº KL çº¦æŸçš„åŸºå‡†æ¨¡å‹\n",
    "   - **é€šå¸¸æ˜¯**ï¼šSFT æ¨¡å‹ï¼ˆå†»ç»“å‚æ•°ï¼‰\n",
    "   - **ä½œç”¨**ï¼šé˜²æ­¢ç­–ç•¥æ¨¡å‹åç¦»å¤ªè¿œ\n",
    "   - **å…³é”®**ï¼šå‚è€ƒæ¨¡å‹å¿…é¡»ä¸ç­–ç•¥æ¨¡å‹åˆå§‹åŒ–ç›¸åŒ\n",
    "\n",
    "3. **æŸå¤±ç±»å‹**ï¼š\n",
    "   - **sigmoid**ï¼šæ ‡å‡†çš„ sigmoid æŸå¤±ï¼ˆæ¨èï¼‰\n",
    "   - **hinge**ï¼šhinge æŸå¤±ï¼ˆåœ¨æŸäº›åœºæ™¯ä¸‹æ›´ç¨³å®šï¼‰\n",
    "\n",
    "### ğŸ’¡ DPO å®è·µå»ºè®®\n",
    "\n",
    "1. **æ•°æ®è´¨é‡**ï¼šåå¥½æ•°æ®è´¨é‡ç›´æ¥å½±å“ DPO æ•ˆæœ\n",
    "2. **beta è°ƒä¼˜**ï¼šæ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ beta å‚æ•°\n",
    "3. **å‚è€ƒæ¨¡å‹**ï¼šç¡®ä¿å‚è€ƒæ¨¡å‹ä¸ç­–ç•¥æ¨¡å‹åˆå§‹åŒ–ä¸€è‡´\n",
    "4. **è¯„ä¼°æŒ‡æ ‡**ï¼šç›‘æ§ chosen/rejected çš„æ¦‚ç‡å·®å¼‚\n",
    "5. **é˜²æ­¢è¿‡æ‹Ÿåˆ**ï¼šä½¿ç”¨éªŒè¯é›†å®šæœŸè¯„ä¼°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b952911c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
